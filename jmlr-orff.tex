\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

\usepackage{pythontex}
\usepackage{acro}
\usepackage{ragged2e}
\usepackage{cmap} % copy-paste pdf
\usepackage{microtype}
\usepackage{dirtytalk}
\usepackage{pdflscape}
\usepackage{afterpage}

\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{cd}
\usetikzlibrary{matrix}
\usetikzlibrary{calc}
\makeatletter
\tikzset{
  column sep/.code=\def\pgfmatrixcolumnsep{\pgf@matrix@xscale*(#1)},
  row sep/.code   =\def\pgfmatrixrowsep{\pgf@matrix@yscale*(#1)},
  matrix xscale/.code=%
    \pgfmathsetmacro\pgf@matrix@xscale{\pgf@matrix@xscale*(#1)},
  matrix yscale/.code=%
    \pgfmathsetmacro\pgf@matrix@yscale{\pgf@matrix@yscale*(#1)},
  matrix scale/.style={/tikz/matrix xscale={#1},/tikz/matrix yscale={#1}}}
\def\pgf@matrix@xscale{1}
\def\pgf@matrix@yscale{1}
\makeatother

\usepackage[american]{babel}

\usepackage{braket}
\usepackage{colonequals}
\usepackage{amsmath}
\usepackage{nameref}
\usepackage[nameinlink,noabbrev]{cleveref}
\usepackage{mathtools}
\usepackage[algo2e,ruled,linesnumbered]{algorithm2e}
\usepackage{breqn}
\usepackage{commands}

\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{ltablex}
\usepackage{threeparttablex}
\setlength{\extrarowheight}{3pt} % Increase table row height
\newcommand{\tableheadline}[1]{\multicolumn{1}{c}{\spacedlowsmallcaps{#1}}}
\newcommand{\myfloatalign}{\centering} % To be used with each float for alignment
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{caption}

\crefname{algocfline}{Algorithm}{Algorithms}
\Crefname{algocfline}{Algorithm}{Algorithms}
\crefname{algocf}{Algorithm}{Algorithm}
\Crefname{algocf}{Algorithm}{Algorithms}
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{chapter}{Chapter}{Chapters}
\Crefname{chapter}{Chapter}{Chapters}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}
\crefname{subsection}{Subsection}{Subsections}
\Crefname{subsection}{Subsection}{Subsections}
\crefname{theorem}{Theorem}{Theorems}
\Crefname{theorem}{Theorem}{Theorems}
\crefname{proposition}{Proposition}{Propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{lemma}{Lemma}{Lemmas}
\Crefname{lemma}{Lemma}{Lemmas}
\crefname{corollary}{Corollary}{Corollaries}
\Crefname{corollary}{corollary}{Corollaries}
\crefname{equation}{Equation}{Equations}
\Crefname{equation}{Equation}{Equations}
\crefname{remark}{Remark}{Remarks}
\Crefname{remark}{Remark}{Remarks}
\crefname{example}{Example}{Examples}
\Crefname{example}{Example}{Examples}
\creflabelformat{equation}{#2\textup{#1}#3}

% Heading arguments are {volume}{year}{pages}{submitted}{published}
%     {author-full-names}

\jmlrheading{1}{2000}{1--48}{4/00}{10/00}{brault00a}
    {Romain Brault and Florence d'Alch\'e-Buc}

% Short headings should be running head and authors last names

\ShortHeadings{RFFs for OVKs}{Brault and d'Alch\'e-Buc}
\firstpageno{1}

\begin{document}

\title{Random Fourier Features for Operator-Valued kernels}

\author{\name{}Brault Romain
       \email~romain.brault@telecom-paristech.fr \\
       \addr~LTCI\\
       T\'el\'ecom ParisTech\\
       Paris, 46 rue Barrault, France \\
       Universit\'e Paris-Saclay \\
       \AND%
       \name{}Florence d'Alch\'e-Buc
       \email~florence.dalche@telecom-paristech.fr \\
       \addr~LTCI\\
       T\'el\'ecom ParisTech\\
       Paris, 46 rue Barrault, France \\
       Universit\'e Paris-Saclay}

\editor{Francis Bach}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
    Many problems in Machine Learning can be cast into
    vector-valued functions approximation. Operator-Valued Kernels
    \emph{\acl{OVK}s} and vector-valued Reproducing Kernel Hilbert Spaces
    provide a theoretical and practical framework to address that issue,
    extending nicely the well-known setting of scalar-valued kernels.
    However large scale applications are usually not affordable with these
    tools that require an important computational power along with a large
    memory capacity. In this paper, we propose and study scalable methods
    to perform regression with \emph{\acl{OVK}s}. To achieve this goal, we
    extend Random Fourier Features, an approximation technique originally
    introduced for scalar-valued kernels, to \emph{\acl{OVK}s}. The idea is
    to take advantage of an approximated operator-valued feature map in
    order to come up with a linear model in a finite-dimensional space.
\end{abstract}

\begin{keywords}
    Random Fourier Feature, Operator-Valued Kernel
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
This paper is dedicated to the definition of a general and flexible approach to
learn vector-valued functions together with an efficient implementation of the
learning algorithms. To achieve this goal, we study shallow architectures,
namely the product of a (nonlinear) operator-valued feature
$\widetilde{\Phi}(x)$ and a parameter vector $\theta$ such that
$\widetilde{f}(x) = {\widetilde{\Phi}(x)}^* \theta$, and combine two appealing
methodologies: Operator-Valued Kernel Regression and Random Fourier Features.
\paragraph{}
Operator-Valued Kernels \citep{Micchelli2005,Carmeli2010,Kadri_aistat10,
Brouard2011,Alvarez2012} extend the classic scalar-valued kernels to functions
with values in some \emph{output} Hilbert space. As in the scalar case,
\acfp{OVK} are used to build Reproducing Kernel Hilbert Spaces (\acs{RKHS}) in
which representer theorems apply as for ridge regression or other appropriate
loss functional. In these cases, learning a model in the \acs{RKHS} boils down
to learning a function of the form $f(x)=\sum_{i=1}^N K(x,x_i)\alpha_i$ where
$x_1, \ldots, x_N$ are the training input data and each $\alpha_i, i=1, \ldots,
N$ is a vector of the output space $\mathcal{Y}$, and each $K(x,x_i)$ is an
operator on vectors of $\mathcal{Y}$.
\paragraph{}
However, \acsp{OVK} suffer from the same drawbacks as classic
(sca\-lar-va\-lued) kernel machines: they scale poorly to large datasets
because they are exceedingly demanding in terms of memory and computations. We
propose to approximate OVKs by extending a methodology called \acfp{RFF}
\citep{Rahimi2007, Le2013, Yang2015, sriper2015, Bach2015, sutherland2015,
rudi2016generalization} so far developed to speed up scalar-valued kernel
machines. The \acs{RFF} approach linearizes a shift-invariant kernel model by
generating explicitly an approximated feature map $\tilde{\phi}$. \acsp{RFF}
has been shown to be efficient on large datasets and has been further improved
by efficient matrix computations such as \citep[``FastFood'']{Le2013} and
\citep[``SORF'']{felix2016orthogonal}, which are considered as the best large
scale implementations of kernel methods, along with Nystr\"om approaches
proposed in \citet{drineas2005nystrom}. Moreover thanks to \acsp{RFF}, kernel
methods have been proved to be competitive with deep architectures
\citep{lu2014scale, dai2014scalable, yang2015deep}.

\subsection{Outline}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}
\subsection{Introduction to kernel methods}\label{subsec:kernels}
\subsubsection{Kernels and Reproducing Kernel Hilbert Spaces}
The idea of kernel methods
\citep{Aronszajn1950,KIMELDORF1971,boser1992training,
Berlinet2003,Shawe-TaylorBook} is to work in a subset of the set of all
functions, namely a Reproducing Kernel Hilbert Space (\acl{RKHS}), associated
to a well chosen positive definite and symmetric function (\emph{a kernel}).
\paragraph{}
\begin{definition}[Symmetric Positive Definite kernels]
    Let $\mathcal{X}$ be a Polish space. A kernel $k:\mathcal{X} \times
    \mathcal{X} \to \mathbb{R}$ is said to be \acf{PSD} if for any $(x_1,
    \ldots, x_N) \in \mathcal{X}^N$, the (Gram) matrix
    \begin{dmath*}
        \mathbf{K} =
        \begin{pmatrix}
            k(x_i, x_j)
        \end{pmatrix}_{i = 1, j = 1}^{i = N, j = N} \hiderel{\in}
        \mathcal{M}_{N, N}(\mathbb{R})
    \end{dmath*}
    is \acf{SPSD}.
\end{definition}
The following proposition gives sufficient conditions to obtain a \acs{SPSD}
matrix:
\begin{proposition}[SPSD matrix]
    $K$ is SPSD if it is symmetric and one of the following conditions holds:
    \begin{itemize}
        \item The eigenvalues of $\mathbf{K}$ are non-negative
        \item for any column vector $c= (c_1, \ldots, c_N)^\transpose \in
        \mathcal{M}_{N, 1}(\mathbb{R})$,
        \begin{dmath*}
            c^\transpose\mathbf{K}c=\sum_{i,j=1}^N c_ic_jK(x_i,x_i)
            \hiderel{\geq} 0.
        \end{dmath*}
    \end{itemize}
\end{proposition}
One of the most important property of \acs{PSD} kernels \citep{Mohri2012} is
that a \acs{PSD} kernel defines a unique \acs{RKHS}. Note that the converse is
also true.
\begin{theorem}[\citet{Aronszajn1950}]
    Suppose $k$ is a symmetric, positive definite kernel on a set
    $\mathcal{X}$. Then there is a unique Hilbert space of functions
    $\mathcal{H}$ on $\mathcal{X}$ for which $k$ is a reproducing kernel,
    \acs{ie}
    \begin{dgroup}
        \begin{dmath}\label{eq:reproducing-prop}
            \forall x \in \mathcal{X}, k(\cdot, x) \hiderel{\in} \mathcal{H}
        \end{dmath}
        \begin{dmath}
            \forall h \in \mathcal{H}, \forall x \hiderel{\in} \mathcal{X},
            h(x) \hiderel{=} \inner{h,k(\cdot,x)}_{\mathcal{H}}.
        \end{dmath}
    \end{dgroup}
    $\mathcal{H}$ is called a reproducing kernel Hilbert space (\acl{RKHS})
    associated to $k$, and will be denoted, $\mathcal{H}_k$.
\end{theorem}
Another way to use Aronszajn's results is to state the feature map property for
the \acs{PSD} kernels.
\begin{proposition}[Feature map]
     Suppose $k$ is a symmetric, positive definite kernel on a set
     $\mathcal{X}$. Then, there exists a Hilbert space $\mathcal{H}$ and a
     mapping $\phi$ from $\mathcal{X}$ to $\mathcal{H}$ such that:
    \begin{dmath*}
        \forall x, x' \in \mathcal{X},
        k(x,x')=\inner{\phi(x),\phi(x')}_{\mathcal{H}}.
    \end{dmath*}
    The mapping $\phi$ is called a \emph{feature map} and $\mathcal{H}$, a
    feature space.
\end{proposition}
\begin{remark}
    Aronszajn's theorem tells us that there always exists at least one feature
    map, the so-called \emph{canonical feature map} and the feature space
    associate, the \acl{RKHS} $\mathcal{H}_k$
    \begin{dmath*}
        \phi_{canonical}(x)= k(\cdot, x)
    \end{dmath*}
    and $\mathcal{H}= \mathcal{H}_k$.  However there might exist several pairs
    of feature maps and features spaces for a given kernel $k$.
\end{remark}
\subsubsection{Learning in Reproducing Kernel Hilbert Spaces}
Now, back to learning and minimizing the empirical risk, a fair question is
how do I pick-up a function in an infinite dimensional set $\mathcal{H}_k$ in
polynomial time? The answer comes from the regularization and interpolation
theory. To limit the size of the space in which we search of the function
minimizing the empirical risk we add a regularization term to the empirical
risk.
\begin{dmath*}
    \mathcal{J}_{\lambda}(f) = \frac{1}{N} \sum_{i=1}^N L\left(x_i, f,
    y_i\right) + \frac{\lambda}{2}\norm{f}_{\mathcal{H}_k}^2
\end{dmath*}
and we minimize $J_{\lambda}$ instead of $\mathfrak{R}_{\text{emp}}$. Then
the representer theorem (also called minimal norm interpolation theorem) states
the following.
\begin{theorem}[Representer theorem, \citet{Wahba90}]
    If $f_{\seq{s}}$ is a solution of
    \begin{dmath*}
        \argmin_{f\in\mathcal{H}_k} J_{\lambda}(f),
    \end{dmath*}
    where $\lambda > 0$ then $f_{\seq{s}}=\sum_{i=1}^N k(\cdot, x_i) \alpha_i$.
\end{theorem}
We note the vector $\boldsymbol{\alpha} = (\alpha_i)_{i=1}^N$ and the matrix
$\mathbf{K}=(k(x_i, x_k))_{i, k = 1}^N$. Then we can rewrite
\begin{dmath*}
    \mathcal{J}_{\lambda}(\alpha) = \frac{1}{N} \sum_{i=1}^N L(x_i, \alpha,
    y_i) + \lambda \inner{\boldsymbol{\alpha}, \mathbf{K}\boldsymbol{\alpha}}_2
    / 2
\end{dmath*}
where $f(x_i) = (\mathbf{K}\boldsymbol{\alpha})_i$ for any $x_i \in \seq{s}$.
If we suppose that $L$ is convex in $f$, then it is possible to derive a
polynomial time (in $N$) algorithm minimizing $\mathcal{J}_{\lambda}$. For
instance if we choose $L$ to be the least square loss, then
\begin{dmath}
    \label{eq:ridge_regression}
    \mathcal{J}_{\lambda}(\alpha) = \frac{1}{2N}\norm{\mathbf{K}\alpha -
    (y_i)_{i=1}^N}_2^2 + \lambda \inner{\boldsymbol{\alpha},
    \mathbf{K}\boldsymbol{\alpha}}_2 / 2.
\end{dmath}
This problem is called \emph{Ridge regression}.  By strict convexity and
coercivity of $\mathcal{J}_{\lambda}$, and because $K + \lambda I_N$ is
invertible for any $\lambda > 0$ the unique solution is $\alpha_{\seq{s}} =
\argmin_{\alpha\in\mathbb{R}^N} J_{\lambda}(\alpha) = (\mathbf{K}/N + \lambda
I_N)^{-1}(y_i)_{i=1}^N$. This is an $O\left(N^3\right)$ algorithm.
\paragraph{}
Another way of describing positive definite kernels and \acs{RKHS} consists in
Then any functions in $\mathcal{H}_k$ can be written $f(x)=\inner{\phi(x),
\theta}_{\mathcal{H}}$ In a nutshell the function $\phi$ is called feature map
because it \say{extracts characteristic elements from a vector}. Usually a
feature map takes a vector in an input space with low dimension and maps it to
a higher dimensional space. Put it differently, any function in $\mathcal{H}_k$
is the composition of linear functional $\theta^\adjoint$ with a non linear
feature map $\phi$. Thus if the feature map $\phi$ is fixed (which is
equivalent to fixing the kernel), it is possible to \say{learn} with a linear
class of function $\theta\in\mathcal{H}$ (see \cref{fig:feature_map}).
\begin{figure}
    %\centering\resizebox{\textwidth}{!}{%
    %\begin{tikzpicture}
        %\node[inner sep=0pt] (input) at (0,0)
            %{\includegraphics[width=.35\textwidth]{./gfx/input.eps}};
        %\node[inner sep=0pt] (feature) at (5,-6)
            %{\includegraphics[width=.35\textwidth]{./gfx/feature.eps}};
        %\draw[->,thick] (input.east) -- (feature.west)
            %node[midway,fill=white] {$\phi:\mathcal{X} \to \mathcal{H}$};
    %\end{tikzpicture}}
    \centering
    \begin{tabular}{c}
        \includegraphics[valign=m, width=.5\textheight]{./gfx/input.eps} \\
        $\xdownarrow{2cm} \phi: \enskip \mathcal{X} = \mathbb{R}^2 \to
        \mathcal{H} = \mathbb{R}^3$ \\
        \includegraphics[valign=m, width=.5\textheight]{./gfx/feature.eps}
    \end{tabular}
    \caption[A scalar-valued feature map]{We map the two circles in
    $\mathbb{R}^2$ to $\mathbb{R}^3$. In $\mathbb{R}^3$ it is now possible to
    separate the circles with a linear functional: a plane. We used the feature
    map \\ $\phi(x) = 3.46 \begin{pmatrix} \cos(1.76 x_1 + 2.24 x_2 + 2.75) \\
    \cos(0.40 x_1 + 1.87 x_2 + 5.6) \\ \cos(0.98 x_1 - 0.98 x_2 + 6.05)
    \end{pmatrix}$. \label{fig:feature_map}}
\end{figure}
If we note
\begin{dmath*}
    \boldsymbol{\phi} =
    \begin{pmatrix}
        \phi(x_1) & \dots & \phi(x_N)
    \end{pmatrix}
\end{dmath*}
the \say{matrix} where each column represents the feature map evaluated at the
point $x_i$ with $1 \le i \le N$, the the regularized risk minimization with
the least square loss reads
\begin{dmath*}
    \mathcal{J}_{\lambda} = \frac{1}{2N}\norm{\boldsymbol{\phi}^\transpose
    \theta - (y_i)_{i=1}^N }_{fro}^2 + \frac{\lambda}{2}\norm{\theta}_2^2.
\end{dmath*}
and the unique solution is $\theta_{\seq{s}} =
\left(\boldsymbol{\phi}\boldsymbol{\phi}^\transpose/N + \lambda
I_{\mathcal{H}}\right)^{-1}\boldsymbol{\phi} (y_i)_{i=1}^N$. This is an
$O\left( \dim(\mathcal{H})^2(N + \dim{\mathcal{H}}) \right)$. This algorithm
seems more appealing than its kernel counterpart when many data are given since
once the space $\mathcal{H}$ has been fixed, the algorithm is linear in the
number of training points. However many questions remains. First although it is
possible to design a feature map \emph{ex nihilo}, can we design systematically
a feature map from a kernel? For some kernels (\acs{eg} the gaussian kernel) it
is well known that the Hilbert space corresponding to it has dimension
$\dim(\mathcal{H}) = \infty$. Is it possible to find an approximation of the
kernel such that $\dim(\mathcal{H}) < \infty$? If such a construction is
possible and we know that $N$ training data are available, is it possible to
have a sufficiently good approximation with $\dim(\mathcal{H}) \ll
N$\footnote{When $\dim(\mathcal{H}) \ge N$ then is it is better to use the
kernel algorithm than the feature algorithm. This is called the kernel trick.}?

\subsection{Towards large scale learning with kernels}
Motivated by large scale applications, different methodologies have been
proposed to approximate kernels and feature maps. This subsection briefly
reminds the main approaches based on  Random Fourier Features and Nystr\"om
techniques. Notice that another line of research concerns online learning
method such as \acs{NORMA} developed in \cite{kivinen2004online}, later
extended to the operator-valued kernel case by \citet{audiffren2013online}.  We
start with the seminal work of \citet{Rahimi2007} who show that given a
continuous shift-invariant kernel ($\forall x, z, t \in \mathcal{X}$, $k(x + t,
z + t) = k(x, z)$), it is possible to obtain a feature map called \acs{RFF}
that approximate the given kernel.
\subsubsection{Random Fourier Feature maps}
Random Fourier Feature methodology introduced  by \citet{Rahimi2007} provides a
way to scale up kernel methods when kernels are Mercer and
\emph{translation-invariant}.  We view the input space $\mathcal{X}$ as a group
endowed with the addition. Extensions to other group laws such as
\citet{li2010random} are described in \cref{subsubsec:skewedchi2} within the
general framework of operator-valued kernels.
\paragraph{}
Denote $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ a positive
definite kernel on $\mathbb{R}^d$. A kernel $k$ is said to be
\emph{shift-invariant} or \emph{translation-invariant} for the addition if for
for all $(x,z,t) \in \left(\mathbb{R}^d\right)^3$ we have $k(x+t,z+t) =
k(x,z)$.  Then, we define $k_0: \mathbb{R}^d \to \mathbb{R}$ the function such
that $k(x,z)= k_0(x-z)$. $k_0$ is called the \emph{signature} of kernel $k$.
Bochner's theorem \citep{folland1994course} is the theoretical result that
leads to the Random Fourier Features.
\begin{theorem}[Bochner's theorem]\label{th:bochner-scalar}
    Any continuous positive definite complex function is the \acl{FT} of a
    non-negative measure.
\end{theorem}
It implies that any positive definite, continuous and shift-invariant kernel
$k$, have a continuous and positive definite signature $k_0$, which is the
\acl{FT} $\mathcal{F}$ of a non-negative measure $\mu$. We therefore have the
following corollary.
\begin{corollary}\label{c:bochner-app}
    With the previous notations and assumptions on $k$,
    \begin{dmath}\label{bochner-scalar}
        k(x,z)=k_0(x-z) \hiderel{=} \int_{\mathbb{R}^d} \exp(-\iu
        \inner{\omega,x - z}) d\mu(\omega)
        =\FT{k_0}(\omega).
    \end{dmath}
\end{corollary}
Moreover $\mu = \IFT{k_0}$.  Without loss of generality, we assume that $\mu$
is a probability measure, \acs{ie} $\int_{\mathbb{R}^d} d\mu(\omega)=1$ by
renormalizing the kernel since
\begin{dmath*}
    \int_{\mathbb{R}^d}d\mu(\omega)= \int_{\mathbb{R}^d}\exp(-\iu
    \inner{\omega, 0})d\mu(\omega)\hiderel{=}k_0(0).
\end{dmath*}
and we can write \cref{bochner-scalar} as an expectation over $\mu$. For all
$x$,
$z\in\mathbb{R}^d$
\begin{dmath*}
    k_0(x-z) = \expectation_{\omega\sim\mu}\left[\exp(-\iu \inner{\omega,x -
    z})\right].
\end{dmath*}
Eventuallt, if $k$ is real valued we only write the real part,
\begin{dmath*}
    k(x,z) = \expectation_{\omega\sim\mu}[\cos \inner{\omega,x - z}] =
    \expectation_{\omega\sim\mu}[ \cos \inner{\omega,z} \cos \inner{\omega,x} +
    \sin \inner{\omega,z} \sin \inner{\omega,x}].
\end{dmath*}
Let $\Vect_{j=1}^D x_j$ denote the $Dd$-length column
vector obtained by stacking vectors $x_j \in \mathbb{R}^d$.  The feature map
$\widetilde{\phi}: \mathbb{R}^d \rightarrow \mathbb{R}^{2D}$ defined as
\begin{dmath}
\label{eq:rff}
    \widetilde{\phi}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos{\inner{x,\omega_j}} \\
        \sin{\inner{x,\omega_j}}
    \end{pmatrix}\condition{$\omega_j \hiderel{\sim} \IFT{k_0}$ \acs{iid}}
\end{dmath}
is called a \emph{Random Fourier Feature} (map). Each $\omega_{j}, j=1, \ldots,
D$ is independently and identically sampled from the inverse Fourier transform
$\mu$ of $k_0$. This Random Fourier Feature map provides the following
Monte-Carlo estimator of the kernel: $\widetilde{k}(x, z) =
\widetilde{\phi}(x)^* \widetilde{\phi}(z)$. Using trigonometric identities,
\citet{Rahimi2007} showed that the same feature map can also be written
\begin{dmath}
    \label{eq:rff2}
    \tilde{\phi}(x)=\frac{2}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos(\inner{x,\omega_j} + b_j)
    \end{pmatrix},
\end{dmath}
where $\omega_j \hiderel{\sim} \IFT{k_0}$, $b_j \sim \mathcal{U}(0, 2\pi)$
\acs{iid}.  The feature map defined by \cref{eq:rff} and \cref{eq:rff2} have
been compared in \citet{sutherland2015} where they give the condition under
wich \cref{eq:rff} has lower variance than \cref{eq:rff2}. For instance for the
gaussian kernel, \cref{eq:rff} has always lower variance. In practice,
\cref{eq:rff2} is easier to program. In this manuscript we focus on random
Fourier feature of the form given in \cref{eq:rff}.

\paragraph{}
The dimension $D$ governs the precision of this
approximation, whose uniform convergence towards the target kernel (as defined
in \cref{bochner-scalar}) can be found in \citet{Rahimi2007} and in more recent
papers with some refinements proposed in \citet{sutherland2015} and
\citet{sriper2015}.  Finally, it is important to notice that Random Fourier
Feature approach \emph{only} requires two steps before the application of a
learning algorithm: (1) define the inverse Fourier transform of the given
shift-invariant kernel, (2) compute the randomized feature map using the
spectral distribution $\mu$.  \citet{Rahimi2007} show that fGor the Gaussian
kernel $k_0(x-z) = \exp(-\gamma \norm{x - z}_2^2)$, the spectral distribution
$\mu$ is a Gaussian distribution. For the Laplacian kernel $k_0(x-z) =
exp(-\gamma \norm{x - z}_1)$, the spectral distribution is a Cauchy
distribution.

\subsubsection{Extensions of the RFF method}
\paragraph{}
The seminal idea of \citet{Rahimi2007} has open a large literature on random
features. Nowadays, many classes of kernels other than translation invariant
are now proved to have an efficient random feature representation.
\citet{kar2012random} proposed random feature maps for dot product kernels
(rotation invariant) and \citet{hamid2014compact} improved the rate of
convergence of the approximation error for such kernels by noticing that
feature maps for dot product kernels are usually low ranki and may not utilize
the capacity of the projected feature  space  effectively. \Citet{pham2013fast}
proposed fast random feature maps for polynomial kernels.
\paragraph{}
\Citet{li2010random} generalized the original \acs{RFF} of \citet{Rahimi2007}.
Instead of computing feature maps for shift-in\-va\-riant kernels on the
additive group $(\mathbb{R}^d, +)$, they used the generalized Fourier transform
on any locally compact abelian group to derive random features on the
multiplicative group $(\mathbb{R}^d, *)$. In the same spirit
\citet{yang2014random} noticed that an theorem equivalent to Bochner's theorem
exists on the semi-group $(\mathbb{R}_{>0}^d, +)$. From this they derived
\say{Random Laplace} features and used them to approximate kernels adapted to
learn on histograms.
\paragraph{}
To speed-up the convergence rate of the random features approximation,
\citet{yang2014quasi} proposed to sample the random variable from a quasi
Monte-Carlo sequence instead of \acs{iid}~random variables. \Citet{Le2013}
proposed the \say{Fastfood} algorithm to reduce the complexity of computing a
\acs{RFF} --using structured matrices and a fast Walsh-Hadarmard transform--
from $O(Dd)$ to $O(D\log(d))$. More recently \citet{felix2016orthogonal}
proposed also an algorithm \say{SORF} to compute Gaussian \acs{RFF} in
$O(D\log(d))$ but with better convergence rates than \say{Fastfood}
\citep{Le2013}.  \Citet{mukuta2016kernel} proposed a data dependent features
map (comparable to the Nystro\"m method) by estimating the distribution of the
input data, and then finding the eigenfunction decomposition of Mercer's
integral operator associated to the kernel.
\paragraph{}
In the context of large scale learning and deep learning, \citet{lu2014scale}
showed that \acsp{RFF} can achieve performances comparable to deep-learning
methods by combining multiple kernel learning and composition of kernels along
with a scalable parallel implementation. \Citet{dai2014scalable} and
\citet{xie2015scale} combined \acsp{RFF} and stochastic gradient descent to
defined an online learning algorithm called \say{Doubly stochastic gradient
descent} adapted to large scale learning. \Citet{yang2015deep} proposed and
studied the idea of replacing the last fully interconnected layer of a deep
convolutional neural network \citep{lecun1995convolutional} by the
\say{Fastfood} implementation of \acsp{RFF}.
\paragraph{}
Eventually \citet{Yang2015} introduced the algorithm \say{\`A la Carte}, based
on \say{Fastfood} which is able to learn the spectral distribution

\subsection{Locally compact Abelian groups}
\begin{definition}[\acf{LCA} group.]
    A group $\mathcal{X}$ endowed with a binary operation $\groupop$ is said to
    be a Locally Compact Abelian group if $\mathcal{X}$ is a topological
    \emph{commutative} group \acs{wrt}~$\groupop$ for which every point has a
    compact neighborhood and is Hausdorff (T2).
\end{definition}
Moreover given a element $z$ of a \ac{LCA} group $\mathcal{X}$, we define the
set $z\groupop\mathcal{X}=\mathcal{X}\groupop z=\Set{z\groupop x|\forall
x\in\mathcal{X}}$ and the set $\mathcal{X}^{-1}=\Set{x^{-1}|\forall
x\in\mathcal{X}}$.  We also note $e$ the neutral element of $\mathcal{X}$ such
that $x\groupop e=e \groupop x= e$ for all $x\in\mathcal{X}$.  Throughout this
paper we focus on positive definite function. Let $\mathcal{Y}$ be a complex
separable Hilbert space. A function $f:\mathcal{X}\to\mathcal{Y}$ is positive
definite if for all $N\in\mathbb{N}$ and all $y\in\mathcal{Y}$,
\begin{dmath}
    \label{eq:positive_definite} \sum_{i,j=1}^N\inner*{y_i,
    f\left(x_j^{-1}\groupop x_i\right)y_j}_{\mathcal{Y}}\ge 0
\end{dmath}
for all sequences $(y_i)_{i\in\mathbb{N}_N^*}\in\mathcal{Y}^N$ and all sequences
$(x_i)_{i\in\mathbb{N}_N^*}\in\mathcal{X}^N$. If $\mathcal{Y}$ is real we add
the assumption that $f(x^{-1})=f(x)^*$ for all $x\in\mathcal{X}$

\subsection{Even and odd functions}
Let $\mathcal{X}$ be a \ac{LCA} group and $\mathbb{K}$ be a field viewed as an
additive group. We say that a function $f:\mathcal{X}\to\mathbb{K}$ is even if
for all $x\in\mathcal{X}$, $f(x)=f\left(\inv{x}\right)$ and odd if
$f(x)=-f\left(\inv{x}\right)$. The definition can be extended to
operator-valued functions.
\begin{definition}[Even and odd operator-valued function on a \ac{LCA} group]
    Let $\mathcal{X}$ be a measured \ac{LCA} group and $\mathcal{Y}$ be a
    Hilbert space, and $\mathcal{L}(\mathcal{Y})$ the space of bounded linear
    operators from $\mathcal{Y}$ to itself viewed as an additive group. A
    function $f:\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ is (weakly) even if for
    all $x\in\mathcal{X}$ and all $y$, $y'\in\mathcal{Y}$,
    \begin{dmath}
        \inner{y,f\left(\inv{x}\right)y'}_{\mathcal{Y}} =
        \inner{y,f(x)y'}_{\mathcal{Y}}
    \end{dmath}
    and (weakly) odd if
    \begin{dmath}
        \inner{y,f\left(\inv{x}\right)y'}_{\mathcal{Y}} =
        -\inner{y,f(x)y'}_{\mathcal{Y}}
    \end{dmath}
\end{definition}
It is easy to check that if $f$ is odd then
$\int_{\mathcal{X}}\inner{y,f(x)y'}_{\mathcal{Y}}d\Haar(x)=0$.  Besides the
product of an even and an odd function is odd. Indeed for all $f$,
$g\in\mathcal{F}(\mathcal{X};\mathcal{L}(\mathcal{Y}))$, where $f$ is even and
$g$ odd. Define $h(x)=\inner{y,f(x)g(x)y'}$. Then we have
\begin{dmath}
    h\left(\inv{x}\right) = \inner{y, f\left(\inv{x}\right)
    g\left(\inv{x}\right)y'}_{\mathcal{Y}}
    \hiderel{=}\inner{y,f(x)\left(-g(x)\right)y'}_{\mathcal{Y}}
    =-h(x).
\end{dmath}
\subsection{Characters}
\label{subsec:character} \acf{LCA} groups are central to the general definition
of Fourier Transform which is related to the concept of Pontryagin
duality~\citep{folland1994course}.  Let $(\mathcal{X}, \groupop)$ be a \ac{LCA}
group with $e$ its neutral element and the notation, $\inv{x}$, for the inverse
of $x \in \mathcal{X}$. A \emph{character} is a complex continuous homomorphism
$\omega:\mathcal{X}\to\mathbb{U}$ from $\mathcal{X}$ to the set of complex
numbers of unit module $\mathbb{U}$. The set of all characters of $\mathcal{X}$
forms the Pontryagin \emph{dual  group} $\dual{\mathcal{X}}$. The dual group of
an \ac{LCA} group is an \ac{LCA} group so that we can endow
$\dual{\mathcal{X}}$ with a \say{dual} Haar measure noted $\dual{\Haar}$. Then
the dual group operation is defined by
\begin{dmath*}
    (\omega_1 \groupop' \omega_2)(x)=\omega_1(x)\omega_2(x) \hiderel{\in}
    \mathbb{U}.
\end{dmath*}
\paragraph{}
The Pontryagin duality theorem states that $\dual{\dual{\mathcal{X}}}\cong
\mathcal{X}$. \acs{ie}~there is a canonical isomorphism between any \ac{LCA}
group and its double dual. To emphasize this duality the following notation is
usually adopted
\begin{dmath}
    \label{eq:paringdef} \omega(x) = \pairing{x, \omega} \hiderel{=}
    \pairing{\omega, x} \hiderel{=} x(\omega),
\end{dmath}
where $x\in\mathcal{X}\cong\dual{\dual{\mathcal{X}}}$ and
$\omega\in\dual{\mathcal{X}}$. The form $\pairing{\cdot,\cdot}$ defined in
\cref{eq:paringdef} is called (duality) pairing. Another important property
involves the complex conjugate of the pairing which is defined as
\begin{dmath}
    \conj{\pairing{x, \omega}} = \pairing*{\inv{x}, \omega} \hiderel{=}
    \pairing*{x, \inv{\omega}}.
\end{dmath}
\begin{table}[htb]
    \caption{Classification of \acl{FT}s in terms of their domain and transform
    domain.}
    \label{tab:dual_and_pairing}
    \centering
    \begin{tabularx}{\textwidth}{cccX}
        \toprule
            \multicolumn{1}{c}{$\mathcal{X}=$} &
            \multicolumn{1}{c}{$\dual{\mathcal{X}}\cong$} &
            \multicolumn{1}{c}{Operation} & \multicolumn{1}{l}{Pairing} \\
        \cmidrule{1-4}
            $\mathbb{R}^d$ & $\mathbb{R}^d$ & $+$ & $\pairing{x,\omega} =
            \exp\left(\iu \inner{x, \omega}_2\right)$ \\ $\mathbb{R}^d_{*,+}$ &
            $\mathbb{R}^d$ & $\cdot$ & $\pairing{x,\omega} =\exp\left( \iu
            \inner{\log(x), \omega}_2 \right)$ \\ $(-c;+\infty)^d$ &
            $\mathbb{R}^d$ & $\odot$ & $\pairing{x,\omega} =\exp\left( \iu
            \inner{\log(x+c), \omega}_2 \right)$ \\
        \bottomrule
    \end{tabularx}
\end{table}
\paragraph{}
We notice that for any pairing depending of $\omega$, there exists a function
$h_{\omega}: \mathcal{X} \to \mathbb{R}$ such that $(x,\omega)= \exp(\iu
h_{\omega}(x))$ since any pairing maps into $\mathbb{U}$. Moreover,
\begin{dmath*}
    \pairing*{x \groupop \inv{z},\omega} = \omega(x)\omega\left(\inv{z}\right)
    =\exp\left(+\iu h_{\omega}\left(x\right)\right)\exp\left(+\iu
    h_{\omega}\left(\inv{z}\right)\right) =\exp\left(+\iu
    h_{\omega}\left(x\right)\right)\exp\left(-\iu
    h_{\omega}\left(z\right)\right).
\end{dmath*}
%The following example shows how to determine the (Pontryagin) dual of a
%\ac{LCA} group.
%\begin{example}[\citet{folland1994course}]
    %\label{ex:additive_group_lca} On the additive group
    %$\mathcal{X}=(\mathbb{R},+)$ we have $\dual{\mathbb{R}}\cong\mathbb{R}$
    %with the duality pairing $\pairing{x,\omega}=\exp\left(\iu x\omega\right)$
    %for all $x\in\mathbb{R}$ and all $\omega\in\mathbb{R}$. The Haar measure on
    %$\mathcal{X}$ is the Lebesgue measure.
%\end{example}
%We also especially mention the duality pairing associated to the skewed
%multiplicative \ac{LCA} product group. This group together with the operation
%$\odot$ has  been proposed by~\citet{li2010random} to handle histograms
%features especially useful in image recognition applications. Let
%$\mathcal{X}=(-c_k;+\infty)_{k=1}^d$, where $c_k\in\mathbb{R}_+$, endowed with
%the group operation $\odot$ defined component-wise for all $x$,
%$z\in\mathcal{X}$ as follow.
%\begin{dmath*}
    %x \odot z \colonequals ((x_k+c_k)(z_k+c_k) - c_k)_{k=1}^d.
%\end{dmath*}
%\begin{example}[\citet{li2010random}]
    %On the skewed multiplicative group $\mathcal{X}=((-c,+\infty), \odot)$ we
    %have $\dual{\mathbb{(-c,+\infty)}}\cong\mathbb{R}$, with duality pairing
    %$\pairing{x,\omega}=\exp(\iu\log(x+c)\omega)$ for all $x\in\mathcal{X}$ and
    %all $\omega\in\dual{\mathcal{X}}$. The Haar measure on $\mathcal{X}$ is
    %given for all $\mathcal{Z}\in\mathcal{B}(\mathcal{X})$ by
    %$\Haar(\mathcal{X})=\int_{\mathcal{Z}}(z+c)^{-1}d\Leb(z)$.
%\end{example}
%It is easy to extend the pontryagin dual of groups to dual groups, as well as
%defining the pairing on the dual group using the following
%proposition~\citep{folland1994course}
%\begin{proposition}
    %Let $(\mathcal{X}_i)_{i\in\mathbb{N}}$ be a collection of \ac{LCA} groups.
    %Then
    %\begin{dmath*}
        %\dual{\left(\prod_{i\in\mathbb{N}} \mathcal{X}_i\right)} \cong
        %\prod_{i\in\mathbb{N}} \dual{\mathcal{X}_i}
    %\end{dmath*}
%\end{proposition}
%Hence $\dual{\mathbb{R}^d}\cong\mathbb{R}^d$ with duality pairing
%\begin{dmath*}
    %\pairing{x,\omega}=\exp\left(\iu\sum_{k=1}^d x_k\omega_k \right),
%\end{dmath*}
%hence $h_\omega(x)=\sum_{k=1}^d\omega_k x_k=\inner{x,\omega}_2$. For the
%skewed multiplicative group $\dual{(-c_k;+\infty)_{k=1}^d}\cong\mathbb{R}^d$
%and the duality pairing is defined by
%\begin{dmath*}
    %\label{eq:pairing-skewed} \pairing{x, \omega} =
    %\exp\left(\iu\sum_{k=1}^d\log(x_k+c_k)\omega_k\right).
%\end{dmath*}
%Hence $h_\omega(x)=\sum_{k=1}^d
%\log(x_k+c_k)\omega_k=\inner{\log(x+c),\omega}_2$. Eventually the natural Haar
%measure on a product group is the product measure. \acs{eg}~for
%$\mathcal{X}=\mathbb{R}^d$, the Haar measure on $\mathbb{R}^d$ is the d-th
%\power of the Lebesgue measure on $\mathbb{R}$.
\Cref{tab:dual_and_pairing}
provides an explicit list of pairings for various groups based on
$\mathbb{R}^d$ or its subsets. The interested reader can refer
to~\citet{folland1994course} for a more detailed construction of \ac{LCA},
Pontryagin duality and \acl{FT}s on \ac{LCA}.

\subsection[The Fourier Transform]{The \acl{FT}}
For a function with values in a separable Hilbert space, $f\in
L^1(\mathcal{X},\Haar;\mathcal{Y})$, we denote $\FT{f}$ its \acf{FT} which is
defined by
\begin{dmath*}
        \forall \omega \in \dual{\mathcal{X}},\enskip \FT{f}(\omega)
        \hiderel{=}\int_{\mathcal{X}} \conj{\pairing{x,\omega}}f(x)d\Haar(x).
\end{dmath*}
The \acf{IFT} of a function $g\in L^1(\dual{\mathcal{X}},\dual{\Haar};
\mathcal{Y})$ is noted $\IFT{g}$ defined by
\begin{dmath*}
    \forall x \in \mathcal{X},\enskip \IFT{g}(x)
    \hiderel{=}\int_{\dual{\mathcal{X}}} \pairing{x,\omega}g(\omega)
    d\dual{\Haar}(\omega),
\end{dmath*}
We also define the flip operator $\mathcal{R}$ by $(\mathcal{R}f)(x)
\colonequals f\left(\inv{x}\right)$.
\begin{theorem}[Fourier inversion]
    \label{th:fourier_inversion} Given a measure $\Haar$ defined on
    $\mathcal{X}$, there exists a unique suitably normalized dual measure
    $\dual{\Haar}$ on $\dual{\mathcal{X}}$ such that for all $f \in
    L^1(\mathcal{X}, \Haar;\mathcal{Y})$ and if $\FT{f} \in
    L^1(\dual{\mathcal{X}}, \dual{\Haar}; \mathcal{Y})$ we have
    \begin{dmath}
        \label{fourier-l1} f(x) \hiderel{=} \int_{\dual{\mathcal{X}}}
        \pairing{x, \omega} \FT{f}(\omega) d\dual{\Haar}(\omega) \condition{for
        $\Haar$-almost all $x\in \mathcal{X}$.}
    \end{dmath}
    \acs{ie}~such that
    $(\mathcal{R}\mathcal{F}\FT{f})(x)=\mathcal{F}^{-1}\FT{f}(x)=f(x)$ for
    $\Haar$-almost all $x\in\mathcal{X}$. If $f$ is continuous this relation
    holds for all $x\in\mathcal{X}$.
\end{theorem}
Thus when a Haar measure $\Haar$ on $\mathcal{X}$ is given, the measure on
$\dual{\mathcal{X}}$ that makes \cref{th:fourier_inversion} true is called the
dual measure of $\Haar$, noted $\dual{\Haar}$. Let $c\in\mathbb{R}_*$ If
$c\Haar$ is the measure on $\mathcal{X}$, then $c^{-1}\dual{\Haar}$ is the dual
measure on $\dual{\mathcal{X}}$. Hence one must replace $\dual{\Haar}$ by
$c^{-1}\dual{\Haar}$ in the inversion formula to compensate. Therefore,
\emph{we always take the Haar measure $\dual{\Haar}$ on $\dual{\mathcal{X}}$ to
be the dual of the given Haar measure $\Haar$ on $\mathcal{X}$}. Whenever
$\dual{\Haar}=\Haar$ we say that the Haar measure is self-dual. For the
familiar case of a scalar-valued function $f$ on the \ac{LCA} group
$(\mathbb{R}^d, +)$, we have for all $\omega\in
\dual{\mathcal{X}}=\mathbb{R}^d$
\begin{dmath}
    \label{fourier-R-plus}
    \FT{f}(\omega)
    =\int_{\mathcal{X}} \conj{\pairing{x,\omega}}f(x)d\Haar(x)
    =\int_{\mathbb{R}^d} \exp(-\iu \inner{x,\omega}_2)f(x) d\Leb(x),
\end{dmath}
the Haar measure being here the Lebesgue measure. Notice that the normalization
factor of $\dual{\Haar}$ on $\dual{\mathcal{X}}$ depends on the measure $\Haar$
on $\mathcal{X}$ \emph{and} the duality pairing. For instance let
$\mathcal{X}=(\mathbb{R}^d, +)$. In \cref{ex:additive_group_lca} we showed that
$\dual{\mathcal{X}}\cong\mathbb{R}^d$ with pairing
$\pairing{x,\omega}=\exp(\iu\inner{ x,\omega}_2)$, for all $x\in\mathcal{X}$
and $\omega\in\dual{\mathcal{X}}$.  If one endow $\mathcal{X}$ with the
Lebesgue measure as the Haar measure, the Haar measure on the dual is defined
for all $\mathcal{Z}\in\mathcal{B}(\mathbb{R}^d)$ by
\begin{dmath*}
    \Haar(\mathcal{Z})\hiderel{=}\Leb(\mathcal{Z}),
    \quad\text{and}\quad
    \dual{\Haar}(\mathcal{Z})\hiderel{=}\frac{1}{(2\pi)^d}\Leb(\mathcal{Z}),
\end{dmath*}
in order to have $\mathcal{F}^{-1}\FT{f}=f$. If one use the cleaner equivalent
pairing $\pairing{x,\omega}=\exp(2\iu\pi \inner{x, \omega}_2)$ rather than
$\pairing{x,\omega}=\exp(\iu \inner{x,\omega}_2)$, then
\begin{dmath*}
    \dual{\Haar}(\mathcal{Z})=\Leb(\mathcal{Z}).
\end{dmath*}
The pairing $\pairing{x,\omega}=\exp(2\iu\pi \inner{x,\omega}_2)$ looks more
attractive in theory since it limits the messy factor outside the integral sign
and make the Haar measure self-dual. However it is of lesser use in practice
since it yields additional unnecessary computation when evaluating the pairing.
Hence for symmetry reason on $(\mathbb{R}^d, +)$ and reduce computations we
settle with the Haar measure on $\mathbb{R}^d$ groups (additive and
multiplicative) defined as
\begin{dmath*}
    \dual{\Haar}(\mathcal{Z})
    = \Haar(\mathcal{Z})
    \hiderel{=} \frac{1}{\sqrt{2\pi}^d} \Leb(\mathcal{Z}).
\end{dmath*}
We conclude this subsection by recalling the injectevity property of the
\acl{FT}.
\begin{corollary}[\acl{FT} injectivity]
    Given $\mu$ and $\nu$ two measures, if $\FT{\mu}=\FT{\nu}$ then $\mu=\nu$.
    Moreover given two functions $f$ and $g\in
    L^1(\mathcal{X},\Haar;\mathcal{Y})$ if $\FT{f}=\FT{g}$ then $f=g$
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{On Operator-Valued Kernels}
\label{sec:background_on_operator-valued_kernels} We now introduce the theory
of \acf{vv-RKHS} that provides a flexible framework to study and learn
vector-valued functions. The fundations of the general theory of scalar kernel
is mostly due to \citet{Aronszajn1950}  and provides a unifying point of view
for the study of an important class of Hilbert spaces of real or complex valued
functions. It has been first applied in the theory of partial differential
equation. The theory of \acfp{OVK} which extends the scalar-valued kernel was
first developped by \citet{Pedrick57} in his Ph.~D Thesis. Since then it has
been successfully applied to machine learning by many authors. In particular we
introduce the notion of \aclp{OVK} following the propositions of
\citet{Micchelli2005,carmeli2006vector,Carmeli2010}.
\subsection{Definitions and properties}
\label{subsec:def_properties} In machine learning the goal is often to find a
function $f$ belonging to a space of function
$\mathcal{F}(\mathcal{X};\mathcal{Y})$ that minimizes some criterion (see
\cref{sec:about_statistical_learning}). The class of functions we consider are
functions living in a Hilbert space
$\mathcal{H}\subset\mathcal{F}(\mathcal{X};\mathcal{Y})$. The completeness
allows to consider sequences of functions $f_n \in\mathcal{H}$ where the limit
$f_n\to f$ is in $\mathcal{H}$. Moreover the existence of an inner product
gives rise to a norm and also makes $\mathcal{H}$ a metric space.
\paragraph{}
Among all these functions $f\in\mathcal{H}$, we consider a subset of functions
$f\in\mathcal{H}_K\subset\mathcal{H}$ such that the evaluation map
$\text{ev}_x:f\mapsto f(x)$ is bounded for all $x$. \acs{ie} such
that~$\norm{\text{ev}_x}_{\mathcal{H}_K}\le C_x\in\mathbb{R}$ for all $x$. For
scalar valued kernel the evaluation map is a linear functional. Thus by Riesz's
representation theorem there is an isomorphism between evaluating a function at
a point and an inner product: $f(x)=\text{ev}_x f = \inner{K_x, f}_K$. From
this we deduce the reproducing property $K(x,z)=\inner{K_x, K_z}_K$ which is
the cornerstone of many proofs in machine learning and functional analysis.
When dealing with vector-valued functions, the evaluation map $\text{ev}_x$ is
no longer a linear functional, since it is vector-valued. However, inspired by
the theory of scalar valued kernel, many authors showed that if the evaluation
map of functions with values in a Hilbert space $\mathcal{Y}$ is bounded, a
similar reproducing property can be obtained; namely $\inner{y',
K(x,z)y}=\inner{K_x y', K_z y}_K$ for all $y$, $y'\in\mathcal{Y}$. This
motivates the following definition of a \acf{vv-RKHS}.
\begin{definition}[\acl{vv-RKHS}~\citep{carmeli2006vector,Micchelli2005}]
    Let $\mathcal{Y}$ be a (real or complex) Hilbert space. A \acl{vv-RKHS} on
    a locally compact second countable topological space $\mathcal{X}$ is a
    Hilbert space $\mathcal{H}$ such that
    \begin{enumerate}
        \item the elements of $\mathcal{H}$ are functions from $\mathcal{X}$ to
        $\mathcal{Y}$ (\acs{ie}~$\mathcal{H} \subset \mathcal{F}(\mathcal{X},
        \mathcal{Y})$);
        \item for all $x\in\mathcal{X}$, there exists a positive constant $C_x$
        such that for all $f\in\mathcal{H}$
        \begin{dmath}
            \label{eq:RKHS_bounded}
            \norm{f(x)}_{\mathcal{Y}}\le C_x\norm{f}_{\mathcal{H}}.
        \end{dmath}
    \end{enumerate}
\end{definition}
Throughout this section we show that a \ac{vv-RKHS} defines a unique
po\-si\-ti\-ve-de\-fi\-ni\-te function called \acf{OVK} and conversely an
\ac{OVK} uniquely defines a \ac{vv-RKHS}. The bijection between \acsp{OVK} and
\acsp{vv-RKHS} has been first proved by~\citet{Senkene73} in 1973. In this
introduction to \acsp{OVK} we follow the definitions and most recent proofs
of~\citet{Carmeli2010}.
\begin{definition}[positive-definite \acl{OVK} acting on a complex Hilbert
space]
    \label{def:reproducing_kernel}
    Given $\mathcal{X}$ a locally compact second countable topological space
    and  $\mathcal{Y}$ a complex Hilbert Space, a map
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ is called an
    positive-definite \acl{OVK} kernel if
    \begin{dmath}
        \sum_{i,j=1}^N\inner{K(x_i,x_j)y_j,y_i}_{\mathcal{Y}}\ge 0,
    \end{dmath}
    for all $N\in\mathbb{N}$, for all sequences of points $(x_i)_{i=1}^N$ in
    $\mathcal{X}^N$ and all sequences of points $(y_i)_{i=1}^N$ in
    $\mathcal{Y}^N$. \label{def:ovk}
\end{definition}
If $\mathcal{Y}$ is a complex Hilbert space, a positive-definite \acl{OVK} is
always self-adjoint, \acs{ie}~$K(x,z)=K(z,x)^\adjoint$. This gives rise to the
following definition of positive definite \acl{OVK} acting on a real Hilbert
space.
\begin{definition}[positive-definite \acl{OVK} acting on a real Hilbert space]
    \label{def:reproducing_kernel_real} Given $\mathcal{X}$ a locally compact
    second countable topological space and $\mathcal{Y}$ a real Hilbert Space,
    a map $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ is called
    a positive-definite \acl{OVK} kernel if
    \begin{dmath}
        K(x,z)=K(z,x)^\adjoint
    \end{dmath}
    and
    \begin{dmath}
        \sum_{i,j=1}^N\inner{K(x_i,x_j)y_j,y_i}_{\mathcal{Y}}\ge 0,
    \end{dmath}
    for all $N\in\mathbb{N}$, for all sequences of points $(x_i)_{i=1}^N$ in
    $\mathcal{X}^N$, and all sequences of points  $(y_i)_{i=1}^N$ in
    $\mathcal{Y}^N$. \label{def:ovk_real}
\end{definition}
As in the scalar case any \acl{vv-RKHS} defines a unique positive-definite
\acl{OVK} and conversely a positive-definite \acl{OVK}defines a unique
\acl{vv-RKHS}.
\begin{proposition}[\citep{carmeli2006vector}]
    \label{pr:unique_rkhs} Given a \acl{vv-RKHS} there is a unique
    positive-definite \acl{OVK}
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$.
\end{proposition}
iven $x\in\mathcal{X}$,
$K_x:\mathcal{Y}\to\mathcal{F}(\mathcal{X};\mathcal{Y})$ denotes the linear
operator whose action on a vector $y$ is the function
$K_xy\in\mathcal{F}(\mathcal{X};\mathcal{Y})$ defined for all $z\in\mathcal{X}$
by $K_x=\text{ev}_x^\adjoint$. As a consequence we have that
\begin{dmath}
    \label{eq:trivial_feature_op}
    K(x,z)y\hiderel{=}\text{ev}_x\text{ev}_z^\adjoint y\hiderel{=}K_x^\adjoint
    K_zy\hiderel{=}(K_zy)(x).
\end{dmath}
\paragraph{}
Some direct consequences follow from the definition.
\begin{enumerate}
    \item The kernel reproduces the value of a function $f\in\mathcal{H}$ at a
    point $x\in\mathcal{X}$ since for all $y\in\mathcal{Y}$ and
    $x\in\mathcal{X}$,
    $\text{ev}_x^\adjoint y=K_xy=K(\cdot,x)y$ so that
    \begin{dmath}
        \label{eq:reproducing_prop} \inner{f(x),y}_{\mathcal{Y}}
        \hiderel{=}\inner{f,K(\cdot, x)y}_{\mathcal{H}}
        \hiderel{=}\inner{K_x^*f,y}_{\mathcal{Y}}.
    \end{dmath}
    \item The set $\Set{K_xy| \forall x\in\mathcal{X}, \forall
    y\in\mathcal{Y}}$ is total in $\mathcal{H}$. Namely,
    \begin{dmath*}
        \left(\bigcup_{x\in\mathcal{X}}\Ima K_x\right)^{\perp}=\Set{0}.
    \end{dmath*}
    If $f\in(\cup_{x\in\mathcal{X}}\Ima K_x)^{\perp}$, then for all
    $x\in\mathcal{X}$, $f\in(\Ima K_x)^{\perp}=\Ker K_x^\adjoint$, hence
    $f(x)=0$ for all $x\in\mathcal{X}$ that is $f=0$.  \item Finally for all
    $x\in\mathcal{X}$ and all $f\in\mathcal{H}$, $\norm{f(x)}_{\mathcal{Y}}\le
    \sqrt{\norm{K(x,x)}_{\mathcal{Y},\mathcal{Y}}}\norm{f}_{\mathcal{H}}$. This
    comes from the fact that $\norm{K_x}_{\mathcal{Y} ,\mathcal{H}} =
    \norm{K_x^\adjoint}_{\mathcal{H}, \mathcal{Y} }=
    \sqrt{\norm{K(x,x)}_{\mathcal{Y}, \mathcal{Y}}}$ and the operator norm is
    sub-multiplicative.
\end{enumerate}
Additionally given a positive-definite \acl{OVK}, it defines a unique
\ac{vv-RKHS}.
\begin{proposition}[\citep{carmeli2006vector}]
    Given a positive-definite \acl{OVK}
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$, there is a
    unique \acl{vv-RKHS} $\mathcal{H}$ on $\mathcal{X}$ with reproducing kernel
    $K$.
\end{proposition}
Since an positive-definite \acl{OVK} defines a unique \acf{vv-RKHS} and
conversely a \ac{vv-RKHS} defines a unique \acl{OVK}, we denotes the Hilbert
space $\mathcal{H}$ endowed with the scalar product $\inner{\cdot,\cdot}$
respectively $\mathcal{H}_K$ and $\inner{\cdot,\cdot}_K$. From now we refer to
positive-definite \aclp{OVK} or reproducing \aclp{OVK} as \aclp{OVK}. As a
consequence, given $K$ an
\acl{OVK}, define $K_x=K(\cdot,x)$ we have
\begin{dgroup}
    \begin{dmath}
        \label{eq:kernel_operator_product}
        K(x,z)=K^\adjoint_x K_z \enskip\forall x,z\hiderel{\in}\mathcal{X}
    \end{dmath},
    \begin{dmath}
        \label{eq:span_RKHS}
        \mathcal{H}_K=\lspan\Set{K_x y | \forall
        x\hiderel{\in}\mathcal{X},\enskip\forall y\hiderel{\in}\mathcal{Y} }.
    \end{dmath}
\end{dgroup}
Where $\lspan$ is the closed span of a given set. Another way to describe
functions of $\mathcal{H}_K$ consists in using a suitable feature map.
\begin{proposition}[Feature Operator~\citep{Carmeli2010}]
    \label{pr:feature_operator} Let $\mathcal{H}$ be any Hilbert space and
    $\Phi:\mathcal{X}\to\mathcal{L}(\mathcal{Y};\mathcal{H})$, with $\Phi_x :=
    \Phi(x)$. Then the operator $W : \mathcal{H} \to \mathcal{F}(\mathcal{X};
    \mathcal{Y})$ defined for all $g \in\mathcal{H}$, and for all
    $x\in\mathcal{X}$ by $(W g)(x) = \Phi_x^\adjoint g$ is a partial isometry
    from $\mathcal{H}$ onto the \ac{vv-RKHS} $\mathcal{H}_K$ with reproducing
    kernel
    \begin{dmath*}
        K(x,z)=\Phi^\adjoint_x\Phi_z, \enskip \forall x,
        z\hiderel{\in}\mathcal{X}.
    \end{dmath*}
    $W^\adjoint W$ is the orthogonal projection onto
    \begin{dmath*}
          (\Ker W)^\perp = \lspan\Set{\Phi_x y | \forall
          x\hiderel{\in}\mathcal{X},\enskip\forall y\hiderel{\in}\mathcal{Y}}.
    \end{dmath*}
    Then
    \begin{dmath}
        \label{eq:norm_relation_w} \norm{f}_K=\inf\Set{\norm{g}_{\mathcal{H}}
        | \forall g \in\mathcal{H},\enskip Wg=f}.
    \end{dmath}
\end{proposition}
\subsection{Some applications of Operator-valued kernels}
We give here a non exhaustive list of works concerning \aclp{OVK}.  A good
review of \aclp{OVK} has been conducted in \citet{Alvarez2012}. For a
theoretical introduction to \acsp{OVK} the interested reader can refer to the
papers \citet{carmeli2006vector, caponnetto2008, Carmeli2010}. Generalization
bounds for \acs{OVK} have been studied in \citet{Sindhwani2013,
kadri2015operator,sangnier2016joint, maurer2016vector}.
\paragraph{}
Operator-valued Kernel Regression has first been studied in the context of
Ridge Regression and Multi-task learning by \citet{Micchelli2005}.
\paragraph{}
\citet{Macedo2008, Baldassare2012} showed the interest of spectral algorithms
in Ridge regression and introduced vector field learning as a new multiple
output task in Machine Learning community. \citet{Wahlstrom2013} applied vector
field learning with \acs{OVK}-based Gaussian processes to the reconstruction of
magnetic fields (which are curl-free).
\paragraph{}
Multi-task regression \citet{micchelli2004kernels}  and structured multi-class
classification \citep{Dinuzzo2011,minh2013unifying,mroueh2012multiclass} are
undoubtedly the first target applications for working in \acl{vv-RKHS}.
\aclp{OVK} have been shown useful to provide a general framework for structured
output prediction \citep{Brouard2011,Brouard2016_jmlr}) with a link to Output
Kernel Regression \citep{Kadri_icml2013}. Beyond structured classification,
other various applications such as link prediction, drug activity prediction or
recently metabolite identification \citep{brouard2016fast} and  image
colorization \citep{ha2010image} have been developed.
\paragraph{}
The works of \citet{Kadri_aistat10,kadri2015operator} have been the precursors
of  regression with functional values, opening a new avenue of applications.
Appropriate algorithms devoted to on-line learning have been also derived  by
\citet{audiffren2013online}.
\paragraph{}
Kernel learning was addressed at least in two ways: first with using Multiple
Kernel Learning in \citet{Kadri_nips2012} and second, using various penalties,
smooth ones in \citet{Dinuzzo2011, ciliberto2015} for decomposable kernels
and non smooth ones in \citet{lim2015operator} using proximal methods in the
case of decomposable and transformable kernels.
\paragraph{}
Dynamical modeling was tackled in the context of multivariate time series
modelling in \citet{lim2013okvar,Sindhwani2013,lim2015operator} and as a
generalization of Recursive Least Square Algorithm in
\citet{amblard2015operator}.
\paragraph{}
\citet{sangnier2016joint} recently explored the minimization of
a pinball loss under regularizing constraints induced by a well chosen
decomposable kernel in order to handle joint quantile regression.
\subsection{Shift-Invariant \acs{OVK} on \acs{LCA} groups}
The main subjects of interest of
\cref{ch:operator-valued_random_fourier_features} are shift-invariant
\acl{OVK}. When referring to a shift-invariant \ac{OVK}
$K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ we assume that
$\mathcal{X}$ is a locally compact second countable topological group with
identity $e$.
\begin{definition}[Shift-invariant \ac{OVK}]
    A reproducing \acl{OVK}
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ is called
    shift-invariant\footnote{Also referred to as \emph{translation-invariant}
    \ac{OVK}.} if for all $x$, $z$, $t\in\mathcal{X}$,
    \begin{dmath}
        \label{eq:def_shift_inv}
        K(x\groupop t, z\groupop t) = K(x, z).
    \end{dmath}
\end{definition}
A shift-invariant kernel can be characterized by a function of one variable
$K_e$ called the signature of $K$. Here $e$ denotes the neutral element of the
\ac{LCA} group $\mathcal{X}$ endowed with the binary group operation
$\groupop$.
\begin{proposition}[Kernel signature~\citep{Carmeli2010}]
    \label{pr:kernel_signature} Let
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ be a
    reproducing kernel. The following conditions are equivalents.
    \begin{enumerate}
        \item \label{pr:kernel_signature_1} $K$ is a positive-definite
        shift-invariant \acl{OVK}.
        \item \label{pr:kernel_signature_2} There
        is a positive-definite function
        $K_e:\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ such that
        $K(x,z)=K_e(\inv{z}\groupop x)$.
    \end{enumerate}
    If one of the above conditions is satisfied, then
    \begin{dgroup}
        \begin{dmath}
            \label{pr:kernel_signature_4} \norm{K(x,x)}=\norm{K_e(e)} \qquad
            \forall x\hiderel{\in}\mathcal{X}
        \end{dmath}
    \end{dgroup}
\end{proposition}
The notation $K_e$ for the function of completely positive type associated with
the reproducing kernel $K$ is consistent with the definition given by
\cref{eq:trivial_feature_op} since for all $x\in\mathcal{X}$ and all
$y\in\mathcal{Y}$
\begin{dmath*}
    (K_ey)(x)=K_e(x)y.
\end{dmath*}
\subsection{Examples of \aclp{OVK}}
\label{subsec:ovk-ex}
In this subsection we list some \acfp{OVK} that have been used successfully in
the litterature. We do not recall the proof that the following kernels are well
defined are refer the interrested reader to the respective authors original
work.
\paragraph{}
\acsp{OVK} have been first introduced in Machine Learning to solve multi-task
regression problems. Multi-task regression is encountered in many fields such
as structured classification when classes belong to a hierarchy for instance.
Instead of solving independently $p$ single output regression task, one would
like to take advantage of the relationships between output variables when
learning and making a decision.
\begin{proposition}[Decomposable kernel \citep{Micheli2013}]
    \label{dec-kernel}
    Let $\Gamma$ be a non-negative operator of $\mathcal{L}_+(\mathcal{Y})$.
    $K$ is said to be a \emph{decomposable}
    kernel\footnote{Some authors also refer to as \emph{separable} kernels.} if for
    all $(x,z) \in \mathcal{X}^2$,
    \begin{dmath*}
        K(x,z) \colonequals k(x,z)\Gamma,
    \end{dmath*}
    where $k$ is a \emph{scalar} kernel.
\end{proposition}
When $\mathcal{Y}=\mathbb{R}^p$, the operator $\Gamma$ can be represented by a
matrix which can be interpreted as encoding the relationships between the
outputs coordinates.
If a graph coding for the proximity between tasks is known, then it is shown
in~\citet{Evgeniou2005,Baldassare2010,Alvarez2012} that $\Gamma$ can be chosen
equal to the pseudo inverse $L^{\dagger}$ of the graph Laplacian such that the
norm in $\mathcal{H}_K$ is a graph-regularizing penalty for the outputs
(tasks).  When no prior knowledge is available, $\Gamma$ can be set to the
empirical covariance of the output training data or learned with one of the
algorithms proposed in the literature~\citep{Dinuzzo2011, Sindhwani2013,
Lim2015}. Another interesting property of the decomposable kernel is its
universality (a kernel which may approximate an arbitrary continuous target
function uniformly on any compact subset of the input space). A reproducing
kernel $K$ is said \emph{universal} if the associated \ac{vv-RKHS}
$\mathcal{H}_K$ is \emph{dense} in the space of continuous functions
$\mathcal{C}(\mathcal{X},\mathcal{Y})$.  The conditions for a kernel to be
universal have been discussed in~\citet{caponnetto2008,Carmeli2010}. In
particular they show that a decomposable kernel is universal provided that the
scalar kernel $k$ is universal and the operator $\Gamma$ is injective.
Given $(e_k)_{k=1}^p$ a basis of $\mathcal{Y}$, we recall here how the matrix
$\Gamma$ act as a regularizer between the components of the outputs $f_k =
\inner{f(\cdot), e_k}_{\mathcal{Y}}$ of a function $f\in\mathcal{H}_K$.
\begin{proposition}[Kernels and Regularizers~\citep{Alvarez2012}]
    \label{pr:kernel_reg}
    Let $K(x,z) \colonequals k(x,z)\Gamma$ for all $x$, $z\in\mathcal{X}$ be a
    decomposable kernel where $\Gamma$ is a matrix of size $p\times p$. Then
    for all $f\in\mathcal{H}_K$,
    \begin{dmath}
        \norm{f}_K = \sum_{i,j=1}^p
        \left(\Gamma^\dagger\right)_{ij}\inner{f_i,f_j}_k
    \end{dmath}
    where $f_i=\inner{f,e_i}$ (resp $f_j=\inner{f,e_j}$), denotes the $i$-th
    (resp $j$-th) component of $f$.
\end{proposition}
We prove a generalized version of \cref{pr:kernel_reg} to any \acl{OVK} in
\cref{subsec:regularization_property}.
\paragraph{}
Curl-free and divergence-free kernels provide an interesting application of
operator-valued kernels~\citep{Macedo2008, Baldassare2012, Micheli2013} to
\emph{vector field} learning, for which input and output spaces have the same
dimensions ($d=p$). Applications cover shape deformation
analysis \citep{Micheli2013} and magnetic fields
approximations \citep{Wahlstrom2013}. These kernels discussed in
\citep{Fuselier2006} allow encoding input-dependent similarities between
vector-fields. An illustration of a synthetic $2D$ curl-free and divergence
free fields are given respectively in \cref{fig:curl-field} and
\cref{fig:div-field}. To obain the curl-free field we took the gradient of
a mixture of five two dimensional Gaussians (since the gradient of a potential
is always curl-free). We generated the divergence-free field by taking the
orthogonal of the curl-free field.
\begin{figure}
    \centering
    \includegraphics[trim=1.8cm 1cm 2cm 1cm,width=.8\textwidth,clip=true]{./gfx/curl_field.eps}
    \caption{Synthetic $2D$ curl-free field \label{fig:curl-field}.}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[trim=1.8cm 1cm 2cm 1cm,width=.8\textwidth,clip=true]{./gfx/div_field.eps}
    \caption{Synthetic $2D$ divergence-free field \label{fig:div-field}.}
\end{figure}
\begin{proposition}[Curl-free and Div-free kernel \citep{Macedo2008}]
    \label{curl-div-free}
    Assume $\mathcal{X}=(\mathbb{R}^d, +)$ and $\mathcal{Y}=\mathbb{R}^p$ with
    $d=p$. The \emph{divergence-free} kernel is defined as
    \begin{dmath*}\label{div-def}
        K^{div}(x,z)=K^{div}_0(\delta) \hiderel{=} (\nabla\nabla^\transpose  -
        \Delta I) k_0(\delta)
    \end{dmath*}
    and the \emph{curl-free} kernel as
    \begin{dmath*}
        \label{curl-def} K^{curl}(x,z) \hiderel{=} K_0^{curl}(\delta)
        =-\nabla\nabla^\transpose k_0(\delta),
    \end{dmath*}
    where $\nabla$ is the gradient operator\footnote{See
    \cref{subsec:gradient_methods} for a formal definition of the operator
    $\nabla$.}, $\nabla\nabla^\transpose $ is the Hessian operator and $\Delta$
    is the Laplacian operator.
\end{proposition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main contribution: Operator Random Fourier Features}
We present in this section a construction methodology devoted to
shift-invariant $\mathcal{Y}$-Mercer operator-valued kernels defined on any
\acf{LCA} group, noted ($\mathcal{X}, \groupop)$, for some operation noted
$\groupop$. This allows us to use the general context of Pontryagin duality for
\acl{FT} of functions on \acs{LCA} groups. Building upon a generalization of
the celebrated Bochner's theorem for operator-valued measures, an
operator-valued kernel is seen as the \emph{\acl{FT}} of an operator-valued
positive measure. From that result, we extend the principle of \acs{RFF} for
scalar-valued kernels and derive a general methodology to build \acf{ORFF} when
operator-valued kernels are shift-invariant according to the chosen group
operation. Elements of this chapter have been developped
in~\citet{brault2016random}.
\paragraph{}
We present a construction of feature maps called \acf{ORFF}, such that $f:
x\mapsto \tildePhi{\omega}(x)^\adjoint \theta$ is a continuous function that
maps an arbitrary \acs{LCA} group $\mathcal{X}$ as input space to an arbitrary
output Hilbert space $\mathcal{Y}$. First we define a functional \emph{Fourier
feature map}, and then propose a Monte-Carlo sampling from this feature map to
construct an approximation of a shift-invariant $\mathcal{Y}$-Mercer kernel.
Then, we prove the convergence of the kernel approximation
$\tilde{K}(x,z)=\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(z)$ with high
probability on \emph{compact} subsets of the \acs{LCA} $\mathcal{X}$.
Eventually we conclude with some numerical experiments.

\subsection{Theoretical study}
\label{sec:theoretical_study}
The following proposition of~\citet{Zhang2012, neeb1998operator} extends
Bochner's theorem to any shift-invariant $\mathcal{Y}$-Mercer kernel.
\begin{proposition}[Operator-valued Bochner's theorem~\citep{Zhang2012,
neeb1998operator}]
    \label{pr:operator_valued_bochner}
    If a function $K$ from $\mathcal{X} \times \mathcal{X}$ to $\mathcal{Y}$ is
    a shift-invariant $\mathcal{Y}$-Mercer kernel on $\mathcal{X}$, then there
    exists a unique positive projection-valued measure $\dual{Q}:
    \mathcal{B}(\mathcal{X}) \to
    \mathcal{L}_+(\mathcal{Y})$ such that for all $x$, $z \in \mathcal{X}$,
    \begin{dmath}
        \label{eq:bochner-gen}
        K(x, z) = \int_{\dual{\mathcal{X}}} \conj{\pairing{x \groupop \inv{z},
        \omega}} d\dual{Q}(\omega),
    \end{dmath}
    where $\dual{Q}$ belongs to the set of all the projection-valued measures
    of bounded variation on the $\sigma$-algebra of Borel subsets of
    $\dual{\mathcal{X}}$. Conversely, from any positive operator-valued measure
    $\dual{Q}$, a shift-invariant kernel $K$ can be defined by
    \cref{eq:bochner-gen}.
\end{proposition}
Although this theorem is central to the spectral decomposition of
shift-invariant $\mathcal{Y}$-Mercer \acs{OVK}, the following results proved
by~\citet{Carmeli2010} provides insights about this decomposition that are more
relevant in practice. It first gives the necessary conditions to build
shift-invariant $\mathcal{Y}$-Mercer kernel with a pair $(A, \dual{\mu})$ where
$A$ is an operator-valued function on $\dual{\mathcal{X}}$ and $\dual{\mu}$ is
a real-valued positive measure on $\dual{\mathcal{X}}$. Note that obviously
such a pair is not unique and the choice of this paper may have an impact on
theoretical properties as well as practical computations.  Secondly it also
states that any \acs{OVK} have such a spectral decomposition when $\mathcal{Y}$
is finite dimensional or $\mathcal{X}$.
\begin{proposition}[\citet{Carmeli2010}]
    \label{pr:mercer_kernel_bochner}
    Let $\dual{\mu}$ be a positive measure on
    $\mathcal{B}(\mathcal{\dual{\mathcal{X}}})$ and $A: \dual{\mathcal{X}}\to
    \mathcal{L}(\mathcal{Y})$ such that $\inner{A(\cdot)y,y'}\in
    L^1(\mathcal{X},\dual{\mu})$ for all $y,y'\in\mathcal{Y}$ and
    $A(\omega)\succcurlyeq 0$ for $\dual{\mu}$-almost all
    $\omega\in\dual{\mathcal{X}}$. Then, for all $\delta \in \mathcal{X}$,
    \begin{dmath}
        \label{eq:AK0}
        K_e(\delta)=\int_{\dual{\mathcal{X}}} \conj{\pairing{\delta, \omega}}
        A(\omega) d\dual{\mu}(\omega)
    \end{dmath}
    is the kernel signature of a shift-invariant $\mathcal{Y}$-Mercer kernel
    $K$ such that $K(x,z)=K_e(x \groupop \inv{z})$. The \acs{vv-RKHS}
    $\mathcal{H}_K$ is embed in
    $L^2(\dual{\mathcal{X}},\dual{\mu};\mathcal{Y}')$ by means of the feature
    operator
    \begin{dmath}
        \label{eq:feature_operator}
        (Wg)(x)=\int_{\mathcal{\dual{X}}} \conj{\pairing{x,\omega}} B(\omega)
        g(\omega) d\dual{\mu}(\omega),
    \end{dmath}
    Where $B(\omega)B(\omega)^\adjoint=A(\omega)$ and both integrals converge
    in the weak sense. If $\mathcal{Y}$ is finite dimensional or $\mathcal{X}$
    is compact, any shift-invariant kernel is of the above form for some pair
    $(A, \dual{\mu})$.
\end{proposition}
\paragraph{}
When $p=1$ one can always assume $A$ is reduced to the scalar $1$, $\dual{\mu}$
is still a bounded positive measure and we retrieve the Bochner theorem applied
to the scalar case (\cref{th:bochner-scalar}).
\paragraph{}
\Cref{pr:mercer_kernel_bochner} shows that a pair $(A,\dual{\mu})$ entirely
characterize an \acs{OVK}. Namely a given measure $\dual{\mu}$ and a function
$A$ such that $\inner{y', A(.)y}\in L^1(\mathcal{X},\dual{\mu})$ for all $y$,
$y'\in\mathcal{Y}$ and $A(\omega)\succcurlyeq 0$ for $\dual{\mu}$-almost all
$\omega$, give rise to an \acs{OVK}. Since $(A,\dual{\mu})$ determine a unique
kernel we can write
$\mathcal{H}_{(A,\dual{\mu})}{\scriptstyle\implies}\mathcal{H}_K$ where $K$ is
defined as in \cref{eq:AK0}. However the converse is not true: Given a
$\mathcal{Y}$-Mercer shift invariant \acl{OVK}, there exist infinitely many
pairs $(A,\dual{\mu})$ that characterize an \acs{OVK}.
\paragraph{}
The main difference between \cref{eq:bochner-gen} and
\cref{eq:AK0} is that the first one characterizes an \acs{OVK}
by a unique \acf{POVM}, while the second one shows that the \acs{POVM} that
uniquely characterize a $\mathcal{Y}$-Mercer \acs{OVK} has an operator-valued
density with respect to a \emph{scalar} measure $\dual{\mu}$; and that this
operator-valued density is not unique.
\paragraph{}
Finally \cref{pr:mercer_kernel_bochner} does not provide any
\emph{constructive} way to obtain the pair $(A,\dual{\mu})$ that characterizes
an \acs{OVK}. The following \cref{subsec:sufficient_conditions} is based on
another proposition of~\citeauthor{carmeli2006vector} and shows that if the
kernel signature $K_e(\delta)$ of an $\acs{OVK}$ is in $L^1$ then it is
possible to construct \emph{explicitly} a pair $(C,\dual{\Haar})$ from it.
Additionally, we show that we can always extract a scalar-valued
\emph{probability} density function from $C$ such that we obtain a pair
$(A,\probability_{\dual{\mu},\rho})$ where $\probability_{\dual{\mu},\rho}$ is
a \emph{probability} distribution absolutely continuous with respect to
$\dual{\mu}$ and with associated \ac{pdf}~$\rho$. Thus for all
$\mathcal{Z}\subset\mathcal{B}(\dual{\mathcal{X}})$,
\begin{dmath*}
    \probability_{\dual{\mu},\rho}(\mathcal{Z})=\int_{\mathcal{Z}}
    \rho(\omega)d\dual{\mu}(\omega).
\end{dmath*}
When the reference measure $\dual{\mu}$ is the Lebesgue measure, we note
$\probability_{\dual{\mu},\rho}=\probability_\rho$. For any function
$f:\mathcal{X}\times\dual{\mathcal{X}}\times\mathcal{Y}\to\mathbb{R}$, we also
use the notation
\begin{dmath*}
    \expectation_{\dual{\Haar}, \rho}\left[f(x, \omega, y)\right]
    =\expectation_{\omega\sim\probability_{\dual{\Haar},\rho}}\left[f(x, \omega,
    y)\right]
    =\int_{\dual{\mathcal{X}}} f(x, \omega, y) d\probability_{\dual{\Haar},
    \rho}(\omega)
    =\int_{\dual{\mathcal{X}}} f(x, \omega, y)\rho(\omega) d\dual{\Haar}(\omega).
\end{dmath*}
where the two last equalities hold by the transfer theorem and the fact that
$\probability_{\dual{\Haar}, \rho}$ has density $\rho$.

\subsubsection{Sufficient conditions of existence}
\label{subsec:sufficient_conditions}
While \cref{pr:mercer_kernel_bochner} gives some insights on how to build an
approximation of a $\mathcal{Y}$-Mercer kernel, we need a theorem that provides
an explicit construction of the pair $(A, \probability_{\dual{\mu},\rho})$ from
the kernel signature $K_e$. Proposition 14 in~\citet{Carmeli2010} gives the
solution, and also provides a sufficient condition for
\cref{pr:mercer_kernel_bochner} to apply.
\begin{proposition}[\citet{Carmeli2010}]
    \label{pr:inverse_ovk_Fourier_decomposition}
    Let $K$ be a shift-invariant $\mathcal{Y}$-Mercer kernel of signature
    $K_e$.  Suppose that for all $z \in \mathcal{X}$ and for all $y$, $y'
    \in\mathcal{Y}$, the function
    \begin{dmath*}
        \inner{K_e(.)y,y'}_{\mathcal{Y}}\in L^1(\mathcal{X},\Haar)
    \end{dmath*}
    where $\mathcal{X}$ is endowed with the group law $\groupop$. Denote $C:
    \dual{X} \to \mathcal{L}(Y)$, the function defined for all $\omega \in
    \dual{\mathcal{X}}$ that satisfies for all $y$, $y'$ in $\mathcal{Y}$:
    \begin{dmath}\label{eq:CK0}
        \inner{y',C(\omega)y}_{\mathcal{Y}}= \int_{\mathcal{X}}
        \pairing{\delta, \omega}\inner{y',
        K_e(\delta)y}_{\mathcal{Y}}d\Haar(\delta) = \IFT{\inner{y',
        K_e(\cdot)y}}_{\mathcal{Y}}(\omega).
    \end{dmath}
    Then
    \begin{enumerate}
        \item $C(\omega)$ is a bounded non-negative operator for all $\omega
        \in \dual{\mathcal{X}}$,
        \item $\inner{y, C(\cdot)y'}_{\mathcal{Y}}\in
        L^1\left(\dual{\mathcal{X}},\dual{\Haar}\right)$ for all
        $y,y'\in\mathcal{X}$,
        \item for all $\delta\in\mathcal{X}$ and for all $y$, $y'$ in
        $\mathcal{Y}$,
        \begin{dmath*}
            \inner{y', K_e(\delta)y}_{\mathcal{Y}}=
            \int_{\dual{\mathcal{X}}}\conj{\pairing{\delta,\omega}}\inner{y',
            C(\omega)y}_{\mathcal{Y}}d\dual{\Haar}(\omega) =\FT{\inner{y',
            C(\cdot)y}_{\mathcal{Y}}}(\delta).
        \end{dmath*}
    \end{enumerate}
\end{proposition}
We found some confusion in the literature whether a kernel is the
\acl{FT} or \acl{IFT} of a measure. However \cref{lm:C_characterization}
clarifies the relation between the \acl{FT} and \acl{IFT} for a translation
invariant \acl{OVK}. Notice that in the real scalar case the \acl{FT} and
\acl{IFT} of a shift-invariant kernel are the same, while the difference is
significant for \acs{OVK}.
\paragraph{}
The following lemma is a direct consequence of the definition of $C(\omega)$ as
the \acl{FT} of the adjoint of $K_e$ and also helps to simplify the definition
of \acs{ORFF}.
\begin{lemma}
    \label{lm:C_characterization}
    Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer kernel
    such that for all $y$, $y'\in\mathcal{Y}$, $\inner{y',
    K_e(\cdot)y}_{\mathcal{Y}}\in L^1(\mathcal{X},\Haar)$ and let
    \begin{dmath*}
        \inner{y', C(\cdot)y}_{\mathcal{Y}}=\IFT{\inner{y',
        K_e(\cdot)y}_{\mathcal{Y}}}.
    \end{dmath*}
    Then
    \begin{enumerate}
        \item \label{lm:C_characterization_1} $C(\omega)$ is self-adjoint and
        $C$ is even.
        \item \label{lm:C_characterization_2} $\IFT{\inner{y',
        K_e(\cdot)y}_{\mathcal{Y}}} = \FT{\inner{y',
        K_e(\cdot)y}_{\mathcal{Y}}}$.
        \item \label{lm:C_characterization_3} $K_e(\delta)$ is self-adjoint and
        $K_e$ is even.
    \end{enumerate}
\end{lemma}
While \cref{pr:inverse_ovk_Fourier_decomposition} gives an explicit form of the
operator $C(\omega)$ defined as the \acl{FT} of the kernel $K$, it is not
really convenient to work with the Haar measure $\dual{\Haar}$ on
$\mathcal{B}(\dual{\mathcal{X}})$. However it is easily possible to turn
$\dual{\Haar}$ into a probability measure to allow efficient integration over
an infinite domain.
\paragraph{}
The following proposition allows to build a spectral decomposition of a
shift-invariant $\mathcal{Y}$-Mercer kernel on a \acs{LCA} group $\mathcal{X}$
endowed with the group law $\groupop$ with respect to a scalar probability
measure, by extracting a scalar probability density function from $C$.
\begin{proposition}[Shift-invariant $\mathcal{Y}$-Mercer kernel spectral
decomposition]
\label{pr:spectral}
    Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer
    kernel. If for all $y$, $y' \in\mathcal{Y}$, $\inner{K_e(.)y,y'}\in
    L^1(\mathcal{X},\Haar)$ then there exists a positive probability measure
    $\probability_{\dual{\Haar},\rho}$ and an operator-valued function $A$ an
    such that for all $y,$ $y'\in\mathcal{Y}$,
    \begin{dmath}
        \label{eq:expectation_spec} \inner{y', K_e(\delta)y}
        =\expectation_{\dual{\Haar},\rho}\left[\conj{\pairing{\delta,
        \omega}}\inner{y', A(\omega)y}\right],
    \end{dmath}
    with
    \begin{dmath}
        \label{eq:comega} \inner{y', A(\omega)y}\rho(\omega) = \FT{\inner{y',
        K_e(\cdot)y}}(\omega).
    \end{dmath}
    Moreover
    \begin{enumerate}
        \item for all $y,$ $y'\in\mathcal{Y}$, $\inner{A(.)y,y'}\in
        L^1(\dual{\mathcal{X}}, \probability_{\dual{\Haar},\rho})$,
        \item $A(\omega)$ is non-negative for
        $\probability_{\dual{\Haar},\rho}$-almost all
        $\omega\in\dual{\mathcal{X}}$,
        \item $A(\cdot)$ and $\rho(\cdot)$ are even functions.
    \end{enumerate}
\end{proposition}
\subsection{Examples of spectral decomposition}
\label{subsec:dec_examples}
In this section we give examples of spectral decomposition for various
$\mathcal{Y}$-Mercer kernels, based on \cref{pr:spectral}.

\subsubsection{Gaussian decomposable kernel}
\label{par:gaussian_dec} Recall that a decomposable $\mathbb{R}^p$-Mercer
introduced in the Background section has the form $K(x,z)=k(x,z)\Gamma$, where
$k(x,z)$ is a scalar Mercer kernel and $\Gamma\in\mathcal{L}(\mathbb{R}^p)$ is
a non-negative operator. Let us focus on
$K^{dec,gauss}_e(\cdot)=k_e^{gauss}(\cdot)\Gamma$, the Gaussian decomposable
kernel where $K_e^{dec, gauss}$ and $k_e^{gauss}$ are respectively the
signature of $K$ and $k$ on the additive group $\mathcal{X}=(\mathbb{R}^d,+)$
-- $\acs{ie}~\delta=x-z$ and $e=0$. The well known Gaussian kernel is defined
for all $\delta\in\mathbb{R}^d$ as follows
\begin{dmath*}
    k^{\text{gauss}}_0(\delta)\hiderel{=}\exp\left(
    -\frac{1}{2\sigma^2}\norm{\delta}^2_2\right)
\end{dmath*}
where $\sigma \in \mathbb{R}_{>0}$ is an hyperparameter corresponding to the
bandwith of the kernel. The --Pontryagin-- dual group of
$\mathcal{X}=(\mathbb{R}^d,+)$ is $\dual{\mathcal{X}}\cong(\mathbb{R}^d,+)$
with the pairing
\begin{dmath*}
    \pairing{\delta,\omega}=\exp\left(\iu\inner{\delta,\omega}\right)
\end{dmath*}
where $\delta$ and $\omega\in\mathbb{R}^d$. In this case the Haar measures on
$\mathcal{X}$ and $\dual{\mathcal{X}}$ are in both cases the Lebesgue measure.
However in order to have the property that $\IFT{\FT{f}}=f$ and
$\IFT{f}=\mathcal{R}\FT{f}$ one must normalize both measures by
$\sqrt{2\pi}^{-d}$, \acs{ie}~for all
$\mathcal{Z}\in\mathcal{B}\left(\mathbb{R}^d\right)$,
\begin{dgroup*}
    \begin{dmath*}
        \sqrt{2\pi}^{d}\Haar(\mathcal{Z}) = \Leb(\mathcal{Z}) \text{\ and}
    \end{dmath*}
    \begin{dmath*}
        \sqrt{2\pi}^{d}\dual{\Haar}(\mathcal{Z}) = \Leb(\mathcal{Z}).
    \end{dmath*}
\end{dgroup*}
Then the \acl{FT} on $(\mathbb{R}^d,+)$ is
\begin{dmath*}
    \FT{f}(\omega)
    =\int_{\mathbb{R}^d} \exp\left(-\iu\inner{\delta, \omega}\right) f(\delta)
    d\Haar(\delta)
    =\int_{\mathbb{R}^d} \exp\left(-\iu\inner{\delta, \omega}\right) f(\delta)
    \frac{d\Leb(\delta)}{\sqrt{2\pi}^d}.
\end{dmath*}
Since $k^{\text{gauss}}_0\in L^1$ and $\Gamma$ is bounded, it is possible to
apply \cref{pr:spectral}, and obtain for all $y$ and $y'\in\mathcal{Y}$,
\begin{dmath*}
    \inner*{y',C^{dec,gauss}(\omega)y}
    =\FT{\inner*{y',K^{dec,gauss}_0(\cdot)y}}(\omega)
    =\FT{k_0^{gauss}}(\omega)\inner*{y', \Gamma y}.
\end{dmath*}
Thus
\begin{dmath*}
    C^{dec,gauss}(\omega)
    =\int_{\mathbb{R}^d}\exp\left(-\iu\inner{\omega, \delta} -
    \frac{\norm{\delta}^2_2}{2\sigma^2}\right)
    \frac{d\Leb(\delta)}{\sqrt{2\pi}^d} \Gamma.
\end{dmath*}
Hence
\begin{dmath*}
    C^{dec,gauss}(\omega)
    =\underbrace{\frac{1}{\sqrt{2\pi\frac{1}{\sigma^2}}^d}\exp\left(
    -\frac{\sigma^2}{2}\norm{\omega}^2_2\right)\sqrt{2\pi}^d}_{\rho(\cdot)
    =\mathcal{N}(0,\sigma^{-2}I_d)\sqrt{2\pi}^d}\underbrace{\Gamma}_{A(\cdot)
    =\Gamma}.
\end{dmath*}
Therefore the canonical decomposition of $C^{dec,gauss}$ is
$A^{dec,gauss}(\omega)=\Gamma$ and
$\rho^{dec,gauss}=\mathcal{N}(0,\sigma^{-2}I_d)\sqrt{2\pi}^d$, where
$\mathcal{N}$ is the Gaussian probability distribution. Note that this
decomposition is done with respect to the \emph{normalized} Lebesgue measure
$\dual{\Haar}$, meaning that for all
$\mathcal{Z}\in\mathcal{B}(\dual{\mathcal{X}})$,
\begin{dmath*}
    \probability_{\dual{\Haar}, \mathcal{N}(0, \sigma^{-2} I_d)
    \sqrt{2\pi}^d}(\mathcal{Z})
    =\int_{\mathcal{Z}} \mathcal{N}(0, \sigma^{-2} I_d) \sqrt{2\pi}^d
    d\dual{\Haar}(\omega)
    =\int_{\dual{\mathcal{X}}}\mathcal{N}(0,\sigma^{-2}I_d)d\Leb(\omega)
    =\probability_{\mathcal{N}(0,\sigma^{-2}I_d)}(\mathcal{Z}).
\end{dmath*}
Thus, the same decomposition with respect to the usual --non-normalized--
Lebesgue measure $\Leb$ yields
\begin{dgroup}
    \begin{dmath}
        A^{dec,gauss}(\cdot)=\Gamma
    \end{dmath}
    \begin{dmath}
        \rho^{dec,gauss}=\mathcal{N}(0,\sigma^{-2}I_d).
    \end{dmath}
\end{dgroup}
\subsubsection{Skewed-$\chi^2$ decomposable kernel}
\label{subsubsec:skewedchi2}
The skewed-$\chi^2$ scalar kernel \citep{li2010random}, useful for image
processing, is defined on the \acs{LCA} group
$\mathcal{X}=(-c_k;+\infty)_{k=1}^d$, with $c_k\in\mathbb{R}_{>0}$ and endowed
with the group operation $\odot$. Let $(e_k)_{k=1}^d$ be the standard basis of
$\mathcal{X}$ and ${}_k:x\mapsto \inner{x,e_k}$. The operator $\odot:
\mathcal{X}\times\mathcal{X}\to\mathcal{X}$ is defined by
\begin{dmath*}
    x\odot z = \left((x_k + c_k)(z_k + c_k) - c_k\right)_{k=1}^d.
\end{dmath*}
The identity element $e$ is $\left(1-c_k\right)_{k=1}^d$ since $(1-c) \odot x =
x$. Thus the inverse element $x^{-1}$ is $((x_k+c_k)^{-1} - c_k)_{k=1}^d$. The
skewed-$\chi^2$ scalar kernel reads
\begin{dmath}
    k^{skewed}_{1-c}(\delta)
    =\prod_{k=1}^d\frac{2}{\sqrt{\delta_k+c_k}+\sqrt{\frac{1}{\delta_k+c_k}}}.
\end{dmath}
The dual of $\mathcal{X}$ is $\dual{\mathcal{X}}\cong\mathbb{R}^d$ with the
pairing
\begin{dmath*}
    \pairing{\delta,\omega}
    =\prod_{k=1}^d\exp\left(\iu\log(\delta_k+c_k)\omega_k\right).
\end{dmath*}
The Haar measure are defined for all
$\mathcal{Z}\in\mathcal{B}((-c;+\infty)^d)$ and all
$\dual{\mathcal{Z}}\in\mathcal{B}(\mathbb{R}^d)$ by
\begin{dgroup*}
    \begin{dmath*}
        \sqrt{2\pi}^d\Haar(\mathcal{Z})
        =\int_{\mathcal{Z}}\prod_{k=1}^d\frac{1}{z_k+c_k}d\Leb(z)
    \end{dmath*}
    \begin{dmath*}
        \sqrt{2\pi}^d\dual{\Haar}(\dual{\mathcal{Z}})=\Leb(\dual{\mathcal{Z}}).
    \end{dmath*}
\end{dgroup*}
Thus the \acl{FT} is
\begin{dmath*}
    \FT{f}(\omega)
    =\int_{(-c;+\infty)^d}\prod_{k=1}^d\frac{\exp\left(-\iu
    \log(\delta_k+c_k)\omega_k\right)}{\delta_k +
    c_k}f(\delta)\frac{d\Leb(\delta)}{\sqrt{2\pi}^d}.
\end{dmath*}
Then, applying Fubini's theorem over product space, and the fact that each
dimension is independent
\begin{dmath*}
    \FT{k_0^{skewed}}(\omega)
    =\prod_{k=1}^d\int_{-c_k}^{+\infty}\frac{2\exp\left(-\iu
    \log(\delta_k+c_k)\omega_k\right)}{(\delta_k +
    c_k)\left(\sqrt{\delta_k + c_k} + \sqrt{\frac{1}{\delta_k + c_k}}\right)}
    \frac{d\Leb(\delta_k)}{\sqrt{2\pi}^d}.
\end{dmath*}
Making the change of variable $t_k=(\delta_k+c_k)^{-1}$ yields
\begin{dmath*}
    \FT{k_0^{skewed}}(\omega)
    = \prod_{k=1}^d\int_{-\infty}^{+\infty} \frac{2 \exp\left(-\iu t_k
    \omega_k\right)}{\exp\left(\frac{1}{2} t_k \right) + \exp\left(-\frac{1}{2}
    t_k \right)} \frac{d\Leb(t_k)}{\sqrt{2\pi}^d}
    =\sqrt{2\pi}^d\prod_{k=1}^d\sech(\pi\omega_k).
\end{dmath*}
Since $k^{\text{skewed}}_{1-c}\in L^1$ and $\Gamma$ is bounded, it is possible
to apply \cref{pr:spectral}, and obtain
\begin{dmath*}
    C^{dec,skewed}(\omega)
    =\FT{k_{1-c}^{skewed}}(\omega)\Gamma
    =\underbrace{\sqrt{2\pi}^d\prod_{k=1}^d\sech(\pi \omega_k)}_{\rho(\cdot) =
    \mathcal{S}(0, 2^{-1})^d\sqrt{2 \pi}^d}\underbrace{\Gamma}_{A(\cdot)}.
\end{dmath*}
Hence the decomposition with respect to the usual --non-normalized-- Lebesgue
measure $\Leb$ yields
\begin{dgroup}
    \begin{dmath}
        A^{dec,skewed}(\cdot)=\Gamma
    \end{dmath}
    \begin{dmath}
        \rho^{dec,skewed}=\mathcal{S}\left(0,2^{-1}\right)^d.
    \end{dmath}
\end{dgroup}
\subsubsection{Curl-free Gaussian kernel}
The curl-free Gaussian kernel is defined as
$K^{curl,gauss}_0=-\nabla\nabla^\transpose k_0^{gauss}$. Here
$\mathcal{X}=(\mathbb{R}^d, +)$ so the setting is the same than
\cref{par:gaussian_dec}.
\begin{dmath*}
    C^{curl,gauss}(\omega)_{ij}=
    \FT{K^{curl,gauss}_{1-c}(\cdot)_{ij}}(\omega)
    =\FT{-\frac{d^2}{d\delta_id\delta_j}k^{gauss}_0}(\omega)
    =-(\iu\omega_i)(\iu\omega_j)\FT{k_0^{gauss}}(\omega)
    =\omega_i\omega_j\FT{k_0^{gauss}}(\omega)
    =\sqrt{2\pi\frac{1}{\sigma^2}}^d\exp\left(
    -\frac{\sigma^2}{2}\norm{\omega}^2_2\right)\sqrt{2\pi}^d\omega_i\omega_j.
\end{dmath*}
Hence
\begin{dmath*}
    C^{curl,gauss}(\omega)
    =\underbrace{\frac{1}{\sqrt{2\pi\frac{1}{\sigma^2}}^d}\exp\left(
    -\frac{\sigma^2}{2} \norm{\omega}^2_2\right) \sqrt{2\pi}^d}_{\mu(\cdot)
    =\mathcal{N}(0 ,\sigma^{-2} I_d)\sqrt{2 \pi}^d}
    \underbrace{\omega\omega^\transpose
    }_{A(\omega)=\omega\omega^\transpose }.
\end{dmath*}
Here a canonical decomposition is
$A^{curl,gauss}(\omega)=\omega\omega^\transpose $ for all
$\omega\in\mathbb{R}^d$ and
$\mu^{curl,gauss}=\mathcal{N}(0,\sigma^{-2}I_d)\sqrt{2\pi}^d$ with respect to
the normalized Lebesgue measure $d\omega$. Again the decomposition with respect
to the usual --non-normalized-- Lebesgue measure is for all
$\omega\in\mathbb{R}^d$
\begin{dgroup}
    \begin{dmath}
        A^{curl,gauss}(\omega)=\omega\omega^\transpose
    \end{dmath}
    \begin{dmath}
        \mu^{curl,gauss}=\mathcal{N}(0,\sigma^{-2}I_d).
    \end{dmath}
\end{dgroup}
\subsubsection{Divergence-free kernel}
The divergence-free Gaussian kernel is defined as
$K^{div,gauss}_0=(\nabla\nabla^\transpose -\Delta)k_0^{gauss}$ on the group
$\mathcal{X}=(\mathbb{R}^d, +)$. The setting is the same than
\cref{par:gaussian_dec}. Hence
\begin{dmath*}
    C^{div,gauss}(\omega)_{ij}
    = \FT{K^{div,gauss}_0(\cdot)_{ij}}(\omega)
    = \FT{\frac{\partial^2}{\partial \delta_i \partial \delta_j}k^{gauss}_0 -
    \delta_{i=j} \sum_{k=1}^d \frac{\partial^2}{\partial \delta_k
    \partial\delta_k} k^{gauss}_0}(\omega)
    = \left(-(\iu \omega_i)(\iu \omega_j) - \delta_{i=j}\sum_{k=1}^d(\iu
    \omega_k)^2\right) \FT{k_0^{gauss}}
    =\left(\delta_{i=j}\sum_{k=1}^d \omega_k^2 - \omega_i \omega_j\right)
    \FT{k_0^{gauss}}(\omega).
\end{dmath*}
Hence
\begin{dmath*}
    C^{div,gauss}(\omega)
    = \underbrace{\frac{1}{\sqrt{2\pi\frac{1}{\sigma^2}}^d}\exp\left(
    -\frac{\sigma^2}{2} \norm{\omega}^2_2\right) \sqrt{2\pi}^d}_{\rho(\cdot) =
    \mathcal{N}(0, \sigma^{-2} I_d)\sqrt{2 \pi}^d}\underbrace{\left(I_d
    \norm{\omega}_2^2 - \omega \omega^\transpose
    \right)}_{A(\omega)=I_d\norm{\omega}_2^2-\omega\omega^\transpose }.
\end{dmath*}
Thus the canonical decomposition with respect to the normalized Lebesgue
measure is $A^{div,gauss}(\omega)=I_d\norm{\omega}_2^2-\omega\omega^\transpose
$ and the measure
\begin{dmath*}
    \rho^{div,gauss}=\mathcal{N}(0,\sigma^{-2}I_d)\sqrt{2\pi}^d.
\end{dmath*}
The canonical decomposition with respect to the usual Lebesgue measure is
\begin{dgroup}
    \begin{dmath}
        A^{div,gauss}(\omega)=I_d\norm{\omega}_2^2-\omega\omega^\transpose
    \end{dmath}
    \begin{dmath}
        \rho^{div,gauss}=\mathcal{N}(0,\sigma^{-2}I_d).
    \end{dmath}
\end{dgroup}

\subsection{Operator-valued Random Fourier Features (ORFF)}
\label{sec:building_ORFF}
\subsubsection{Building Operator-valued Random Fourier Features}
As shown in \cref{pr:spectral} it is
always possible to find a pair $(A, \probability_{\dual{\Haar},\rho})$ from a
shift invariant $\mathcal{Y}$-Mercer \acl{OVK} $K_e$ such that
$\probability_{\dual{\Haar},\rho}$ is a probability measure, \acs{ie}
$\int_{\dual{\mathcal{X}}} \rho d\dual{\Haar}=1$ where $\rho$ is the density of
$\probability_{\dual{\Haar},\rho}$ and
$K_e(\delta)=\expectation_{\dual{\Haar},
\rho}{\conj{\pairing{\delta,\omega}}A(\omega)}$. In order to obtain an
approximation of $K$ from a decomposition $(A,
\probability_{\dual{\Haar},\rho})$ we turn our attention to a Monte-Carlo
estimation of the expectation in \cref{eq:expectation_spec} characterizing a
$\mathcal{Y}$-Mercer shift-invariant \acl{OVK}. % \paragraph{}
%  \probability_{\dual{\Haar},\rho}
\begin{proposition}
    \label{cr:ORFF-kernel} Let $K(x,z)$ be a shift-invariant
    $\mathcal{Y}$-Mercer kernel with signature $K_e$ such that for all $y$,
    $y'\in\mathcal{Y}$, $\inner{y', K_e(\cdot)y}\in L^1(\mathcal{X},\Haar)$.
    Then one can find a pair $(A, \probability_{\dual{\Haar},\rho})$ that
    satisfies \cref{pr:spectral}. \acs{ie} for
    $\probability_{\dual{\Haar},\rho}$-almost all $\omega$, and all $y,
    y'\in\mathcal{Y}$,
    \begin{dmath*}
        \inner{y, A(\omega)y'}\rho(\omega)=\FT{\inner{y',
        K_e(\cdot)y}}(\omega).
    \end{dmath*}
    \paragraph{}
    If $(\omega_j)_{j=1}^D$ be a sequence of
    $D\in\mathbb{N}^*$ \acs{iid}~random variables following the law
    $\probability_{\dual{\Haar},\rho}$ then the operator-valued function
    $\tilde{K}$ defined for $(x,z) \in \mathcal{X}\times
    \mathcal{X}$ as
    \begin{dmath*}
        \tilde{K}(x,z)= \frac{1}{D} \sum_{j=1}^D
        \conj{\pairing{x\groupop\inv{z},\omega_j}} A(\omega_j)
    \end{dmath*}
    is an approximation\footnote{\acs{ie} it satisfies for all $x$,
    $z\in\mathcal{X}$, $\tilde{K}(x, z) \converges{\acs{asurely}}{D\to\infty}
    K(x, z)$ in the weak operator topology, where $K$ is a $\mathcal{Y}$-Mercer
    \acs{OVK}.} of $K$.
\end{proposition}
\begin{proof}
    %Let us first notice that for a given $D$, $\tilde{K}$ satisfies the
    %properties of a shift-invariant $\mathcal{Y}$-Mercer kernel.  Second, F
    From the strong law of large numbers
    \begin{dmath*}
        \frac{1}{D} \sum_{j=1}^D \conj{\pairing{x\groupop\inv{z},\omega_j}}
        A(\omega_j) \converges{\acs{asurely}}{D \to \infty}
        \expectation_{\dual{\Haar}, \rho}[\conj{\pairing{x \groupop z^{-1},
        \omega_j} }A(\omega)]
    \end{dmath*}
    where the integral converges in the weak operator topology. Then by
    \cref{pr:spectral},
    \begin{dmath*}
        \expectation_{\dual{\Haar}, \rho}[\conj{\pairing{x \groupop z^{-1},
        \omega_j}}A(\omega)] = K_e(x\groupop\inv{z}).
    \end{dmath*}
\end{proof}
Now, for efficient computations as motivated in the introduction, we are
interested in finding an approximated \emph{feature map} instead of a kernel
approximation. Indeed, an approximated feature map will allow to build linear
models in regression tasks. The following proposition deals with the feature
map construction.
\begin{proposition}
    \label{cr:ORFF-map-kernel} Assume the same conditions as
    \cref{cr:ORFF-kernel}. Moreover, if one can define $B: \dual{\mathcal{X}}
    \to \mathcal{L}(\mathcal{Y}',\mathcal{Y})$ such that for
    %such that for all
    %$y\in\mathcal{Y}$ and all $y' \in\mathcal{Y}'$,
    %%$\inner{y, B(\cdot)y'} \in
    %L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar},\rho})$,  and for
    $\probability_{\dual{\Haar},\rho}$-almost all $\omega$, and all $y,
    y'\in\mathcal{Y}$,
    \begin{dmath*}
        \inner{y, B(\omega)B(\omega)^\adjoint y'}\rho(\omega)= \inner{y,
        A(\omega)y'}\rho(\omega) \hiderel{=} \FT{\inner{y,
        K_e(\cdot)y'}}(\omega),
    \end{dmath*}
    then the function $\tildePhi{\omega}:\dual{\mathcal{X}} \to
    \mathcal{L}(\mathcal{Y}, \Vect_{j=1}^D \mathcal{Y'})$ defined for all $y
    \in \mathcal{Y}$ as
    follows: \begin{dmath*}
        \tildePhi{\omega}(x)y
        = \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
        \omega_j}B(\omega_j)^\adjoint y \condition{$\omega_j \sim
        \probability_{\dual{\Haar},\rho}$ \ac{iid},}
    \end{dmath*}
    is an approximated feature map\footnote{\acs{ie}~it satisfies
    for all $x,z\in\mathcal{X}$, $\tildePhi{\omega}(x)^\adjoint
    \tildePhi{\omega}(z)\converges{\acs{asurely}}{D\to\infty}K(x,z)$ in the
    weak operator topology, where $K$ is a $\mathcal{Y}$-Mercer \acs{OVK}.} for
    the kernel $K$.
\end{proposition}
\begin{remark}
    We find a decomposition such that
    $A(\omega_j)=B(\omega_j)B(\omega_j)^\adjoint $ for all $j\in\mathbb{N}^*_D$
    either by exhibiting a closed-form or using a numerical decomposition.
    Such a decomposition always exists since $A(\omega)$ is positive
    semi-definite for all $\omega\in\dual{\mathcal{X}}$.
\end{remark}
Notice that an \acs{ORFF} map as defined in \cref{cr:ORFF-map-kernel} is also
the Monte-Carlo sampling of the corresponding functional Fourier feature map
$\Phi_x: \mathcal{Y} \to L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar},
\rho}; \mathcal{Y}' )$ as defined in \cref{pr:fourier_feature_map}.  Indeed,
for all
$y\in\mathcal{Y}$ and all $x\in\mathcal{X}$,
\begin{dmath*}
    \tildePhi{\omega}(x)y = \Vect_{j=1}^D (\Phi_x y)(\omega_j)
    \condition{$\omega_j \sim \probability_{\dual{\Haar}, \rho}$ \acs{iid}.}
\end{dmath*}
\Cref{cr:ORFF-map-kernel} allows us to define \cref{alg:ORFF_construction} for
constructing \acs{ORFF} from an operator valued kernel.
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\begin{center}
    \begin{algorithm2e}[H]\label{alg:ORFF_construction}
        \SetAlgoLined
        \Input{$K(x, z)=K_e(\delta)$ a $\mathcal{Y}$-shift-invariant Mercer
        kernel on $(\mathcal{X}, \groupop)$ such that $\forall
        y,y'\in\mathcal{Y},$ $\inner{y', K_e(\cdot)y}\in L^1(\mathbb{R}^d,
        \Haar)$ and $D$ the number of features.}
        \Output{A random feature $\tildePhi{\omega}(x)$ such that
        $\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(z) \approx K(x,z)$}
        \BlankLine
        Define the pairing $\pairing{x, \omega}$ from the \acs{LCA} group
        $(\mathcal{X}, \groupop)$\; Find a decomposition
        $(A,\probability_{\dual{\Haar},\rho})$ and $B$ such that
        \begin{dmath*}
            B(\omega)B(\omega)^\adjoint \rho(\omega)=
            A(\omega)\rho(\omega)=\IFT{K_e}(\omega)\text{\;}
        \end{dmath*}
        \nl Draw $D$ \acs{iid} realizations $(\omega_j)_{j=1}^D$ from the
        probability distribution $\probability_{\dual{\Haar},\rho}$\;
        \nl \Return
        $\begin{cases}
            \tildePhi{\omega}(x) \in \mathcal{L}(\mathcal{Y}, \tildeH{\omega})
            &: y \mapsto \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
            \omega_j}B(\omega_j)^\adjoint y \\
            \tildePhi{\omega}(x)^\adjoint \in\mathcal{L}(\tildeH{\omega},
            \mathcal{Y}) &: \theta \mapsto \frac{1}{\sqrt{D}} \sum_{j=1}^D
            \pairing{x, \omega_j}B(\omega_j)\theta_j
        \end{cases}$\;
        \caption{Construction of \acs{ORFF} from \acs{OVK}}
    \end{algorithm2e}
\end{center}
\paragraph{}
We give a numerical illustration of different $\tilde{K}$ built from different
\acs{iid} realization $(\omega_j)_{j=1}^D$,
$\omega_j\sim\probability_{\dual{\Haar},\rho}$. In \cref{fig:not_Mercer}, we
represent the approximation of  a reference function (black line) defined as
$(y_1, y_2)^\transpose = f(x_i)=\sum_{j=1}^{250}\mathbf{K}_{ij}u_j$ where
$u_j\sim\mathcal{N}(0,I_2)$ and $K$ is a Gaussian decomposable kernel defined
as
\begin{dmath*}
    \mathbf{K}_{ij}=\exp\left(-\frac{1}{2(0.1)^2(x_i - x_j)^2}\right)\Gamma
    \condition{for $i$, $j\in\mathbb{N}^*_{250}$.}
\end{dmath*}
We took $\Gamma=.5 I_2 +.5 1_2$ such that the outputs $y_1$ and $y_2$ share
some similarities.  We generated $250$ points equally separated on the segment
$(-1;1)$.
\begin{dmath*}
    \mathbf{K}_{ij}=\exp\left(-\frac{1}{2(0.1)^2(x_i - x_j)^2}\right)\Gamma
    \condition{for $i$, $j\in\mathbb{N}^*_{250}$.}
\end{dmath*}
We took $\Gamma=.5 I_2 +.5 1_2$ such that the outputs $y_1$ and $y_2$ share
some similarities.  We generated $250$ points equally separated on the segment
$(-1;1)$.
\begin{pycode}[not_mercer]
sys.path.append('./src/')
import not_mercer

not_mercer.main()
\end{pycode}
\begin{figure}[htb]
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./not_Mercer.pgf}}')}
    \caption[Approximation of a function in a vv-RKHS using different
    realizations of Operator Random Fourier Feature]{Approximation of a
    function in a VV-RKHS using different realizations of Operator Random
    Fourier Feature.
    Top row and bottom row correspond to two different realizations of
    $\tildeK{\omega}$, which are \emph{different} \acl{OVK}. However when $D$
    tends to infinity, the different realizations of $\tildeK{\omega}$ yield
    the same \acs{OVK}.}
    \label{fig:not_Mercer}
\end{figure}
Then we computed an approximate kernel matrix $\tilde{\mathbf{K}}\approx
\mathbf{K}$ for $25$ increasing values of $D$ ranging from $1$ to $10^4$. The
two graphs in \cref{fig:not_Mercer} on the top row shows that the more the
number of features increases the closer the model
$\widetilde{f}(x_i)=\sum_{j=1}^{250}\tilde{\mathbf{K}}_{ij}u_j$ is to $f$.
The bottom row shows the same experiment but for a different realization of
$\tilde{\mathbf{K}}$. When $D$ is small the curves of the bottom and top rows are
very dissimilar --and sine wave like-- while they both converge to $f$ when
$D$ increase.
\paragraph{}
We introduce a \emph{functional} feature map, we call \emph{Fourier
Feature map}, defined by the following proposition as a direct consequence of
\cref{pr:mercer_kernel_bochner}.
\begin{proposition}[Functional Fourier feature map]
    \label{pr:fourier_feature_map} Let $\mathcal{Y}$ and $\mathcal{Y}'$ be two
    Hilbert spaces. If there exist an operator-valued function
    $B:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y},\mathcal{Y}')$ such that
    for all $y$, $y'\in\mathcal{Y}$,
    \begin{dmath*}
        \inner{y, B(\omega)B(\omega)^\adjoint y'}_{\mathcal{Y}}
        =\inner{y', A(\omega)y}_{\mathcal{Y}}
    \end{dmath*}
    $\dual{\mu}$-almost everywhere and $\inner{y', A(\cdot)y}\in
    L^1(\dual{\mathcal{X}},\dual{\mu})$ then the operator $\Phi_x$ defined for
    all $y$ in $\mathcal{Y}$ by
    \begin{dmath}
        \label{eq:feature_shiftinv_map}
        (\Phi_x y)(\omega)=\pairing{x,\omega}B(\omega)^\adjoint y,
    \end{dmath}
    is \emph{a feature map}\footnote{\acs{ie}~it satisfies for all $x$, $z \in
    \mathcal{X}$, $\Phi_x^\adjoint \Phi_z=K(x,z)$ where $K$ is a
    $\mathcal{Y}$-Mercer \acs{OVK}.} of some shift-invariant
    $\mathcal{Y}$-Mercer kernel $K$.
\end{proposition}
With this notation we have $\Phi: \mathcal{X} \to \mathcal{L}(\mathcal{Y};
L^2(\dual{\mathcal{X}}, \dual{\mu}; \mathcal{Y}'))$ such that $\Phi_x\in
\mathcal{L}(\mathcal{Y}; L^2(\dual{\mathcal{X}}, \dual{\mu}; \mathcal{Y}'))$
where $\Phi_x\colonequals\Phi(x)$.
\begin{figure}[htb]
    \centering
    \resizebox{\textwidth}{!}{%
    \input{./gfx/feature_relationship.tikz}}
    \caption[Relationships between feature-maps.]{Relationships between
    feature-maps. For any realization of
    $\omega_j\sim\probability_{\dual{\Haar},\rho}$ \ac{iid},
    $\tildeH{\omega} = \Vect_{j=1}^D \mathcal{Y}'$.}
    \label{fig:rel_features}
\end{figure}
Notice that an \acs{ORFF} map as defined in \cref{cr:ORFF-map-kernel} is also
the Monte-Carlo sampling of the corresponding functional Fourier feature map
$\Phi_x: \mathcal{Y} \to L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar},
\rho}; \mathcal{Y}' )$ as defined in \cref{pr:fourier_feature_map}.  Indeed,
for all $y\in\mathcal{Y}$ and all $x\in\mathcal{X}$,
\begin{dmath*}
    \tildePhi{\omega}(x)y = \Vect_{j=1}^D (\Phi_x y)(\omega_j)
    \condition{$\omega_j \sim \probability_{\dual{\Haar}, \rho}$ \acs{iid}}
\end{dmath*}

\subsection{From Operator Random Fourier Feature maps to OVKs}
It is also interesting to notice that we can go the other way and define from
the general form of an \acl{ORFF}, an operator-valued kernel.
\begin{proposition}[Operator Random Fourier Feature map]
    \label{pr:ORFF-map} Let $\mathcal{Y}$ and $\mathcal{Y}'$ be two Hilbert
    spaces. If one defines an operator-valued function on the dual of a LCA
    group $\mathcal{X}$, $B: \dual{\mathcal{X}} \to
    \mathcal{L}(\mathcal{Y},\mathcal{Y}')$, and a probability measure
    $\probability_{\dual{\Haar},\rho}$ on $\mathcal{B}(\dual{\mathcal{X}})$,
    such that for all $y\in\mathcal{Y}$ and all $y'\in\mathcal{Y}'$, $\inner{y,
    B(\cdot)y'} \in L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar},\rho})$,
    then the operator-valued function
    \begin{dmath*}
        \tildePhi{\omega}: \mathcal{X} \hiderel{\to}
        \mathcal{L}\left(\mathcal{Y}, \vect_{j=1}^D\mathcal{Y}'\right)
    \end{dmath*}
    defined for all $x \in \mathcal{X}$ and for all $y\in\mathcal{Y}$ by
    \begin{dmath}
        \label{eq:phitilde}
        \tildePhi{\omega}(x)y
        = \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
        \omega_j}B(\omega_j)^\adjoint y\condition{$\omega_j \sim
        \probability_{\dual{\Haar, \rho}}$, \acs{iid}, }
    \end{dmath}
    is an approximated feature map of some $\mathcal{Y}$-Mercer operator-valued
    kernel\footnote{\acs{ie}~it satisfies $\tildePhi{\omega}(x)^\adjoint
    \tildePhi{\omega}(z)\converges{\acs{asurely}}{D\to\infty}K(x,z)$ in the
    weak operator topology, where $K$ is a $\mathcal{Y}$-Mercer \acs{OVK}}.
\end{proposition}
The difference between \cref{pr:ORFF-map} and \cref{cr:ORFF-map-kernel} is that
in \cref{pr:ORFF-map} we do not assume that $A(\omega)$ and
$\probability_{\dual{Haar}, \rho}$ have been obtained from \cref{pr:spectral}.
We conclude by showing that any realization of an approximate feature map gives
a proper operator valued kernel. Hence we can always view $\tilde{K}(x,
z)=\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(z)$ ---where
$\tildePhi{\omega}$ is defined as in \cref{cr:ORFF-kernel} (construction from
an \acs{OVK}) or \cref{pr:ORFF-map}--- as a $\mathcal{Y}$-Mercer and thus apply
all the classic results of the \acl{OVK} theory on $\tilde{K}$.
\begin{proposition}
    \label{pr:orff_defines_kernel}
    Let $\seq{\omega}\in\dual{\mathcal{X}}^D$. If for all $y$,
    $y'\in\mathcal{Y}$
    \begin{dmath*}
        \inner{y', \tildeK{\omega}_e\left(x\groupop
        z^{-1}\right)y}_{\mathcal{Y}}
        =\inner{\tildePhi{\omega}(x)y',
        \tildePhi{\omega}(z)y}_{\tildeH{\omega}}
        =\inner*{y', \frac{1}{D}\sum_{j=1}^D \conj{\pairing{x\groupop
        z^{-1},\omega_j}}B(\omega_j)B(\omega_j)^*y}_{\mathcal{Y}},
    \end{dmath*}
    for all $x$, $z\in\mathcal{X}$, then $\tildeK{\omega}$ is a shift-invariant
    $\mathcal{Y}$-Mercer \acl{OVK}.
\end{proposition}
Note that the above theorem does not considers the $\omega_j$'s as random
variables and therefore does not shows the convergence of the kernel
$\widetilde{K}$ to some target kernel $K$. However is shows that any
realization of $\widetilde{K}$ when $\omega_j$'s  are random variables yields
a valid $\mathcal{Y}$-Mercer operator-valued kernel.
Note that the above theorem does not considers the $\omega_j$'s as random
variables and therefore does not shows the convergence of the kernel
$\widetilde{K}$ to some target kernel $K$. However is shows that any
realization of $\widetilde{K}$ when $\omega_j$'s  are random variables yields
a valid $\mathcal{Y}$-Mercer operator-valued kernel.
\paragraph{}
Indeed, as a results of \cref{pr:orff_defines_kernel}, in the same way we
defined an \acs{ORFF}, we can define an approximate feature operator
$\tildeW{\omega}$ which maps $\tildeH{\omega}$ onto
$\mathcal{H}_{\tildeK{\omega}}$, where
\begin{dmath*}
    \tildeK{\omega}(x,z)=
    \tildePhi{\omega}(x)^\adjoint\tildePhi{\omega}(z)\condition{for
    all $x$, $z\in\mathcal{X}$.}
\end{dmath*}
\begin{definition}[Random Fourier feature operator]
    Let $\seq{\omega}=(\omega_j)_{j=1}^D\in\dual{\mathcal{X}}^D$ and let
    \begin{dmath*}
        \tildeK{\omega}_e=\frac{1}{D}\sum_{j=1}^D
        \conj{\pairing{\cdot,\omega_j}}B(\omega_j)B(\omega_j)^*.
    \end{dmath*}
    We call random Fourier feature operator the linear application
    $\tildeW{\omega}:\tildeH{\omega}\to \mathcal{H}_{\tildeK{\omega}}$ defined
    as
    \begin{dmath*}
        \left(\tildeW{\omega} \theta\right)(x)
        \colonequals \tildePhi{\omega}(x)^\adjoint \theta
        =\frac{1}{\sqrt{D}}\sum_{j=1}^D
        \conj{\pairing{x,\omega_j}}B(\omega_j)\theta_j
    \end{dmath*}
    where
    $\theta=\Vect_{j=1}^D\theta_j \in\tildeH{\omega}$.
    Then from \cref{pr:feature_operator},
    \begin{dmath*}
        \left(\Ker \tildeW{\omega}\right)^\perp
        = \lspan\Set{\tildePhi{\omega}(x)y | \forall x\in\mathcal{X},\enskip
        \forall y\in\mathcal{Y}} \hiderel{\subseteq} \tildeH{\omega}.
    \end{dmath*}
\end{definition}
The random Fourier feature operator is useful to show the relations between the
random Fourier feature map with the functional feature map defined in
\cref{pr:fourier_feature_map}. The relationship between the generic feature map
(defined for all \acl{OVK}) the functional feature map (defining a
shift-invariant $\mathcal{Y}$-Mercer \acl{OVK}) and the random Fourier feature
map is presented in \cref{fig:rel_features}.
\begin{proposition}
    \label{pr:phitilde_phi_rel}
    For any $g\in \mathcal{H}=L^2(\mathcal{\dual{X}},
    \probability_{\dual{\Haar},\rho}; \mathcal{Y}')$, let
    \begin{dmath*}
        \theta \colonequals \frac{1}{\sqrt{D}}\Vect_{j=1}^D g(\omega_j),
        \enskip \omega_j \sim \probability_{\dual{\Haar},\rho}
        \enskip\text{\ac{iid}~}.
    \end{dmath*}
    Then
    \begin{enumerate}
        \item \label{pr:cv_feature_map_1} $\left(\tildeW{\omega}
        \theta\right)(x)=\tildePhi{\omega}(x)^\adjoint \theta
        \converges{\acs{asurely}}{D\to\infty} \Phi_x^\adjoint g=(Wg)(x)$,
        \item \label{pr:cv_feature_map_2} $\norm{\theta}_{\tildeH{\omega}}^2
        \converges{\acs{asurely}}{D\to\infty} \norm{g}_{\mathcal{H}}^2$,
    \end{enumerate}
\end{proposition}
We write $\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(x)\approx K(x,z)$
when $\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(x)
\converges{\acs{asurely}}{} K(x,z)$ in the weak operator topology when $D$
tends to infinity. With mild abuse of notation we say that
$\tildePhi{\omega}(x)$ is an approximate feature map of the functional feature
map $\Phi_x$ \acs{ie}~$\tildePhi{\omega}(x)\approx \Phi_x$, when for all $y'$,
$y\in\mathcal{Y}$,
\begin{dmath*}
    \inner{y, K(x,z)y'}_{\mathcal{Y}}=\inner{\Phi_x y, \Phi_z
    y'}_{L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar}, \rho};
    \mathcal{Y'})} \approx \inner{\tildePhi{\omega}(x)y,
    \tildePhi{\omega}(x)y'}_{\tildeH{\omega}}\colonequals \inner{y,
    \tilde{K}(x,z)y'}_{\mathcal{Y}}
\end{dmath*}
where $\Phi_x$ is defined in the sense of \cref{pr:fourier_feature_map}.

\subsection{Examples of Operator Random Fourier Feature maps}
\label{subsec:examples_ORFF} We now give two examples of operator-valued random
Fourier feature map. First we introduce the general form of an approximated
feature map for a matrix-valued kernel on the additive group
$(\mathbb{R}^d,+)$.
\begin{example}[Matrix-valued kernel on the additive group]
    \label{ex:additive_group} In the following let $K(x,z)=K_0(x-z)$ be a
    $\mathcal{Y}$-Mercer matrix-valued kernel on $\mathcal{X}=\mathbb{R}^d$,
    invariant \acs{wrt}~the group operation $+$. Then the function
    $\tildePhi{\omega}$ defined as follow is an \acl{ORFF} of $K_{0}$.
    \begin{dmath*}
        \tildePhi{\omega}(x)y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}B(\omega_j)^\adjoint y \\
            \sin{\inner{x,\omega_j}_2}B(\omega_j)^\adjoint y
        \end{pmatrix}
        \condition{$\omega_j \sim \probability_{\dual{\Haar},\rho}$
        \acs{iid}.}
    \end{dmath*}
    for all $y\in\mathcal{Y}$.
\end{example}
In particular we deduce the following features maps for the kernels proposed in
\cref{subsec:dec_examples}.
\begin{itemize}
    \item For the decomposable gaussian kernel
    $K_0^{dec,gauss}(\delta)=k_0^{gauss}(\delta)\Gamma$ for all
    $\delta\in\mathbb{R}^d$, let $BB^\adjoint=\Gamma$. A bounded --and
    unbounded-- \acs{ORFF} map is
    \begin{dmath*}
        \tildePhi{\omega}(x)y
        =\frac{1}{\sqrt{D}} \Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2} B^\adjoint y \\
            \sin{\inner{x,\omega_j}_2}B^\adjoint y
        \end{pmatrix}
        =(\tildephi{\omega}(x)\otimes B^\adjoint)y,
    \end{dmath*}
    where $\omega_j \hiderel{\sim}
    \probability_{\mathcal{N}(0,\sigma^{-2}I_d)}$ \ac{iid}~and
    $\tildephi{\omega}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos{\inner{x,\omega_j}_2} \\
        \sin{\inner{x,\omega_j}_2}
    \end{pmatrix}$ is a scalar \acs{RFF}
    map~\citep{Rahimi2007}.
    \item For the curl-free gaussian kernel,
    $K_0^{curl,gauss}=-\nabla\nabla^\transpose k_0^{gauss}$ an unbounded
    \acs{ORFF} map is
    \begin{dmath}
        \label{eq:unbounded_curl_free_orff}
        \tildePhi{\omega}(x)y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}\omega_j^\transpose y \\
            \sin{\inner{x,\omega_j}_2}\omega_j^\transpose y
        \end{pmatrix},
    \end{dmath}
    $\omega_j \hiderel{\sim} \probability_{\mathcal{N}(0,\sigma^{-2}I_d)}$
    \ac{iid}~and a bounded \acs{ORFF} map is
    \begin{dmath*}
        \tildePhi{\omega}(x) y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}\frac{\omega_j^\transpose
            }{\norm{\omega_j}} y \\
            \sin{\inner{x,\omega_j}_2}\frac{\omega_j^\transpose
            }{\norm{\omega_j}} y
        \end{pmatrix}
        \condition{$\omega_j \hiderel{\sim} \probability_{\rho}$ \ac{iid}.}
    \end{dmath*}
    where $\rho(\omega) = \frac{\sigma^2\norm{\omega}^2}{d} \mathcal{N}(0,
    \sigma^{-2} I_d)(\omega)$ for all $\omega\in\mathbb{R}^d$.
    \item For the divergence-free gaussian kernel
    $K_0^{div,gauss}(x,z)=(\nabla\nabla^\transpose -\Delta I_d)
    k_0^{gauss}(x,z)$ an unbounded \acs{ORFF} map is
    \begin{dmath}
        \label{eq:unbounded_div_free_orff}
        \tildePhi{\omega}(x) y
        =\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}B(\omega_j)^\transpose y \\
            \sin{\inner{x,\omega_j}_2}B(\omega_j)^\transpose y
        \end{pmatrix}
    \end{dmath}
    where $\omega_j \hiderel{\sim} \probability_{\rho}$ \ac{iid}~and
    $B(\omega)=\left(\norm{\omega}I_d-\omega\omega^\transpose \right)$ and
    $\rho=\mathcal{N}(0,\sigma^{-2}I_d)$ for all $\omega\in\mathbb{R}^d$. A
    bounded \acs{ORFF} map is
    \begin{dmath*}
        \tildePhi{\omega}(x) y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}B(\omega_j)^\transpose y \\
            \sin{\inner{x,\omega_j}_2}B(\omega_j)^\transpose y
            \end{pmatrix}
            \condition{$\omega_j \hiderel{\sim} \probability_{\rho}$ \ac{iid},}
    \end{dmath*}
    where $B(\omega) = \left(I_d - \frac{\omega\omega^\transpose
    }{\norm{\omega}^2}\right)$ and $\rho(\omega) =
    \frac{\sigma^2\norm{\omega}^2}{d}\mathcal{N}(0,\sigma^{-2}I_d)$ for all
    $\omega\in\mathbb{R}^d$.
\end{itemize}
The second example extends scalar-valued Random Fourier Features on the skewed
multiplicative group --described in \cref{subsec:character} and
\cref{subsubsec:skewedchi2}-- to the operator-valued case.
\begin{example}[Matrix-valued kernel on the skewed multiplicative group]
    In the following, $K(x,z)=K_{1-c}(x\odot z^{-1})$ is a $\mathcal{Y}$-Mercer
    matrix-valued kernel on $\mathcal{X}=(-c;+\infty)^d$ invariant
    \acs{wrt}~the group operation\footnote{The group operation $\odot$ is
    defined in \cref{subsubsec:skewedchi2}.} $\odot$. Then the function
    $\tildePhi{\omega}$ defined as follow is an \acl{ORFF} of $K_{1-c}$.
    \begin{dmath*}
        \tildePhi{\omega}(x) y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{\log(x+c),\omega_j}_2}B(\omega_j)^\adjoint y \\
            \sin{\inner{\log(x+c),\omega_j}_2}B(\omega_j)^\adjoint y
        \end{pmatrix},
    \end{dmath*}
    $\omega_j \sim \probability_{\dual{\Haar},\rho}$ \ac{iid}, for all
    $y\in\mathcal{Y}$.
\end{example}
\subsection{Regularization property}
\label{subsec:regularization_property}
We have shown so far that it is always possible to construct a feature map that
allows to approximate a shift-invariant $\mathcal{Y}$-Mercer kernel. However we
could also propose a construction of such map by studying the regularization
induced with respect to the \acl{FT} of a target function $f\in \mathcal{H}_K$.
In other words, what is the norm in $L^2(\dual{\mathcal{X}}, \dual{\Haar};
\mathcal{Y}')$ induced by $\norm{\cdot}_K$?
\begin{proposition}
    \label{pr:fourier_reg_ovk}
    Let $K$ be a shift-invariant $\mathcal{Y}$-Mercer Kernel such that for all
    $y$, $y'$ in $\mathcal{Y}$, $\inner{y', K_e(\cdot)y}_{\mathcal{Y}}\in
    L^1(\mathcal{X}, \Haar)$. Then for all $f\in\mathcal{H}_K$
    \begin{dmath}
        \norm{f}^2_K = \displaystyle\int_{\dual{\mathcal{X}}}
        \frac{\inner*{\FT{f}(\omega), A\left(\omega\right)^\dagger
        \FT{f}(\omega)}_{\mathcal{Y}}}{\rho(\omega)} d\dual{\Haar}(\omega).
        \label{eq:reg_L2}
    \end{dmath}
    where $\inner{y', A(\omega)y}\rho(\omega)\colonequals\FT{\inner{y',
    K_e(\cdot)y}}(\omega)$.  \label{pr:regularization}
\end{proposition}
Note that if $K(x,z)=k(x,z)$ is a scalar kernel then for all $\omega$ in
$\dual{\mathcal{X}}$, $A(\omega)=1$. Therefore we recover the well known result
for kernels that is for any $f\in\mathcal{H}_k$ we have $\norm{f}_k =
\int_{\dual{\mathcal{X}}} \FT{k_e}(\omega)^{-1} \FT{f}(\omega)^2
d\dual{\Haar}(\omega)$~\citep{Yang2012, vertregularization,
smola1998connection}. Eventually from this last equation we also recover
\cref{pr:kernel_reg} for decomposable kernels. If
$A(\omega)=\Gamma\in\mathcal{L}_+(\mathbb{R}^p)$,
\begin{dmath}
    \norm{f}_K = \sum_{i,j=1}^p
    \left(\Gamma^\dagger\right)_{ij}\inner{f_i,f_j}_k
\end{dmath}
We also note that the regularization property in $\mathcal{H}_K$ does not
depends (as expected) on the decomposition of $A(\omega)$ into
$B(\omega)B(\omega)^\adjoint $.  Therefore the decomposition should be chosen
such that it optimizes the computation cost. For instance if
$A(\omega)\in\mathcal{L}(\mathbb{R}^p)$ has rank $r$, one could find an
operator $B(\omega)\in\mathcal{L}(\mathbb{R}^p, \mathbb{R}^r)$ such that
$A(\omega)=B(\omega)B(\omega)^\adjoint$. Moreover, in light of
\cref{pr:regularization} the regularization property of the kernel with respect
to the \acl{FT}, it is also possible to define an approximate feature map of an
\acl{OVK} from its regularization properties in the \acs{vv-RKHS} as proposed
in \cref{alg:ORFF2_construction}.
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\begin{center}
    \begin{algorithm2e}\label{alg:ORFF2_construction}
        \SetAlgoLined
        \Input{%
        \begin{itemize}
            \item The pairing $\pairing{x, \omega}$ of the \acs{LCA} group
            $(\mathcal{X}, \groupop)$.
            \item A probability measure $\probability_{\dual{\Haar},\rho}$ with
            density $\rho$ \acs{wrt}~the haar measure $\dual{\Haar}$ on
            $\dual{\mathcal{X}}$.
            \item An operator-valued function
            $B:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y},\mathcal{Y}')$ such
            that for all $y$ $y'\in\mathcal{Y}$, $\inner{y',
            B(\cdot)B(\cdot)^\adjoint y}\in
            L^1(\dual{\mathcal{X}},\probability_{\dual{\Haar},\rho})$.
            \item $D$ the number of features.
        \end{itemize}}
        \Output{A random feature $\tildePhi{\omega}(x)$ such that
        $\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(z) \approx K(x,z)$.}
        \BlankLine
        Draw $D$ random vectors $(\omega_j)_{j=1}^D$ \ac{iid}~from the
        probability law $\probability_{\dual{\Haar},\rho}$\;
        \Return $
        \begin{cases}
            \tildePhi{\omega}(x) \in\mathcal{L}(\mathcal{Y}, \tildeH{\omega})
            &: y \mapsto \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
            \omega_j}B(\omega_j)^\adjoint y \\
            \tildePhi{\omega}(x)^\adjoint \in\mathcal{L}(\tildeH{\omega},
            \mathcal{Y}) &: \theta \mapsto \frac{1}{\sqrt{D}} \sum_{j=1}^D
            \pairing{x, \omega_j}B(\omega_j)\theta_j
        \end{cases}$\;
        \caption{Construction of \acs{ORFF}}
    \end{algorithm2e}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main contribution: convergence with high probability of the
\acpdfstring{ORFF} estimator}
\label{sec:consistency_of_the_ORFF_estimator}
We are now interested in a non-asymptotic analysis of the \ac{ORFF}
approximation of shift-invariant $\mathcal{Y}$-Mercer kernels on \acs{LCA}
group $\mathcal{X}$ endowed with the operation group $\groupop$ where
$\mathcal{X}$ is a Banach space (The more general case where $\mathcal{X}$ is
a Polish space is discussed in the appendix \cref{subsec:concentration_proof}).
For a given $D$, we study how close is the
approximation $\tilde{K}(x,z)=\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)$ to the
target kernel $K(x,z)$ for any $x,z$ in $\mathcal{X}$.
\paragraph{}
If $A\in\mathcal{L}_+(\mathcal{Y})$ we denote
$\norm{A}_{\mathcal{Y},\mathcal{Y}}$ its operator norm, which amounts to the
square root of the largest eigenvalue of $A$ when $\mathcal{Y}=\mathbb{R}^p$ is
finite dimensional. For $x$ and $z$ in some non-empty compact $\mathcal{C}
\subset \mathbb{R}^d$, we consider: $F(x \groupop \inv{z})
=\tilde{K}(x,z)-K(x,z)$ and study how the uniform norm
\begin{dmath}\label{eq:norm_inf}
    \norm{\tilde{K}-K}_{\mathcal{C}\times\mathcal{C}}
    = \sup_{(x,z)\in\mathcal{C}\times\mathcal{C}}
    \norm{\tilde{K}(x,z)-K(x,z)}_{\mathcal{Y},\mathcal{Y}}
\end{dmath}
behaves according to $D$. All along this document we denote $\delta=x\groupop
z^{-1}$ for all $x$ and $z\in\mathcal{X}$. \Cref{fig:approximation_error}
empirically shows convergence of three different \acs{OVK} approximations for
$x,z$ sampled from the compact $[-1,1]^4$ and using an increasing number of
sample points $D$. The log-log plot shows that all three kernels have the same
convergence rate, up to a multiplicative factor.
\begin{figure}[!ht]
    \centering
    \resizebox{\textwidth}{!}{\input{./gfx/approximation.pgf}}
    \caption[\acs{ORFF} reconstruction error]{Error reconstructing the target
    operator-valued kernel $K$ with \acs{ORFF}
    approximation $\tilde{K}$ for the decomposable, curl-free and
    divergence-free kernel.}
    \label{fig:approximation_error}
\end{figure}
\paragraph{}
In order to bound the error with high probability, we turn to concentration
inequalities devoted to random matrices~\citep{Boucheron}. The concentration
phenomenon can be summarized in the following sentence of
\citet{ledoux2005concentration}. \say{A random variable that depends (in a
smooth way) on the influence of many random variables (but not too much on any
of them) is essentially constant}.
\paragraph{}
A typical application is the study of the deviation of the empirical mean of
\acl{iid} random variables to their expectation. This means that given an error
$\epsilon$ between the kernel approximation $\tildeK{\omega}$ and the true
kernel $K$, if we are given enough samples to construct $\tildeK{\omega}$, the
probability of measuring an error greater than $\epsilon$ is essentially zero
(it drops at an exponential rate with respect to the number of samples $D$). To
measure the error between the kernel approximation and the true kernel at a
given point many metrics are possible. \acs{eg} any matrix norm such as the
Hilbert-Schmidt norm, trace norm, the operator norm or Schatten norms. In this
work we focus on measuring the error in terms of operator norm. For all $x$,
$z\in\mathcal{X}$ we look for a bound on
\begin{dmath*}
    \probability_{\rho} \Set{(\omega_j)_{j=1}^D | \norm{\tildeK{\omega}(x, z) -
    K(x, z)}_{\mathcal{Y}, \mathcal{Y}} \ge \epsilon }
    =
    \probability_{\rho} \Set{(\omega_j)_{j=1}^D | \sup_{0\neq y\in\mathcal{Y}}
    \frac{\norm{(\tildeK{\omega}(x, z) - K(x,
    z))y}_{\mathcal{Y}}}{\norm{y}_{\mathcal{Y}}} \ge \epsilon}
\end{dmath*}
In other words, given any vector $y\in\mathcal{Y}$ we study how the residual
operator $\tildeK{\omega} - K$ is able to send $y$ to zero. We believe that
this way of measuring the \say{error} to be more intuitive. Moreover, on
contrary to an error measure with the Hilbert-Schmidt norm, the operator norm
error does not grows linearly with the dimension of the output space as the
Hilbert-Schmidt norm does. On the other hand the Hilbert-schmidt norm makes the
studied random variables Hilbert space valued, for which it is much easier to
derive concentration inequalities \citep{smale2007learning, pinelis1994optimum,
naor2012banach}. Note that in the scalar case ($A(\omega)= 1$) the
Hilbert-Schmidt norm error and the operator norm are the same and measure the
deviation between $\tildeK{\omega}$ and $K$ as the absolute value of their
difference.
\paragraph{}
A raw concentration inequality of the kernel estimator gives the error on one
point. If one is interesting in bounding the maximum error over $N$ points,
applying a union bound on all the point would yield a bound that grows linearly
with $N$. This would suggest that when the number of points increase, even if
all of them are concentrated in a small subset of $\mathcal{X}$, we should draw
increasingly more features to have an error below $\epsilon$ with high
probability. However if we restrict ourselves to study the error on a compact
subset of $\mathcal{X}$ (and in practice data points lies often in a closed
bounded subset of $\mathbb{R}^d$), we can cover this compact subset by a finite
number of closed balls and apply the concentration inequality and the union
bound only on the center of each ball. Then if the function
$\norm{\tildeK{\omega}_e-K_e}$ is smooth enough on each ball (\acs{ie}
Lipschitz) we can guarantee with high probability that the error between the
centers of the balls will not be too high. Eventually we obtain a bound in the
worst case scenario on all the points in a subset $\mathcal{C}$ of
$\mathcal{X}$. This bound depends on the covering number
$\mathcal{N}(\mathcal{C}, r)$ of $\mathcal{X}$ with ball of radius $r$. When
$\mathcal{X}$ is a Banach space, the covering number is proportional to the
diameter of the diameter of $\mathcal{C}\subseteq\mathcal{X}$.
\paragraph{}
Prior to the presentation of general results, we briefly recall the uniform
convergence of \acs{RFF} approximation for a scalar shift invariant kernel on
the additive \acs{LCA} group $\mathbb{R}^d$ and introduce a direct corollary
about decomposable shift-invariant \acs{OVK} on the \acs{LCA} group
$(\mathbb{R}^d, +)$.
\subsection{Random Fourier Features in the scalar case and decomposable OVK}
\citet{Rahimi2007} proved the uniform convergence of \acf{RFF} approximation
for a scalar shift-invariant kernel on the \acs{LCA} group $\mathbb{R}^d$
endowed with the group operation $\groupop=+$. In the case of the
shift-invariant decomposable \acs{OVK}, an upper bound on the error can be
obtained as a direct consequence of the result in the scalar case obtained
by~\citet{Rahimi2007} and other authors~\citep{sutherland2015, sriper2015}.
\begin{theorem}[Uniform error bound for \ac{RFF},~\citet{Rahimi2007}]
    \label{rff-scalar-bound}
    Let $\mathcal{C}$ be a compact of subset of $\mathbb{R}^d$ of diameter
    $\abs{\mathcal{C}}$. Let $k$ be a shift invariant kernel, differentiable
    with a bounded second derivative and $\probability_{\rho}$ its normalized
    \acl{IFT} such that it defines a probability measure. Let
    \begin{dmath*}
        \widetilde{k}=\sum_{j=1}^D\cos{\inner{\cdot, \omega_j}}
        \hiderel{\approx} k(x,z) \enskip\text{and}\enskip
        \sigma^2\hiderel{=}\expectation_{\rho}
        \norm{\omega}^2_2.
    \end{dmath*}
    Then we have
    \begin{dmath*}
        \probability_{\rho}\Set{(\omega_j)_{j=1}^D |
        \norm{\tilde{k}-k}_{\mathcal {C}\times\mathcal{C}}\ge \epsilon } \le
        2^8\left( \frac{\sigma \abs{\mathcal{C}}}{\epsilon} \right)^2\exp\left(
        -\frac{\epsilon^2D}{4(d+2)} \right)
    \end{dmath*}
\end{theorem}
From \cref{rff-scalar-bound}, we can deduce the following corollary about the
uniform convergence of the \acs{ORFF} approximation of the decomposable kernel.
We recall that for a given pair $x$, $z$ in $\mathcal{C}$, $\tilde{K}(x,z)=
\tildePhi{\omega}(x)^* \tildePhi{\omega}(z)=\Gamma\tilde{k}(x,z)$ and
$K_0(x-z)=\Gamma \expectation_{\dual{\Haar},\rho}[\tilde{k}(x,z)]$.
\begin{corollary}[Uniform error bound for decomposable \acs{ORFF}]
    \label{c:dec-bound}
    Let $\mathcal{C}$ be a compact of subset of $\mathbb{R}^d$ of diameter
    $\abs{\mathcal{C}}$. Let $K$ be a decomposable kernel built from a positive
    operator self-adjoint $\Gamma$, and $k$ a shift invariant kernel with bounded
    second derivative such that
    \begin{dmath*}
        \widetilde{K}=\sum_{j=1}^D\cos{\inner{\cdot, \omega_j}}\Gamma
        \hiderel{\approx} K \enskip\text{and}\enskip
        \sigma^2\hiderel{=}\expectation_{\rho}
        \norm{\omega}^2_2.
    \end{dmath*}
    Then
    \begin{dmath*}
        \probability_{%
        \rho}\Set{(\omega_j)_{j=1}^D|\norm{\widetilde{K}-K}_{\mathcal{C} \times
        \mathcal{C}}\ge \epsilon } \le 2^8\left( \frac{\sigma
        \norm{\Gamma}_{\mathcal{Y},\mathcal{Y}} \abs{\mathcal{C}}}{\epsilon}
        \right)^2\exp\left( -\frac{\epsilon^2D}{4\norm{\Gamma}_2^2(d+2)} \right)
    \end{dmath*}
\end{corollary}
Please note that a similar corollary could have been obtained for the recent
result of~\citet{sutherland2015} who refined the bound proposed by Rahimi and
Recht by using a Bernstein concentration inequality instead of the Hoeffding
inequality. More recently~\citet{sriper2015} showed an optimal bound for
\acl{RFF}. The improvement of~\citet{sriper2015} is mainly in the constant
factors where the bound does not depend linearly on the diameter
$\abs{\mathcal{C}}$ of $\mathcal{C}$ but exhibit a logarithmic dependency
$\log\left(\abs{\mathcal{C}}\right)$, hence requiring significantly less random
features to reach a desired uniform error with high probability. Moreover,
\citet{sutherland2015} also considered a bound on the expected max error
$\expectation_{\dual{\Haar}, \rho} \norm{\widetilde{K}-K}_{\infty}$, which is
obtained using Dudley's entropy integral~\citep{dudley1967sizes, Boucheron} as
a bound on the supremum of an empirical process by the covering number of the
indexing set. This useful theorem is also part of the proof of
\citet{sriper2015}.
\subsection{Uniform convergence of \acpdfstring{ORFF} approximation on
\acpdfstring{LCA} groups}
In this analysis, we assume that $\mathcal{Y}$ is finite dimensional, in
\cref{remark:infinite_dimension}, we discuss how the proof could be extended to
infinite dimensional output Hilbert spaces. We propose a bound for \acl{ORFF}
approximation in the general case. It relies on two main ideas:
\begin{enumerate}
    \item a matrix-Bernstein concentration inequality for random matrices need
    to be used instead of concentration inequality for scalar random variables,
    \item a general theorem valid for random matrices with bounded norms such
    as decomposable kernel \acs{ORFF} approximation as well as un\-bound\-ed
    norms such as the \acs{ORFF} approximation we proposed for curl and
    divergence-free kernels that behave as subexponential random variables.
\end{enumerate}
Before introducing the new theorem, we give the definition of the Orlicz norm
which gives a proxy-bound on the norm of subexponential random variables.
\begin{definition}[Orlicz norm~\citep{van1996weak}]
    Let $\psi:\mathbb{R}_+\to\mathbb{R}_+$ be a non-decreasing convex function
    with $\psi(0)=0$. For a random variable $X$ on a measured space
    $(\Omega,\mathcal{T} (\Omega),\mu)$, the quantity
    \begin{dmath*}
        \norm{X}_{\psi} \hiderel{=} \inf \Set{C > 0  |
        \expectation_{\mu}[\psi\left( \abs{X}/C \right)]\le 1}.
    \end{dmath*}
    is called the Orlicz norm of $X$.
\end{definition}
Here, the function $\psi$ is chosen as $\psi(u)=\psi_{\alpha}(u)$ where
$\psi_{\alpha}(u) \colonequals e^{u^{\alpha}}-1$. When $\alpha=1$, a random
variable with finite Orlicz norm is called a \emph{subexponential variable}
because its tails decrease at an exponential rate. Let $X$ be a self-adjoint
random operator. Given a scalar-valued measure $\mu$, we call \emph{variance}
of an operator $X$ the quantity $\variance_{\mu}[X]=\expectation_
{\mu}[X-\expectation_{\mu}[X]]^2$. With this convention if $X$ is a $p\times
p$ Hermitian matrix,
\begin{dmath*}
    \variance_{\mu}[X]_{\ell m}=\sum_{r=1}^p\covariance{X_{\ell r}, X_{rm}}.
\end{dmath*}
Among the possible concentration inequalities adapted to random operators
\citep{tropp2015introduction, minsker2011some, ledoux2013probability,
pinelis1994optimum, koltchinskii2013remark}, we focus on the results of
\citet{tropp2015introduction, minsker2011some}, for their robustness to high or
potentially infinite dimension of the output space $\mathcal{Y}$. To guarantee
a good scaling with the dimension of $\mathcal{Y}$ we introduce the notion of
intrinsic dimension (or effective rank) of an operator.
\begin{definition}
    Let $A$ be a trace class operator acting on a Hilbert space
    $\mathcal{Y}$. We call intrinsic dimension the quantity
    \begin{dmath*}
        \intdim(A) = \frac{\Tr\left[A\right]}{\norm{A}_{\mathcal{Y},
        \mathcal{Y}}}.
    \end{dmath*}
\end{definition}
Indeed the bound proposed in our first publication at \acs{ACML}
\citep{brault2016random} based on \citet{koltchinskii2013remark} depends on $p$
while the present bound depends on the intrinsic dimension of the variance of
$A(\omega)$ which is always smaller than $p$ when the operator $A(\omega)$ is
Hilbert-Schmidt ($p\le\infty$).
\begin{corollary}
    \label{corr:unbounded_consistency}
    Let $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ be a
    shift-invariant $\mathcal{Y}$-Mercer kernel, where $\mathcal{Y}$ is a
    finite dimensional Hilbert space of dimension $p$ and $\mathcal{X}$ a
    finite dimensional Banach space of dimension $d$. Moreover, let
    $\mathcal{C}$ be a closed ball of $\mathcal{X}$ centred at the origin of
    diameter $\abs{\mathcal{C}}$,
    $A:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y})$ and
    $\probability_{\dual{\Haar},\rho}$ a pair such that
    \begin{dmath*}
        \tilde{K}_e = \sum_{j=1}^D \cos{\pairing{\cdot,\omega_j}}A(\omega_j)
        \hiderel{\approx}
        K_e\condition{$\omega_j\sim\probability_{\dual{\Haar}, \rho}$
        \acs{iid}.}.
    \end{dmath*}
    Let $\mathcal{D}_{\mathcal{C}}=\mathcal{C}\groupop\mathcal{C}^{-1}$ and
    \begin{dmath*}
        V(\delta) \succcurlyeq \variance_{\dual{\Haar},\rho}
        \tilde{K}_e(\delta) \condition{for all
        $\delta\in\mathcal{D}_{\mathcal{C}}$}
    \end{dmath*}
    and $H_\omega$ be the Lipschitz constant of the function $h: x\mapsto
    \pairing{x,\omega}$. If the three following constants exist
    \begin{dmath*}
        m \ge \int_{\dual{\mathcal{X}}} H_\omega
        \norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}} d\probability_{\dual{\Haar},
        \rho}(\omega) \hiderel{<} \infty
    \end{dmath*}
    and
    \begin{dmath*}
        u \ge 4\left(\norm{\norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}
        + \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
        \norm{K_e(\delta)}_{\mathcal{Y},\mathcal{Y}}\right) \hiderel{<} \infty
    \end{dmath*}
    and
    \begin{dmath*}
        v \ge \sup_{\delta\in\mathcal{D}_{\mathcal{C}}} D
        \norm{V(\delta)}_{\mathcal{Y}, \mathcal{Y}} \hiderel{<} \infty.
    \end{dmath*}
    Define $p_{int}\ge \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
    \intdim(V(\delta))$, then for all $0 < \epsilon \le m \abs{C}$,
    \begin{dmath*}
        \probability_{\dual{\Haar,\rho}}\Set{(\omega_j)_{j=1}^D |
        \norm{\tilde{K}-K}_{\mathcal{C}\times\mathcal{C}} \ge \epsilon}
        \le 8\sqrt{2} \left( \frac{m\abs{\mathcal{C}}}{\epsilon}
        \right)
        {\left(p_{int}r_{v/D}(\epsilon)\right)}^{\frac{1}{d + 1}}
        \begin{cases}
            \exp\left(-D\frac{\epsilon^2}{8
            v(d+1)\left(1 + \frac{1}{p}\right)}
            \right) \condition{$\epsilon \le
            \frac{v}{u}\frac{1+1/p}{K(v,
            p)}$} \\
            \exp\left(-D\frac{\epsilon}{8u(d+1)K(v,
            p)}\right)\condition{otherwise,}
        \end{cases}
    \end{dmath*}
    where $K(v, p)=\log\left(16 \sqrt{2}
    p\right)+\log\left(\frac{u^2}{v}\right) $ and $r_{v/D}(\epsilon)=1 +
    \frac{3}{\epsilon^2\log^2(1 + D \epsilon / v)}$.
\end{corollary}
We give a comprehensive full proof of the theorem in
\cref{subsec:concentration_proof}. It follows the usual scheme derived
in~\citet{Rahimi2007} and~\citet{sutherland2015} and involves Bernstein
concentration inequality for unbounded symmetric matrices
(\cref{th:Bernstein3}).

\subsection{Dealing with infinite dimensional operators}
\label{remark:infinite_dimension}
We studied the concentration of \acsp{ORFF} under the assumption that
$\mathcal{Y}$ is finite dimensional. Indeed a $d$ term characterizing the
dimension of the input space $\mathcal{X}$ appears in the bound proposed in
\cref{corr:unbounded_consistency}, and when $d$ tends to infinity, the
exponential part goes to zero so that the probability is bounded by a
constant greater than one. Unfortunately, considering unbounded random
operators \citet{minsker2011some} doesn't give any tighter solution.
\paragraph{}
In our first bound presented at \acs{ACML}, we presented a bound based on a
matrix concentration inequality for unbounded random variable. Compared to this
previous bound, \cref{corr:unbounded_consistency} does not depend on the
dimensionality $p$ of the output space $\mathcal{Y}$ but on the intrinsic
dimension of the operator $A(\omega)$. However to remove the dependency in $p$
in the exponential part, we must turn our attention to operator concentration
inequalities for bounded random variable. To the best of our knowledge we are
not aware of concentration inequalities working for \say{unbounded} operator-
valued random variables. Following the same proof than
\cref{corr:unbounded_consistency} we obtain
\begin{corollary}
    \label{corr:bounded_infinite_dim_consistency}
    Let $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ be a
    shift-invariant $\mathcal{Y}$-Mercer kernel, where $\mathcal{Y}$ is a
    Hilbert space and $\mathcal{X}$ a finite dimensional Banach space of
    dimension $D$. Moreover, let $\mathcal{C}$ be a closed ball of
    $\mathcal{X}$ centered at the origin of diameter $\abs{\mathcal{C}}$,
    subset of $\mathcal{X}$, $A:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y})$
    and $\probability_{\dual{\Haar},\rho}$ a pair such that
    \begin{dmath*}
        \tilde{K}_e = \sum_{j=1}^D \cos{\pairing{\cdot,\omega_j}}A (\omega_j)
        \hiderel{\approx}
        K_e\condition{$\omega_j\sim\probability_{\dual{\Haar}, \rho}$
        \acs{iid}.}
    \end{dmath*}
    where $A(\omega_j)$ is a Hilbert-Schmidt operator for all $j \in
    \mathbb{N}^*_D$. Let $\mathcal{D}_{\mathcal{C}}=\mathcal{C} \groupop
    \mathcal{C}^{-1}$ and
    \begin{dmath*}
        V (\delta) \succcurlyeq\variance_{\dual{\Haar},\rho}
        \tilde{K}_e (\delta) \condition{for all
        $\delta\in\mathcal{D}_{\mathcal{C}}$}
    \end{dmath*}
    and $H_\omega$ be the Lipschitz constant of the function $h: x\mapsto
    \pairing{x,\omega}$. If the three following constants exists
    \begin{dmath*}
        m \ge\int_{\dual{\mathcal{X}}} H_{\omega}
        \norm{A (\omega)}_{\mathcal{Y},\mathcal{Y}}
        d\probability_{\dual{\Haar}, \rho}(\omega) \hiderel{<} \infty{}
    \end{dmath*}
    and
    \begin{dmath*}
        u \ge\esssup_{\omega\in\dual{\mathcal{X}}}
        \norm{A (\omega)}_{\mathcal{Y}, \mathcal{Y}} +
        \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
        \norm{K_e (\delta)}_{\mathcal{Y}, \mathcal{Y}} \hiderel{<} \infty{}
    \end{dmath*}
    and
    \begin{dmath*}
        v \ge\sup_{\delta\in\mathcal{D}_{\mathcal{C}}} D
        \norm{V (\delta)}_{\mathcal{Y}, \mathcal{Y}} \hiderel{<} \infty.
    \end{dmath*}
    define $p_{int} \ge \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
    \intdim\left(V(\delta)\right)$ then for all $\sqrt{\frac{v}{D}} +
    \frac{u}{3D} < \epsilon < m\abs{\mathcal{C}}$,
    \begin{dmath*}
        \probability_{\dual{\Haar,\rho}}\Set{(\omega_j)_{j=1}^D |
        \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
        \norm{F (\delta)}_{\mathcal{Y}, \mathcal{Y}} \ge\epsilon} \le~8\sqrt{2}
        \left(\frac{m\abs{\mathcal{C}}}{\epsilon}\right) p_{int}^{\frac{1}{d +
        1}} \exp\left(-D\psi_{v,d,u} (\epsilon) \right)
    \end{dmath*}
    where $\psi_{v,d,u}(\epsilon)=\frac{\epsilon^2}{2(d+1)(v + u
    \epsilon / 3)}$.
\end{corollary}
Again a full comprehensive proof is given in \cref{subsec:concentration_proof}
of the appendix. Notice that in this result, The dimension
$p=\dim{\mathcal{Y}}$ does not appear. Only the intrinsic dimension of the
variance of the estimator. Moreover when $d$ is large, the term
$p_{int}^{\frac{1}{d + 1}}$ goes to one, so that the impact of the intrinsic
dimension on the bound vanish when the dimension of the input space is large.
subsection{Variance of the \acpdfstring{ORFF} approximation}
We now provide a bound on the norm of the variance of $\tilde{K}$, required to
apply \cref{corr:unbounded_consistency,corr:bounded_infinite_dim_consistency}.
This is an extension of the proof of \citet{sutherland2015} to the
operator-valued case, and we recover their results in the scalar case when
$A(\omega)=1$. An illustration of the bound is provided in
\cref{fig:approximation_error_var} for the decomposable and the curl-free
\acs{OVK}.
\begin{proposition}[Bounding the \emph{variance} of $\tilde{K}$]
    \label{pr:variance_bound}
    Let $K$ be a shift invariant $\mathcal{Y}$-Mercer
    kernel on a second countable \ac{LCA} topological space $\mathcal{X}$. Let
    $A:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y})$ and
    $\probability_{\dual{\Haar},\rho}$ a pair such that
    \begin{dmath*}
        \tilde{K}_e = \sum_{j=1}^D \cos{\pairing{\cdot,\omega_j}}A (\omega_j)
        \hiderel{\approx}
        K_e\condition{$\omega_j\sim\probability_{\dual{\Haar}, \rho}$
        \acs{iid}.}
    \end{dmath*}
    Then,
    \begin{dmath*}
        \variance_{\dual{\Haar}, \rho} \left[ \tilde{K}_e(\delta) \right]
        \preccurlyeq \frac{1}{2D} \left( \left( K_e(2\delta) + K_e(e) \right)
        \expectation_{\dual{\Haar}, \rho}\left[ A(\omega) \right] -
        2 K_e(\delta)^2 + \variance_{\dual{\Haar}, \rho}\left[
        A(\omega) \right]\right)
    \end{dmath*}
\end{proposition}
\begin{figure}
\begin{minipage}[c]{.46\linewidth}
    \centering\resizebox{\linewidth}{!}{%
    \input{./gfx/variance_dec.tikz}}
\end{minipage}
\begin{minipage}[c]{.54\linewidth}
    \centering\resizebox{\linewidth}{!}{%
    \input{./gfx/variance_curl.tikz}}
\end{minipage}
    \caption[ORFF variance bound]{Comparison between an empirical bound on the
    norm of the variance of the decomposable (left) and  curl-free (right) ORFF
    obtained and the theoretical bound proposed in \cref{pr:variance_bound}
    versus $D$. \label{fig:approximation_error_var}}
\end{figure}
subsection{Application on decomposable, curl-free and divergence-free
\acpdfstring{OVK}}
First, the two following examples discuss the form of $H_\omega$ for the
additive group and the skewed-multiplicative group. Here we view
$\mathcal{X}=\mathbb{R}^d$ as a Banach space endowed with the Euclidean norm.
Thus the Lipschitz constant $H_{\omega}$ is bounded by the supremum of the norm
of the gradient of $h_{\omega}$.
\begin{example}[Additive group]
    On the additive group, $h_\omega(\delta)=\inner{\omega, \delta}$. Hence
    $H_\omega=\norm{\omega}_2$.
\end{example}
\begin{example}[Skewed-multiplicative group]
    On the skewed multiplicative group, $h_\omega(\delta)=\inner{\omega,
    \log(\delta+c)}$. Therefore
    \begin{dmath*}
        \sup_{\delta\in\mathcal{C}}\norm{\nabla
        h_\omega(\delta)}_2 = \sup_{\delta\in\mathcal{C}}\norm{\omega/(\delta +
        c)}_2.
    \end{dmath*}
    Eventually $\mathcal{C}$ is compact subset of $\mathcal{X}$ and finite
    dimensional thus $\mathcal{C}$ is closed and bounded. Thus
    $H_\omega=\norm{\omega}_2/(\min_{\delta\in\mathcal{C}} \norm{\delta}_2+c)$.
\end{example}
Now we compute upper bounds on the norm of the variance and Orlicz norm of the
three \acsp{ORFF} we took as examples.
\subsubsection{Decomposable kernel}
notice that in the case of the Gaussian decomposable kernel, \acs{ie}
$A(\omega)=A$, $e=0$, $K_0(\delta)= Ak_0(\delta)$, $k_0(\delta) \geq 0$ and
$k_0(\delta)=1$, then we have
\begin{equation*}
    D\norm{\variance_\mu \left[ \tilde{K}_0(\delta)
    \right]}_{\mathcal{Y},\mathcal{Y}}\leq
    (1+k_0(2\delta))\norm{A}_{\mathcal{Y},\mathcal{Y}}/2 + k_0(\delta)^2.
\end{equation*}
\subsubsection{Curl-free and divergence-free kernels:}
recall that in this case $p=d$. For the (Gaussian) curl-free kernel,
$A(\omega)=\omega\omega^*$ where $\omega\in\mathbb{R}^d\sim\mathcal{N}(0,
\sigma^{-2}I_d)$ thus $\expectation_\mu [A(\omega)] = I_d/\sigma^2$ and
$\variance_{\mu}[A(\omega)]=(d+1)I_d/\sigma^4$. Hence,
\begin{equation*}
    D\norm{\variance_\mu \left[ \tilde{K}_0(\delta)
    \right]}_{\mathcal{Y},\mathcal{Y}} \leq
    \frac{1}{2}\norm{\frac{1}{\sigma^2}K_0(2\delta)-2
    K_0(\delta)^2}_{\mathcal{Y},\mathcal{Y}} + \frac{(d+1)}{\sigma^4}.
\end{equation*}
This bound is illustrated by \cref{fig:approximation_error} B, for a given
datapoint. Eventually for the Gaussian divergence-free kernel,
$A(\omega)=I\norm{\omega}_2^2-\omega\omega^*$, thus $\expectation_\mu
[A(\omega)] = I_d(d-1)/\sigma^2$ and $
\variance_{\mu}[A(\omega)]=d(4d-3)I_d/\sigma^4$. Hence,
\begin{equation*}
    D\norm{\variance_\mu \left[ \tilde{K}_0(\delta)
    \right]}_{\mathcal{Y},\mathcal{Y}} \leq
    \frac{1}{2}\norm{\frac{(d-1)}{\sigma^2}K_0(2\delta)-2
    K_0(\delta)^2}_{\mathcal{Y}, \mathcal{Y}}+ \frac{d(4d-3)}{\sigma^4}.
\end{equation*}
To conclude, we ensure that the random variable $\norm{A(\omega)}_{\mathcal{Y},
\mathcal{Y}}$ has a finite Orlicz norm with $\psi=\psi_1$ in these three cases.
\subsubsection{Computing the Orlicz norm}
for a random variable with strictly monotonic moment generating function (MGF),
one can characterize its inverse $\psi_1$ Orlicz norm by taking the functional
inverse of the MGF evaluated at 2 (see \cref{lm:orlicz_mgf} of the
appendix). In other words
$\norm{X}_{\psi_1}^{-1}=\MGF(x)^{-1}_X(2)$. For the Gaussian curl-free and
divergence-free kernel,
\begin{dmath*}
    \norm{A^{div}(\omega)}_{\mathcal{Y},\mathcal{Y}} =
    \norm{A^{curl}(\omega)}_{\mathcal{Y},\mathcal{Y}} \hiderel{=}
    \norm{\omega}_{2}^2,
\end{dmath*}
where$\omega\sim\mathcal{N}(0,I_d/\sigma^2)$, hence $\norm{A(\omega)}_2\sim
\Gamma(p/2,2/\sigma^2)$. The MGF of this gamma distribution is
$\MGF(x)(t)=(1-2t/\sigma^2)^{-(p/2)}$. Eventually
\begin{equation*}
    \norm{\norm{A^{div}(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}^{-1} =
    \norm{\norm{A^{curl}(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}^{-1} =
    \frac{\sigma^2}{2}\left(1-4^{-\frac{1}{p}}\right).
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learning with {OVK}}
\label{sec:learning_with_operator-valued_random-fourier_features} Before
focusing on learning function with an ORFF model, we briefly review the context
of supervised learning in \acs{vv-RKHS}.  model.
\subsection{Supervised learning within \acs{vv-RKHS}}
\subsubsection{General results}
Let $\seq{s} = (x_i,y_i)_{i=1}^N\in\left(\mathcal{X}\times\mathcal{Y}\right)^N$
be a sequence of training samples. Given a local loss function $L:
\mathcal{X}\times\mathcal{F}\times\mathcal{Y}\to \overline{\mathbb{R}}$ such
that $L$ is proper, convex and lower semi-continuous in $\mathcal{F}$, we are
interested in finding a \emph{vector-valued function}
$f_{\seq{s}}:\mathcal{X}\to\mathcal{Y}$, that lives in a \acs{vv-RKHS} and
minimize a tradeoff between a data fitting term $L$ and a regularization term
to prevent from overfitting. Namely finding $f_{\seq{s}}\in\mathcal{H}_K$ such
that
\begin{dmath}
    f_{\seq{s}} = \argmin_{f\in\mathcal{H}_K}
    \frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, f, y_i) +
    \frac{\lambda}{2}\norm{f}^2_{K}
    \label{eq:learning_rkhs}
\end{dmath}
where $\lambda\in\mathbb{R}_+$ is a regularization\footnote{Tychonov
regularization.} parameter. We call the quantity
\begin{dmath*}
    \mathcal{R}_{\seq{s}}(f)=\frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, f, y_i)
    \condition{$\forall f\in\mathcal{H}_K$, $\forall
    \seq{s}\in\left(\mathcal{X}\times\mathcal{Y}\right)^N$.}
\end{dmath*}
the empirical risk of the model $f\in\mathcal{H}_K$ according the
local loss $L$. A common choice for $L$ is the quadratic loss $L:(x,
f, y) \mapsto \norm{f(x)-y}_{\mathcal{Y}}^2$.  We introduce a corollary from
Mazur and Schauder proposed in 1936 (see~\citet{kurdila2006convex,
gorniewicz1999topological}) showing that \cref{eq:learning_rkhs} --and
\cref{eq:learning_rkhs_gen}-- attains a unique mimimizer.
\begin{theorem}[Mazur-Schauder]
    \label{cor:unique_minimizer}
    Let $\mathcal{H}$ be a Hilbert space and $J:\mathcal{H}\to
    \overline{\mathbb{R}}$ be a proper, convex, lower semi-continuous and
    coercive function. Then $J$ is bounded from below and attains a minimizer.
    Moreover if $J$ is strictly convex the minimizer is unique.
\end{theorem}
This is easily verified for Ridge regression. Define
\begin{dmath}
    \label{eq:ridge}
    J_\lambda(f)=\frac{1}{N}\sum_{i=1}^N\norm{f(x_i)-y_i}_{\mathcal{Y}}^2+
    \frac{\lambda}{2}\norm{f}_K^2,
\end{dmath}
where $f\in\mathcal{H}_K$ and $\lambda\in\mathbb{R}_{>0}$. $J_\lambda$ is
continuous\footnote{Reminder, if $f\in\mathcal{H}_k, \text{ev}_x : f\mapsto
f(x)$ is continuous, see \cref{pr:unique_rkhs}.} and strictly convex.
Additionally $J_\lambda$ is coercive since $\norm{f}_K$ is coercive,
$\lambda\in\mathbb{R}_{>0}$, and all the summands of $J_\lambda$ are positive.
Hence for all positive $\lambda$, $f_{\seq{s}} =
\argmin_{f\in\mathcal{H}_K}J_\lambda(f)$ exists, is unique and attained.
%\begin{remark}[\citet{kadri2015operator}]
    %\label{rk:rkhs_bound} We consider the optimization problem proposed in
    %\cref{eq:ridge} where $L:(x_i, f, y_i) \mapsto
    %\norm{f(x_i)-y_i}_{\mathcal{Y}}^2$. If given a training sample $\seq{s}$,
    %we have
    %\begin{dmath*}
        %\frac{1}{N}\sum_{i=1}^N\norm{y_i}_{\mathcal{Y}}^2 \le \sigma_y^2,
    %\end{dmath*}
    %then $\lambda\norm{f_{\seq{s}}}_K\le 2\sigma_y^2$. Indeed, since
    %$\mathcal{H}_K$ is a Hilbert space, $0\in\mathcal{H}_K$, thus
    %\begin{dmath*}
        %\frac{\lambda}{2}\norm{f_{\seq{s}}}^2_{K} \le
        %\frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, f_{\seq{s}}, y_i) +
        %\frac{\lambda}{2}\norm{f_{\seq{s}}}^2_{K} \le
        %\frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, 0, y_i) \hiderel{\le}
        %\sigma_y^2 \condition{by optimality of $f_{\seq{s}}$.}
    %\end{dmath*}
    %Since for all $x\in\mathcal{X}$, $\norm{f(x)}_{\mathcal{Y}}\le
    %\sqrt{\norm{K(x, x)}_{\mathcal{Y},\mathcal{Y}}}\norm{f}_{K}$, the maximum
    %value that the solution $\norm{f_{\seq{s}}(x)}_{\mathcal{Y}}$ of
    %\cref{eq:ridge} can reach is $\sigma_y\sqrt{\frac{2\norm{K(x,
    %x)}_{\mathcal{Y}, \mathcal{Y}}}{\lambda}}$. Thus when solving a Ridge
    %regression problem, given a shift-invariant kernel $K_e$, one should choose
    %\begin{dmath*}
        %0 \hiderel{<} \lambda \hiderel{\le}
        %2\norm{K_e(e)}_{\mathcal{Y}, \mathcal{Y}}\frac{\sigma_y^2}{C^2}.
    %\end{dmath*}
    %with $C\in\mathbb{R}_{>0}$ to have a chance to fit all the $y_i$ with norm
    %$\norm{y_i}_{\mathcal{Y}} \le C$ in the train set.
%\end{remark}

\subsubsection{Representer theorem and Feature equivalence}
Regression in \acl{vv-RKHS} has been well studied~\citep{Alvarez2012,
Argyriou_jmlr09,
Minh_icml13,minh2016unifying,sangnier2016joint,kadri2015operator,Micchelli2005,
Brouard2016_jmlr}, and a cornerstone of learning in \acs{vv-RKHS} is the
representer theorem\footnote{Sometimes referred to as minimal norm
interpolation theorem.}, which allows to replace the search of a minimizer in a
infinite dimensional \acs{vv-RKHS} by a finite number of parameters
$(u_i)_{i=1}^N$, $u_i\in\mathcal{Y}$.
\paragraph{}
In the following we suppose we are given a cost function
$c:\mathcal{Y}\times\mathcal{Y}\to\overline{\mathbb{R}}$, such that $c(f(x),y)$
returns the error of the prediction $f(x)$ \acs{wrt}~the ground truth $y$. A
loss function of a model $f$ with respect to an example
$(x,y)\in\mathcal{X}\times\mathcal{Y}$ can be naturally defined from a cost
function as $L(x,f,y)=c(f(x),y)$. Conceptually the function $c$ evaluates the
quality of the prediction versus its ground truth $y\in\mathcal{Y}$ while the
loss function $L$ evaluates the quality of the model $f$ at a training point
$(x,y)\in\mathcal{X}\times\mathcal{Y}$.
\begin{theorem}[Representer theorem]
    \label{th:representer}
    Let $K$ be a $\mathcal{Y}$-Mercer \acl{OVK} and $\mathcal{H}_K$ its
    corresponding $\mathcal{Y}$-Reproducing Kernel Hilbert space.
    \paragraph{}
    Let $c:\mathcal{Y}\times\mathcal{Y}\to\overline{\mathbb{R}}$ be a cost
    function such that $L(x, f, y)=c(Vf(x), y)$ is a proper convex lower
    semi-continuous function in $f$ for all $x\in\mathcal{X}$ and all
    $y\in\mathcal{Y}$.
    \paragraph{}
    Eventually let $\lambda\in\mathbb{R}_{>0}$ be the Tychonov regularization
    hyperparameters The solution $f_{\seq{s}}\in\mathcal{H}_K$ of the
    regularized optimization problem
    \begin{dmath}
        f_{\seq{s}} = \argmin_{f\in\mathcal{H}_K}
        \frac{1}{N}\displaystyle\sum_{i=1}^N c(f(x_i), y_i) +
        \frac{\lambda}{2}\norm{f}^2_{K}
        \label{eq:learning_rkhs_gen}
    \end{dmath}
    has the form $f_{\seq{s}}=\sum_{j=1}^{N}K(\cdot,x_j)u_{\seq{s},j}$ where
    $u_{\seq{s},j}\in\mathcal{Y}$ and
    \begin{dmath}
        \label{eq:argmin_u} u_{\seq{s}} =
        \argmin_{u\in\Vect_{i=1}^{N}\mathcal{Y}}\frac{1}{N}
        \displaystyle\sum_{i=1}^N c\left(\sum_{k=1}^{N}K(x_i,x_j)u_j,
        y_i\right) + \frac{\lambda}{2}\sum_{k=1}^{N}u_i^\adjoint
        K(x_i,x_k)u_k.
    \end{dmath}
\end{theorem}
The first representer theorem was introduced by~\citet{Wahba90} in the
case where $\mathcal{Y}=\mathbb{R}$. The extension to an arbitrary Hilbert
space $\mathcal{Y}$ has been proved by many authors in different
forms~\citep{Brouard2011,kadri2015operator,Micchelli2005}. The idea behind the
representer theorem is that even though we minimize over the whole space
$\mathcal{H}_K$, when $\lambda>0$, the solution of
\cref{eq:learning_rkhs_gen} falls inevitably into the set
\begin{dmath*}
    \mathcal{H}_{K, \seq{s}}=\Set{\sum_{j=1}^{N}K_{x_j}u_j| \forall
    (u_i)_{i=1}^{N} \in\mathcal{Y}^{N}}.
\end{dmath*}
Therefore the result can be expressed as a finite linear combination of basis
functions of the form $K(\cdot,x_k)$. Remark that we can perform the kernel
expansion of $f_{\seq{s}}=\sum_{j=1}^{N}K(\cdot,x_j)u_{\seq{s},j}$ even though
$\lambda=0$. However $f_{\seq{s}}$ is no longer the solution of
\cref{eq:learning_rkhs_gen} over the whole space $\mathcal{H}_K$ but a
projection on the subspace $\mathcal{H}_{K, \seq{s}}$. While this is in general
not a problem for practical applications, it might raise issues for further
theoretical investigations. In particular, it makes it difficult to perform
theoretical comparison of the \say{exact} solution of
\cref{eq:learning_rkhs_gen} with respect to the \acs{ORFF} approximation
solution given in \cref{th:orff_representer}.
The representer theorem show that minimizing a functional in a \acs{vv-RKHS}
yields a solution which depends on all the points in the training set. Assuming
that for all $x_i$, $x\in\mathcal{X}$ and for all $u_i\in\mathcal{Y}$ it takes
time $O(P)$, to compute $K(x_i, x)u_i$, making a prediction using the
representer theorem takes $O(NP)$. Obviously If $\mathcal{Y}=\mathbb{R}^p$,
Then $P=O(p^2)$ thus making a prediction cost $O(Np^2)$ operations.
%% on commence la partie approche
\subsection{Learning with Operator Random Fourier Feature maps}
Instead learning a model $f$ that depends on all the points of the training
set, we would like to learn a parametric model of the form
$\tildef{\omega}(x) = \tildePhi{\omega}(x)^\adjoint \theta$, where $\theta$
lives in some space $\tildeH{\omega}$. We are interested in
finding a parameter vector $\theta_{\seq{s}}$ such that
\begin{dmath}
    \label{eq:argmin_applied} \theta_{\seq{s}}=\argmin_{\theta\in
    \tildeH{\omega}}
    \frac{1}{N}\sum_{i=1}^Nc\left(\tildePhi{\omega}(x_i)^\adjoint \theta,
    y_i\right) + \frac{\lambda}{2}\norm{\theta}^2_{\tildeH{\omega}}
\end{dmath}
The following theorem states that when $\lambda > 0$ then learning with a
feature map is equivalent to learn with a kernel. Moreover if
$f_{\seq{s}}\in\mathcal{H}_K$ is a solution of \cref{eq:argmin_RKHS_rand} and
$\theta_{\seq{s}}\in\mathcal{H}$ is the solution of
\cref{eq:argmin_RKHS_rand}, then $f_{\seq{s}}=\Phi(\cdot)^\adjoint
\theta_{\seq{s}}$. To the best of our knowledge no such results exist in the
litterature even for scalar-valued kernel. Usually the author suppose that
the class of function $\mathcal{H}_K$ can be replace with $\mathcal{H}$ but
do not study the link between these spaces.
\begin{theorem}[Feature equivalence]
    \label{th:orff_representer} Let $\tildeK{\omega}$ be an \acl{OVK} such that
    for all $x$, $z\in\mathcal{X}$, $\tildePhi{\omega}(x)^\adjoint
    \tildePhi{\omega}(z) = \widetilde{K}(x,z)$ where $\widetilde{K}$ is a
    $\mathcal{Y}$-Mercer \acs{OVK} and $\mathcal{H}_{\tildeK{\omega}}$ its
    corresponding $\mathcal{Y}$-Reproducing kernel Hilbert space.
    \paragraph{}
    Let $c:\mathcal{Y}\times\mathcal{Y}\to\overline{\mathbb{R}}$ be a cost
    function such that $L\left(x, \widetilde{f},
    y\right)=c\left(\widetilde{f}(x), y\right)$ is a proper convex lower
    semi-continuous function in $\widetilde{f}\in\mathcal{H}_{\tildeK{\omega}}$
    for all $x\in\mathcal{X}$ and all $y\in\mathcal{Y}$.
    \paragraph{}
    Eventually let $\lambda\in\mathbb{R}_{>0} \mathbb{R}_+$ be the Tychonov
    regularization hyperparameter. The solution
    $f_{\seq{s}}\in\mathcal{H}_{\tildeK{\omega}}$ of the regularized
    optimization problem
    \begin{dmath}
        \label{eq:argmin_RKHS_rand} \widetilde{f}_{\seq{s}} =
        \argmin_{\widetilde{f}\in\mathcal{H}_{\tildeK{\omega}}}
        \frac{1}{N}\displaystyle\sum_{i=1}^N c\left(\widetilde{f}(x_i),
        y_i\right) +
        \frac{\lambda}{2}\norm{\widetilde{f}}^2_{\tildeK{\omega}}
    \end{dmath}
    has the form $\widetilde{f}_{\seq{s}} = \tildePhi{\omega}(\cdot)^\adjoint
    \theta_{\seq{s}}$, where $\theta_{\seq{s}} \in (\Ker
    \tildeW{\omega})^{\perp}$ and
    \begin{dmath}
        \theta_{\seq{s}}=\argmin_{\theta\in \tildeH{\omega}}
        \frac{1}{N}\sum_{i=1}^Nc\left(\tildePhi{\omega}(x_i)^\adjoint \theta,
        y_i\right) + \frac{\lambda}{2}\norm{\theta}^2_{\tildeH{\omega}}
        \label{eq:arming_RKHS_rand_feat}
    \end{dmath}
\end{theorem}
In the aforementioned theorem, we use the notation $\widetilde{K}$ and
$\tildePhi{\omega}$ because our main subject of interest is the \acs{ORFF} map.
However this theorem works for \emph{any} feature maps $\Phi(x)\in
\mathcal{L}(\mathcal{Y}, \mathcal{H})$ even when $\mathcal{H}$ is infinite
dimensional.\footnote{If $\Phi(x): \mathcal{L}(\mathcal{Y}, \mathcal{H})$ and
$\dim(\mathcal{H})=\infty$, the decomposition $\mathcal{H}=(\Ker W) \oplus
(\Ker W)^\perp$ holds since $\mathcal{H}$ is a Hilbert space and $W$ is a
bounded operator.}.  This shows that when $\lambda>0$ the solution of
\cref{eq:argmin_u} with the approximated kernel $K(x,z) \approx
\tildeK{\omega}(x,z) = \tildePhi{\omega}(x)^\adjoint\tildePhi{\omega}(z)$ is
the same than the solution of \cref{eq:argmin_theta} up to a linear
transformation. Namely, if $u_{\seq{s}}$ is the solution of \cref{eq:argmin_u},
$\theta_{\seq{s}}$ is the solution of \cref{eq:argmin_theta} and $\lambda>0$ we
have
\begin{dmath*}
    \theta_{\seq{s}} = \sum_{i=1}^{N} \tildePhi{\omega}(x_i) (u_{\seq{s}})_i
    \hiderel{\in} (\Ker W)^{\perp} \hiderel{\subseteq} \tildeH{\omega}.
\end{dmath*}
If $\lambda_K=0$ we can still find a solution $u_{\seq{s}}$ of
\cref{eq:argmin_u}. By construction of the kernel expansion, we have
$u_{\seq{s}}\in(\Ker W)^\bot$. However looking at the proof of
\cref{th:orff_representer} we see that $\theta_{\seq{s}}$ might \emph{not}
belong to $(\Ker W)^\bot$. We can compute a residual vector
\begin{dmath*}
    r_{\seq{s}} = \sum_{i=1}^{N} \tildePhi{\omega}(x_i)
    (u_{\seq{s}})_i - \theta_{\seq{s}}.
\end{dmath*}
Since $\sum_{j=1}^N \tildePhi{\omega}(x_j)\in(\Ker W)^\bot$ by
construction, if $r_{\seq{s}}=0$, it means that $\lambda_K$ is large
enough for both representer theorem and \acs{ORFF} representer theorem to
apply. If $r_{\seq{s}}\neq 0$ but $\tildePhi{\omega}(\cdot)^\adjoint
r_{\seq{s}} = 0$ it means that both $\theta_{\seq{s}}$ and $\sum_{j=1}^{N}
\tildePhi{\omega}(x_j) u_{\seq{s}}$ are in $(\Ker W)^\bot$, thus the
representer theorem fails to find the \say{true} solution over the whole space
$\mathcal{H}_{\widetilde{K}}$ but returns a projection onto
$\mathcal{H}_{\tildeK{\omega},\seq{s}}$ of the solution. If $r_{\seq{s}} \neq
0$ and $\tildePhi{\omega}(\cdot)^\adjoint r_{\seq{s}} \neq 0$ means that
$\theta_{\seq{s}}$ is \emph{not} in $(\Ker W)^\bot$, thus the feature
equivalence theorem fails to apply. Since $r_{\seq{s}} = \sum_{i=1}^N
\tildePhi{\omega}(x_i)(u_{\seq{s}})_i - \theta_{\seq{s}}^\perp -
\theta_{\seq{s}}^\parallel$ and $\sum_{i=1}^N
\tildePhi{\omega}(x_i)(u_{\seq{s}})_i$ is in $(\Ker W)^\perp$, with mild abuse
of notation we write $r_{\seq{s}}=\theta^\parallel$. This remark is illustrated
in \cref{fig:representer,fig:representer2}.
\paragraph{}
In \cref{fig:representer}, we generated the data from a since wave to which we
add some gaussian noise. We learned a gaussian kernel based \ac{RFF} model
(blue curve) and a kernel model (yellow curve) where the kernel is obtained
from the \acs{RFF} map. The left column represents the fit of the model to the
points for four different valued of $\lambda$ (top to bottom: $10^{-2}$,
$10^{-5}$, $10e^{10}$, $0$). The middle column shows if the \acs{RFF} solution
$\theta_{\seq{s}}$ is in $(\Ker \tilde{W})^\perp$.  This is true for all values
of $\lambda$. The right column shows that even though $\theta_{\seq{s}}$ is in
$(\Ker \tilde{W})^\perp$, when $\lambda\to0$ learning with \acs{RFF} is
different from learning with the kernel constructed from the \acs{RFF} maps
since the coefficients of $\theta^{\parallel}$ are all different from $0$.
\paragraph{}
\cref{fig:representer2} is the same setting than \cref{fig:representer} except
that we decreased the scale parameter $\sigma$ of the gaussian kernel to make
it overfit, and emphasize that when $\lambda=0$, $\theta_{\seq{s}}$ might not
belong to $(\Ker \tilde{W})^\perp$, as represented on the middle column.

\begin{pycode}[representer]
sys.path.append('./src/')
import representer

err = representer.main()
\end{pycode}

\begin{pycode}[representer2]
sys.path.append('./src/')
import representer2

err = representer2.main()
\end{pycode}

\afterpage{%
\begin{landscape}
    \begin{figure}[tb]
        \pyc{print(r'\centering\resizebox{1.5\textwidth}{!}{\input{./representer.pgf}}')}
        \caption[\acs{ORFF} equivalence theorem.]{\acs{ORFF} equivalence
        theorem. \label{fig:representer}}
    \end{figure}
    \clearpage
    \begin{figure}[tb]
        \pyc{print(r'\centering\resizebox{1.5\textwidth}{!}{\input{./representer2.pgf}}')}
        \caption[\acs{ORFF} equivalence theorem with overfitting.]{\acs{ORFF}
        equivalence theorem with overfitting. \label{fig:representer2}}
    \end{figure}
\end{landscape}}

\section{Solving ORFF-based regression}\label{subsec:gradient_methods}
% We illustrate the ORFF representer theorem (\cref{cr:orff_representer}) on
% two experiment involving scalar valued kernels.
In order to find a solution to \cref{eq:argmin_theta}, we first turn our
attention to gradient descent methods. In the following we let
\begin{dmath}
    \label{eq:cost_functional} J_{\lambda}(\theta) =
    \frac{1}{N}\sum_{i=1}^Nc\left(\tildePhi{\omega}(x_i)^\adjoint \theta,
    y_i\right) + \frac{\lambda}{2}\norm{\theta}^2_{\tildeH{\omega}}
\end{dmath}
Then, we study the complexity in time of the proposed algorithm.
\subsection{Gradient descent methods}\label{subsec:gradient_methods}
Since the solution of \cref{eq:argmin_theta} is
unique when $\lambda>0$, a sufficient and necessary condition is that the
gradient of $J_{\lambda}$ at the minimizer $\theta_{\seq{s}}$ is zero. We use
the Frechet derivative, the strongest notion of derivative in Banach
spaces\footnote{Here we view the Hilbert space $\mathcal{H}$ (feature space)
as a reflexive Banach space.}~\citep{conway2013course, kurdila2006convex} which
directly generalizes the notion of gradient to Banach spaces. A function
$f:\mathcal{H}_0\to\mathcal{H}_1$ is call Frechet differentiable at
$\theta_0\in \mathcal{H}_0$ if there exist a bounded linear operator
$A\in\mathcal{L}(\mathcal{H}_0,\mathcal{H}_1)$ such that
\begin{dmath*}
    \lim_{\norm{h}_{\mathcal{H}_0}\to 0} \frac{\norm{f(\theta_0 + h) -
    f(\theta_0) - Ah}_{\mathcal{H}_1}}{\norm{h}_{\mathcal{H}_0}} = 0
\end{dmath*}
We write
\begin{dmath*}
    (D_Ff)(\theta_0)
    \hiderel{=}\derivativeat{f(\theta)}{\theta}{\theta_0}
    \hiderel{=}A
\end{dmath*}
and call it Frechet derivative of $f$ with respect to $\theta$ at $\theta_0$.
With mild abuse of notation we write
\begin{dmath*}
    \derivativeat{f(\theta)}{\theta}{\theta_0}
    =\derivative{f(\theta_0)}{\theta_0}.
\end{dmath*}
The chain rule is valid in this context \cite[theorem 4.1.1 page
140]{kurdila2006convex}. Namely, let $\mathcal{H}_0$, $\mathcal{H}_1$ and
$\mathcal{H}_2$ be three Hilbert spaces. If a function
$f:\mathcal{H}_0\to\mathcal{H}_1$ is Frechet differentiable at $\theta$ and
$g:\mathcal{H}_1\to \mathcal{H}_2$ is Frechet differentiable at $f(\theta)$
then $g\circ f$ is Frechet differentiable at $\theta$ and for all
$h\in\mathcal{H}_0$
\begin{dmath*}
    \lderivative{(g\circ f)(\theta)}{\theta}\circ h
    =\derivative{g(f(\theta))}{f(\theta)} \circ
    \derivative{f(\theta)}{\theta}\circ h,
\end{dmath*}
or equivalently,
\begin{dmath*}
    D_F(g\circ f)(\theta)\circ h
    = (D_Fg)(f(\theta)) \circ (D_Ff)(\theta)\circ h.
\end{dmath*}
If $f:\mathcal{H}\to\mathbb{R}$ then $(D_F f)(\theta_0)
\in\mathcal{H}^\adjoint$ for all $\theta_0\in\mathcal{H}$, and by Riesz's
representation theorem we define the gradient of $f$ noted $\nabla_{\theta}
f(\theta)\in\mathcal{H}$ as the the vector in $\mathcal{H}$ such that
\begin{dmath*}
    \inner{\nabla_{\theta} f(\theta), h}_{\mathcal{H}} = (D_Ff)(\theta)\circ h
    \hiderel{=} \derivative{f(\theta)}{\theta} \circ h.
\end{dmath*}
For a function $f:\mathcal{H}_0\to\mathcal{H}_1$ we note the jacobian of $f$ as
$\jacobian_{\theta} f(\theta) = \derivative{f(\theta)}{\theta}$. In this
context if $f:\mathcal{H}_0\to\mathcal{H}_1$ and $g:\mathcal{H}_1\to\mathbb{R}$
the chain rule reads for all $h\in\mathcal{H}_0$
\begin{dmath*}
    \lderivative{(g\circ f)(\theta)}{\theta} \circ h
    = \derivative{g(f(\theta))}{f(\theta)} \circ \jacobian_{\theta}f(\theta)
    \circ h.
\end{dmath*}
By Riesz's representation theorem,
\begin{dmath*}
    \inner{\nabla_\theta(g\circ f)(\theta), h}_{\mathcal{H}_0}
    = \inner{\nabla_{f(\theta)}g(f(\theta)) ,
    \jacobian_{\theta}f(\theta)h}_{\mathcal{H}_0}
    = \inner{\left( \jacobian_{\theta} f(\theta) \right)^\adjoint
    \nabla_{f(\theta)} g(f(\theta)), h}_{\mathcal{H}_0}
\end{dmath*}
Hence
\begin{dmath*}
    \nabla_{\theta}(g\circ f)(\theta) =
    \left(\jacobian_{\theta}f(\theta)\right)^\adjoint
    \nabla_{f(\theta)}g(f(\theta)).
\end{dmath*}
Thus by linearity and applying the chaine rule to \cref{eq:argmin_theta} we
have
\begin{dgroup*}
    \begin{dmath*}
        \nabla_{\theta}c\left(\tildePhi{\omega}(x_i)^\adjoint \theta,
        y_i\right)= \tildePhi{\omega}(x_i)V^\adjoint
        \left(\lderivativeat{c\left(y,
        y_i\right)}{y}{\tildePhi{\omega}(x_i)^\adjoint
        \theta}\right)^\adjoint,
    \end{dmath*}
    \begin{dmath*}
        \nabla_{\theta}\norm{\theta}^2_{\tildeH{\omega}}=2\theta.
    \end{dmath*}
\end{dgroup*}
Provided that $c(y,y_i)$ is Frechet differentiable \acs{wrt}~$y$, for all $y$
and $y_i\in\mathcal{Y}$ we have $\nabla_{\theta} J_{\lambda}(\theta) \in
\tildeH{\omega}$ and
\begin{dmath}
    \label{eq:grad_final}
    \nabla_{\theta} J_{\lambda}(\theta) = \frac{1}{N}\sum_{i=1}^N
    \tildePhi{\omega}(x_i) \left(\lderivativeat{c\left(y,
    y_i\right)}{y}{\tildePhi{\omega}(x_i)^\adjoint \theta}\right)^\adjoint +
    \lambda\theta
\end{dmath}
\begin{example}[Naive closed form for the squared error cost]
    Consider the cost function defined for all $y$, $y'\in\mathcal{Y}$ by
    $c(y,y')=\frac{1}{2}\norm{y-y}_{\mathcal{Y}}^2$. Then
    \begin{dmath*}
        \left(\lderivativeat{c\left(y,
        y_i\right)}{y}{\tildePhi{\omega}(x_i)^\adjoint \theta}\right)^\adjoint
        = \left(\tildePhi{\omega}(x_i)^\adjoint \theta-y_i\right).
    \end{dmath*}
    Thus, since the optimal solution $\theta_{\seq{s}}$ verifies
    $\nabla_{\theta_{\seq{s}}} J_{\lambda}(\theta_{\seq{s}}) = 0$ we have
    \begin{dmath*}
        \frac{1}{N}\sum_{i=1}^N
        \tildePhi{\omega}(x_i)\left(\tildePhi{\omega}(x_i)^\adjoint
        \theta_{\seq{s}}-y_i\right) + \lambda \theta_{\seq{s}} = 0.
    \end{dmath*}
    Therefore,
    \begin{dmath}
        \label{eq:iff_solution} \left(\frac{1}{N}\sum_{i=1}^N
        \tildePhi{\omega}(x_i) \tildePhi{\omega}(x_i)^\adjoint +
        \lambda I_{\tildeH{\omega}}\right) \theta_{\seq{s}}
        = \frac{1}{N}\sum_{i=1}^N \tildePhi{\omega}(x_i) y_i.
    \end{dmath}
    Suppose that $\mathcal{Y}\subseteq\mathbb{R}^p$, and for all
    $x\in\mathcal{X}$, $\tildePhi{\omega}(x): \mathbb{R}^{r}\to\mathbb{R}^p$
    where all spaces are endowed with the euclidean inner product. From this we
    can derive \cref{alg:close_form} which returns the closed form solution of
    \cref{eq:cost_functional} for $c(y,y')=\frac{1}{2}\norm{y-y'}_2^2$.
\end{example}
\subsection{Complexity analysis}
\label{subsec:complexity}
\Cref{alg:close_form} constitutes our first step toward large-scale learning
with \aclp{OVK}. We can easily compute the time complexity of
\cref{alg:close_form} when all the operators act on finite dimensional Hilbert
spaces. Suppose that $p=\dim(\mathcal{Y})<\infty$ and for all $x\in\mathcal{X}$,
$\tildePhi{\omega}(x):\mathcal{Y}\to\tildeH{\omega}$ where
$r=\dim(\tildeH{\omega})<\infty$ is the dimension of the redescription space
$\tildeH{\omega}=\mathbb{R}^{r}$. Since $p$ and $r<\infty$, we view the
operators $\tildePhi{\omega}(x)$ and $I_{\tildeH{\omega}}$ as matrices.  Step 1
costs $O_t(Nr^2p)$. Steps 2 costs $O_t(Nrp)$. For step 3, the naive inversion
of the operator costs $O_t(r^3)$. Eventually the overall complexity of
\cref{alg:close_form} is
\begin{dmath*}
    O_t\left(r^2(Np + r)\right),
\end{dmath*}
while the space complexity is $O_s(r^2)$.
\afterpage{%
\begin{center}
    \begin{algorithm2e}[H]
        \label{alg:close_form}
        \SetAlgoLined
        \Input{\begin{itemize}
            \item $\seq{s}=(x_i,
            y_i)_{i=1}^N\in\left(\mathcal{X}\times\mathbb{R}^p\right)^N$ a
            sequence of supervised training points,
            \item $\tildePhi{\omega}(x_i) \in \mathcal{L}\left(\mathbb{R}^p,
            \mathbb{R}^{r}\right)$ a feature map defined for all
            $x_i\in\mathcal{X}$,
            \item $\lambda \in\mathbb{R}_{>0}$ the
            Tychonov regularization term,
        \end{itemize}}
        \Output{A model %
        \begin{dmath*} %
            h:\begin{cases} \mathcal{X} \to \mathbb{R}^p \\
            x\mapsto\tildePhi{\omega}(x)^\transpose
            \theta_{\seq{s}},\end{cases} %
        \end{dmath*} %
        such that $\theta_{\seq{s}}$ minimize \cref{eq:cost_functional}, where
        $c(y,y')=\norm{y-y'}_2^2$ and $\mathbb{R}^r$ and
        $\mathbb{R}^p$ are Hilbert spaces endowed with the euclidean inner
        product.}
        $\mathbf{P} \gets \frac{1}{N}\sum_{i=1}^N
        \tildePhi{\omega}(x_i) \tildePhi{\omega}(x_i)^\transpose
        \in\mathcal{L}(\mathbb{R}^{r}, \mathbb{R}^{r})  $\;
        $\mathbf{Y} \gets \frac{1}{N}\sum_{i=1}^N
        \tildePhi{\omega}(x_i)  y_i \in \mathbb{R}^{r} $\;
        $\theta_{\seq{s}} \gets \text{solve}_{\theta}\left((\mathbf{P} +
        \lambda I_r)\theta = \mathbf{Y} \right)$ \;
        \Return $h: x \mapsto \tildePhi{\omega}(x)^\transpose
        \theta_{\seq{s}}$\;
        \caption{Naive closed form for the squared error cost.}
    \end{algorithm2e}
\end{center}}
This complexity is to compare with the kernelized solution. Let
\begin{dmath*}
    \mathbf{K}:
    \begin{cases}
        \mathcal{Y}^{N} \to \mathcal{Y}^{N} \\
        u\mapsto\Vect_{i=1}^{N+U}\sum_{j=1}^{N+U}K(x_i, x_j)u_j
    \end{cases}
\end{dmath*}
When $\mathcal{Y}=\mathbb{R}$,
\begin{dmath*}
    \mathbf{K}=
    \begin{pmatrix} K(x_1, x_1) & \hdots & K(x_1, x_{N+U}) \\ \vdots
        & \ddots & \vdots \\  K(x_{N+U}, x_1) & \hdots & K(x_{N+U}, x_{N+U})
    \end{pmatrix}
\end{dmath*}
is called the Gram matrix of $K$. When $\mathcal{Y}=\mathbb{R}^p$, $\mathbf{K}$
is a matrix-valued Gram matrix of size $pN\times pN$ where each entry
$\mathbf{K}_{ij}\in\mathcal{M}_{p,p}(\mathbb{R})$. Then the equivalent
kernelized solution $u_{\seq{s}}$ of \cref{th:representer} is
\begin{dmath*}
    \left(\frac{1}{N} \mathbf{K}  + \lambda
    I_{\Vect_{i=1}^{N}\mathcal{Y}}\right)u_{\seq{s}}=\Vect_{i=1}^N y_i.
\end{dmath*}
which has time complexity $O\left(N^3p^3\right)$ and space complexity
$O_s\left(N^2p^2\right)$. Suppose we are given a generic \acs{ORFF} map (see
\cref{subsec:examples_ORFF}). Then $r=2Dp$, where $D$ is the number of samples.
Hence \cref{alg:close_form} is better that its kernelized counterpart when
$r=2Dp$ is small compared to $Np$. Thus, roughly speaking it is better to use
\cref{alg:close_form} when the number of features, $r$, required is small
compared to the number of training points. Notice that \cref{alg:close_form}
has a linear complexity with respect to the number of supervised training
points $N$ so it is better suited to large scale learning provided that $D$
does not grows linearly with $N$.
\paragraph{}
Yet naive learning with \cref{alg:close_form} by viewing all the operators as
matrices is still problematic. Indeed learning $p$ independent models with
scalar Random Fourier Features would cost $O_t\left(D^2p^3(N + D)\right)$ since
$r=2Dp$. This Means that learning vector-valued function has increased the
(expected) complexity from $p$ to $p^3$. However in some cases we can
drastically reduce the complexity by viewing the feature-maps as linear
operators rather than matrices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Efficient learning with ORFF}
\label{subsec:efficient_learning}
When developping \cref{alg:close_form} we considered that the feature map
$\tildePhi{\omega}(x)$ was a matrix from $\mathbb{R}^p$ to $\mathbb{R}^{r}$ for
all $x\in\mathcal{X}$, and therefore that computing
$\tildePhi{\omega}(x)\tildePhi{\omega}(z)^\transpose$ has a time complexity of
$O(r^2p)$.  While this holds true in the most generic senario, in many cases
the feature maps present some structure or sparsity allowing to reduce the
computational cost of evaluating the feature map. We focus on the \acl{ORFF}
given by \cref{alg:ORFF_construction}, developped in \cref{sec:building_ORFF}
and \cref{subsec:examples_ORFF} and treat the decomposable kernel, the
curl-free kernel and the divergence-free kernel as an example. We recall that
if $\mathcal{Y}'=\mathbb{R}^{p'}$ and $\mathcal{Y}=\mathbb{R}^p$, then
$\tildeH{\omega}=\mathbb{R}^{2Dp'}$ thus the \acl{ORFF}s given in
\cref{ch:operator-valued_random_fourier_features} have the form
\begin{dmath*}
    \begin{cases}
        \tildePhi{\omega}(x) \in\mathcal{L}\left(\mathbb{R}^p,
        \mathbb{R}^{2Dp'}\right) &: y \mapsto
        \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
        \omega_j}B(\omega_j)^\transpose  y \\ \tildePhi{\omega}(x)^\transpose
        \in\mathcal{L}\left(\mathbb{R}^{2Dp'}, \mathbb{R}^p\right) &: \theta
        \mapsto \frac{1}{\sqrt{D}} \sum_{j=1}^D \pairing{x,
        \omega_j}B(\omega_j)\theta_j
    \end{cases},
\end{dmath*}
where $\omega_j\sim\probability_{\dual{\Haar}, \rho}$ \ac{iid}~and
$B(\omega_j)\in\mathcal{L}\left(\mathbb{R}^p,\mathbb{R}^{p'}\right)$ for all
$\omega_j\in\dual{\mathcal{X}}$. Hence the \acl{ORFF} can be seen as the block
matrix
\begin{dmath}
    \label{eq:matrix_orff}
    \tildePhi{\omega}(x) =
    \begin{pmatrix}
        \cos\inner{x,\omega_1}B(\omega_1)^\transpose  \\
        \sin\inner{x,\omega_1}B(\omega_1)^\transpose  \\
        \vdots \\
        \cos\inner{x,\omega_D}B(\omega_D)^\transpose  \\
        \sin\inner{x,\omega_D}B(\omega_D)^\transpose
    \end{pmatrix}
    \hiderel{\in}\mathcal{M}_{2Dp',p}\left(\mathbb{R}\right),
\end{dmath}
$\omega_j\sim\probability_{\dual{\Haar}, \rho}$ \ac{iid}.

\subsection{Case of study: the decomposable kernel}
\label{subsec:fast_decomposable}
Throughout this section we show how the mathematical formulation relates to a
concrete (Python) implementation. We propose a Python implementation based on
NumPy~\citep{oliphant2006guide}, SciPy~\citep{jones2014scipy} and
Scikit-learn~\citep{pedregosa2011scikit}. Following \cref{eq:matrix_orff}, the
feature map associated to the decomposable kernel would be
\begin{pycode}[efficient_linop][fontsize=\scriptsize]
r"""Example of efficient implementation of Gaussian decomposable ORFF."""

from time import time

from numpy.linalg import svd
from numpy.random import rand, seed
from numpy import (dot, diag, sqrt, kron, zeros,
                   logspace, log10, matrix, eye, int)
from scipy.sparse.linalg import LinearOperator
from sklearn.kernel_approximation import RBFSampler
from matplotlib.pyplot import savefig, subplots
\end{pycode}

\begin{dmath*}
    \label{eq:matrix_decomposable_orff}
    \tildePhi{\omega}(x) =
    \frac{1}{\sqrt{D}}
    \begin{pmatrix}
        \cos\inner{x,\omega_1} B^\transpose  \\
        \sin\inner{x,\omega_1}B^\transpose  \\ \vdots \\
        \cos\inner{x,\omega_D}B^\transpose  \\
        \sin\inner{x,\omega_D}B^\transpose
    \end{pmatrix}
    \hiderel{=} \underbrace{\frac{1}{\sqrt{D}}
    \begin{pmatrix}
        \cos\inner{x,\omega_1} \\ \sin\inner{x,\omega_1} \\ \vdots \\
        \cos\inner{x,\omega_D} \\ \sin\inner{x,\omega_D}
    \end{pmatrix}}_{\tildephi{\omega}(x)}\otimes B^\transpose ,
    % \hiderel{\in}\mathcal{M}_{2Du'u}\left(\mathbb{R}\right),
\end{dmath*}
$\omega_j\sim\probability_{\dual{\Haar}, \rho}$ \ac{iid}, which would lead to
the following naive python implementation for the Gaussian (RBF) kernel of
parameter $\gamma$, whose associated spectral distribution is
$\probability_{\rho}=\mathcal{N}(0, 2\gamma)$.  Let
$\theta\in\mathbb{R}^{2Dp'}$ and $y\in\mathbb{R^p}$. With such implementation
evaluating a matrix vector product such as $\tildePhi{\omega}(x)^\transpose
\theta$ or $\tildePhi{\omega}(x)y$ have $O_t(2Dp'p)$ time complexity and
$O_s(2Dp'p)$ of space complexity, which is utterly inefficient. Indeed, recall
that if $B\in\mathcal{M}_{p,p'}\left(\mathbb{R}^{p'}\right)$ is matrix, the
operator $\tildePhi{\omega}(x)$ corresponding to the decomposable kernel is
\begin{dmath*}
    \tildePhi{\omega}(x)y =
    \frac{1}{\sqrt{D}}\Vect_{j=1}^D\begin{pmatrix}\cos\inner{x, \omega_j}
    B^\transpose y \\ \sin\inner{x, \omega_j} B^\transpose y \end{pmatrix} =
    \left(\frac{1}{\sqrt{D}}\Vect_{j=1}^D\begin{pmatrix}\cos\inner{x, \omega_j}
    \\ \sin\inner{x, \omega_j} \end{pmatrix}\right)\otimes (B^\transpose y)
\end{dmath*}
and
\begin{dmath}
    \label{eq:phi_transpose_efficient} \tildePhi{\omega}(x)^\transpose \theta =
    \frac{1}{\sqrt{D}} \sum_{j=1}^D \cos\inner{x, \omega_j}B\theta_j +
    \sin\inner{x, \omega_j}B\theta_j = B\left(\frac{1}{\sqrt{D}} \sum_{j=1}^D
    \left(\cos\inner{x, \omega_j} + \sin\inner{x,
    \omega_j}\right)\theta_j\right).
\end{dmath}
Which requires only evaluation of $B$ on $y$ and can be implemented easily
in Python thanks to SciPy's LinearOperator. Note that the computation of these
expressions can be fully vectorized\footnote{See~\citet{walt2011numpy}.} using
the vectorization property of the Kronecker product. In the following we
consider $\Theta \in \mathcal{M}_{2D,u'}(\mathbb{R})$ and the operator
$\vectorize: \mathcal{M}_{p',2D}(\mathbb{R}) \to \mathbb{R}^{2Dp'}$ which turns
a matrix into a vector (\acs{ie}~$\theta_{p'i+j} = \vectorize(\Theta_{ij})$,
$i\in\mathbb{N}_{2D}$ and $j\in\mathbb{N}^*_{p'}$). Then
\begin{dmath*}
    \left(\tildephi{\omega}(x) \otimes B^\transpose \right)^\transpose  \theta
    \hiderel{=} \left(\tildephi{\omega}(x)^\transpose  \otimes B\right)
    \vectorize(\Theta) \hiderel{=} \vectorize\left(B \Theta
    \tildephi{\omega}(x) \right).
\end{dmath*}
with this trick, many authors \citep{Sindhwani2013, brault2016scaling,
rosasco2010learning}
notice that the decomposable kernel usually yields a Stein equation
\citep{penzl1998numerical}.  Indeed rewriting step 3 of \cref{alg:close_form}
gives a system to solve of the form
\begin{dmath*}
    \tildephi{\omega}(X)\tildephi{\omega}(X)^\transpose \Theta B^\transpose B +
    \lambda \Theta - Y \hiderel{=} 0.
    \Leftrightarrow
    \left(\tildephi{\omega}(X) \tildephi{\omega}(X)^\transpose
    \hiderel{\otimes} B^\transpose B \hiderel{+} \lambda I_{2Dp'}\right) \theta
    \hiderel{-} Y \hiderel{=} 0
\end{dmath*}
Many solvers exists to solve efficiently this kind of systems\footnote{For
instance \citet{sleijpen2010bi}.}, but most of them share the particularity
that they are not just restricted to handle Stein equations. Broadly speaking,
iterative solvers (or matrix free solvers) are designed to solve any systems of
equation of ther form $PX=C$, where $P$ is a linear operator (not a matrix).
This is exacly our case where $\tildephi{\omega}(x)\otimes B^T$ is the matrix
form of the operator $\Theta \mapsto \vectorize(B\Theta\tildephi{\omega}X)$.
\paragraph{}
This leads us to the following (more efficient) Python implementation of the
Decomposable \acs{ORFF} \say{operator} to be feed to a matrix-free solvers.
\begin{pyblock}[efficient_linop][fontsize=\scriptsize]
def EfficientDecomposableGaussianORFF(X, A, gamma=1.,
                                      D=100, eps=1e-5, random_state=0):
    r"""Return the efficient ORFF map associated with the data X.

    Parameters
    ----------
    X : {array-like}, shape = [n_samples, n_features]
        Samples.
    A : {array-like}, shape = [n_targets, n_targets]
        Operator of the Decomposable kernel (positive semi-definite)
    gamma : {float},
        Gamma parameter of the RBF kernel.
    D : {integer}
        Number of random features.
    eps : {float}
        Cutoff threshold for the singular values of A.
    random_state : {integer}
        Seed of the generator.

    Returns
    -------
    \tilde{\Phi}(X) : Linear Operator, callable
    """
    # Decompose A=BB^\transpose
    u, s, v = svd(A, full_matrices=False, compute_uv=True)
    B = dot(diag(sqrt(s[s > eps])), v[s > eps, :])

    # Sample a RFF from the scalar Gaussian kernel
    phi_s = RBFSampler(gamma=gamma, n_components=D, random_state=random_state)
    phiX = phi_s.fit_transform(X)

    # Create the ORFF linear operator
    cshape = (D, B.shape[0])
    rshape = (X.shape[0], B.shape[1])
    return LinearOperator((phiX.shape[0] * B.shape[1], D * B.shape[0]),
                          matvec=lambda b: dot(phiX, dot(b.reshape(cshape),
                                               B)),
                          rmatvec=lambda r: dot(phiX.T, dot(r.reshape(rshape),
                                                B.T)))
\end{pyblock}
\subsection{Linear operators in matrix form}
\label{subsec:efficient_linop}
For convenience we give the operators corresponding to the decomposable,
curl-free and divergence-free kernels in matrix form. Let $(x_i)_{i=1}^N$,
$N\in\mathbb{N}^*$, $x_i$'s in $\mathbb{R}^d$, $d\le\infty$ be a sequence of
points in $\mathbb{R}^d$. We note
\begin{dmath*}
    X=\begin{pmatrix}x_1 & \hdots &
    x_N\end{pmatrix}\hiderel{\in}\mathcal{M}_{d,N}
\end{dmath*}
the data matrix where each column represents a data point\footnote{In many
programming language, such as Python, C, C\verb!++! or Java each data point is
traditionally represented by a row in the data matrix (row major formulation).
While this is more natural when parsing a data file, it is less common in
mathematical formulations. In this document we adopt the \emph{column major}
formulation used by Matlab, Fortran or Julia. Moreover although C\verb!++! is
commonly row major, some libraries such as Eigen are column major. When dealing
with row major formulation, one should \say{transpose} all the equations given
in \cref{table:efficient-op}.}. Naturally if
$\tildePhi{\omega}(x):\mathbb{R}^u\to\mathbb{R}^{r_1}$ and
$\tildephi{\omega}(x):\mathbb{R}\to\mathbb{R}^{r_2}$, for all
$x\in\mathbb{R}^d$ we define
\begin{dmath*}
    \tildePhi{\omega}(X)=\begin{pmatrix}\tildePhi{\omega}(x_1) & \hdots &
    \tildePhi{\omega}(x_N)\end{pmatrix}\hiderel{\in}\mathcal{M}_{r_1,Nu}
\end{dmath*}
and
\begin{dmath*}
    \tildephi{\omega}(X)=\begin{pmatrix}\tildephi{\omega}(x_1) & \hdots &
    \tildephi{\omega}(x_N)\end{pmatrix}\hiderel{\in}\mathcal{M}_{r_2, N}
\end{dmath*}
and
\begin{dmath*}
    U=\begin{pmatrix} u_1 & \hdots&  u_N
    \end{pmatrix}\hiderel{\in}\mathcal{M}_{u, N}.
\end{dmath*}
Given a matrix $X\in\mathcal{M}_{m,n}(\mathbb{R})$, we note $X_{\bullet i}$ the
\emph{column} vector corresponding to the $i$-th column of the matrix $X$ and
$X_{i \bullet}$ the \emph{row} vector (covector) corresponding to the $i$-th
line of the matrix $X$. With these notations, if $X\in\mathcal{M}_{m,n}$ and
$Z\in\mathcal{M}_{n,m'}$, $X_{i\bullet}Z_{\bullet j}\in\mathbb{R}$ is the inner
product between the $i$-th row of $X$ and the $j$-th column of $Z$ and
$X_{\bullet i} Z_{j \bullet}\in\mathcal{M}_{m,m'}(\mathbb{R})$ is the outer
product between the $i$-th column of $X$ and $j$-th row of $X$.
\paragraph{}
For the curl-free and divergence-free kernel given in
\cref{subsec:examples_ORFF} we recall the unbounded \acs{ORFF} maps are
respectively for all $y\in\mathcal{Y}$
\begin{dmath*}
    \tildePhi{\omega}(x) y =\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos{\inner{x,\omega_j}_2}\omega_j^\transpose y \\
        \sin{\inner{x,\omega_j}_2}\omega_j^\transpose y
    \end{pmatrix},
\end{dmath*}
and
\begin{dmath*}
    \tildePhi{\omega}(x) y = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos{\inner{x,\omega_j}_2}\left(\norm{\omega_j }_2I_d -
        \frac{\omega_j\omega_j^\transpose }{\norm{\omega_j}_2}\right) y\\
        \sin{\inner{x,\omega_j}_2}\left(\norm{\omega_j}_2I_d-
        \frac{\omega_j\omega_j^\transpose }{\norm{\omega_j}_2}\right) y
    \end{pmatrix},
\end{dmath*}
where $\omega_j\sim \probability_{\mathcal{N}(0,\sigma^{-2}I_d)}$. To avoid
complex index notations we decompose the feature maps $\tildePhi{\omega}(X)$
into two sub feature maps $\tildePhi{\omega}^c$ and $\tildePhi{\omega}^s$
corresponding to the cosine part and the sine part of each feature map. Namely,
for the curl-free kernel, for all $y\in\mathcal{Y}$
\begin{dmath*}
    \tildePhi{\omega}(x) y =
    \begin{cases}
        \tildePhi{\omega}^c(x) y = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}\omega_j^\transpose y
        \end{pmatrix}, \\
        \tildePhi{\omega}^s(x) y = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \sin{\inner{x,\omega_j}_2}\omega_j^\transpose  y
        \end{pmatrix}.
    \end{cases}
\end{dmath*}
In the same way, for the divergence-free kernel,
\begin{dmath*}
    \tildePhi{\omega}(x) y =
    \begin{cases}
        \tildePhi{\omega}^c(x) y  = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}\left(\norm{\omega_j}_2 I_d -
            \frac{\omega_j\omega_j^\transpose}{\norm{\omega_j}_2} \right) y
        \end{pmatrix}, \\
        \tildePhi{\omega}^s(x) y  = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \sin{\inner{x,\omega_j}_2} \left(\norm{\omega_j}_2 I_d -
            \frac{\omega_j\omega_j^\transpose}{\norm{\omega_j}_2} \right) y
        \end{pmatrix}.
    \end{cases}
\end{dmath*}
We also introduce $\tildePhi{\omega}^e$, $e\in\Set{s,c}$ which denotes either
$\tildePhi{\omega}^s$ or $\tildePhi{\omega}^c$. This equivalent formulation
allows us to keep the notation \say{lighter} and closer to a proper
Python/Matlab implementation with vectorization. With these notations, a
summary of efficient linear operators in matrix form is given in
\cref{table:efficient-op}. The complexity of evaluating all this operators is
given in \cref{table:efficient-complexity}.
\afterpage{%
\begin{landscape}
    \begin{table}[htb]{}
        \centering
        \begin{threeparttable}
            \caption[Efficient linear-operators for different
            \acs{ORFF}.]{Efficient linear-operator (in matrix form) for
            different Feature maps. \label{table:efficient-op}}
            \begin{tabularx}{\textheight}{Xcc}
                \toprule
                    Kernel & $\tildePhi{\omega}(X)^\adjoint$ &
                    $\tildePhi{\omega}(X)$ \\
                \midrule
                    Decomposable\tnote{1} &$\Theta\mapsto B\left(\Theta
                    \tildephi{\omega}(X)\right)$ & $Y\mapsto B^\transpose
                    \left(Y\tildephi{\omega}(X)^\transpose \right)$ \\ Gaussian
                    curl-free\tnote{2} & $\Theta^c, \Theta^s\mapsto
                    \displaystyle\sum_{j=1}^D \omega_j \left(\Theta_{j}^c
                    \tildephi{\omega}^{c}(X)_{j\bullet} +
                    \Theta_{j}^s\tildephi{\omega}^{s}(X)_{j\bullet}\right)$ &
                    $Y\mapsto \Theta_j^e=\omega_j^\transpose
                    \left(Y\tildephi{\omega}^{e}(X)_{\bullet j}^\transpose
                    \right)$ \\ Gaussian divergence-free\tnote{2,3} &
                    $\Theta^c, \Theta^s \mapsto \displaystyle\sum_{j=1}^D
                    \left(B(\omega_j) \Theta^{c}_{\bullet j}\right)
                    \tildephi{\omega}^{c}(X)_{j\bullet} +
                    \left(B(\omega_j)\Theta^{s}_{\bullet
                    j}\right)\tildephi{\omega}^{s}(X)_{j\bullet}$ & $Y \mapsto
                    \Theta^e_{\bullet
                    j}=B(\omega_j)\left(Y\tildephi{\omega}^{e}(X)_{\bullet
                    j}^\transpose \right)$ \\
                \bottomrule
            \end{tabularx}
            \begin{tablenotes}
                \item[1] Where $\tildephi{\omega}(X)=\begin{pmatrix}
                \tildephi{\omega}(X_{\bullet 1}) & \hdots &
                \tildephi{\omega}(X_{\bullet N})
                \end{pmatrix}\in\mathcal{M}_{r, N}$ is any design matrix, with
                scalar feature map
                $\tildephi{\omega}:\mathbb{R}^d\to\mathbb{R}^r$ such that
                $\tildephi{\omega}(x)^\adjoint
                \tildephi{\omega}(z)=k(x,z)\in\mathbb{R}$ for all $x$,
                $z\in\mathcal{X}$. The input data
                $X\in\mathcal{M}_{d,N}(\mathbb{R})$, the output data
                $U\in\mathcal{M}_{p,N}(\mathbb{R})$, the parameter matrices
                $\Theta^c$ and $\Theta^s\in\mathcal{M}_{p', r}(\mathbb{R})$ and
                the decomposable operator $B\in\mathcal{M}_{p,p'}(\mathbb{R})$.
                \item[2] Where
                $\tildephi{\omega}^{c}(X)_{ji}=\cos\inner{\omega_j, x_i}$ and
                $\tildephi{\omega}^{s}(X)_{ji}=\sin\inner{\omega_j, x_i}$,
                $j\in\mathbb{N}^*_D$ and $i\in\mathbb{N}^*_N$. Thus
                $\tildephi{\omega}^{c}(X)\in\mathcal{M}_{D,N}(\mathbb{R})$ and
                $\tildephi{\omega}^{s}(X)\in\mathcal{M}_{D,N}(\mathbb{R})$. The
                input data $X\in\mathcal{M}_{d,N}(\mathbb{R})$, the output data
                $U\in\mathcal{M}_{d,N}(\mathbb{R})$, the parameter matrices
                $\Theta^c$ and $\Theta^s\in\mathbb{R}^D$, $\omega_j\sim
                \probability_{\mathcal{N}(0,\sigma^{-2}I_d)}$ \ac{iid}~for all
                $j\in\mathbb{N}^*_D$. Eventually $e\in\Set{s,c}$, namely
                $\Theta^c=\begin{pmatrix} \Theta^{e=c}_1 & \hdots &
                \Theta^{e=c}_D \end{pmatrix}^\transpose $ and
                $\Theta^s=\begin{pmatrix} \Theta^{e=s}_1 & \hdots &
                \Theta^{e=s}_D\end{pmatrix}^\transpose $.
                \item[3] Here,
                $\Theta^c$ and $\Theta^s\in\mathcal{M}_{d,D}(\mathbb{R})$ thus
                $\Theta^c=\begin{pmatrix}\Theta^{e=c}_{\bullet 1} & \hdots &
                \Theta^{e=c}_{\bullet D}\end{pmatrix}$,
                $\Theta^s=\begin{pmatrix}\Theta^{e=s}_{\bullet 1} & \hdots &
                \Theta^{e=s}_{\bullet D}\end{pmatrix}$ and
                $B(\omega)=\left(\norm{\omega}_2I_d-\frac{\omega\omega^\transpose
                }{\norm{\omega}_2}\right)\in\mathcal{M}_{d,d}$.
                \end{tablenotes}
        \end{threeparttable}
    \end{table}
\end{landscape}}
\paragraph{}
It is worth mentioning that the same strategy can be applied in many different
language. For instance in C{}\verb!++!, the library Eigen~\citep{eigenweb}
allows to wrap a sparse matrix with a custom type, where the user overloads the
transpose and dot product operator (as in Python). Then the custom user
operator behaves as a (sparse) matrix --see
\url{https://eigen.tuxfamily.org/dox/group__MatrixfreeSolverExample.html}. With
this implementation the time complexity of $\tildePhi{\omega}(x)^\transpose
\theta$ and $\tildePhi{\omega}(x)y$ falls down to $O_t(2Dp'+p'[)$ and the same
holds for space complexity.
\begin{table}[th]
    \centering
    \caption[Complexity of efficient linear-operators for different
    \acs{ORFF}.]{Complexity of efficient linear-operator (in matrix form) for
    different Feature maps given in \cref{table:efficient-op}.
    \label{table:efficient-complexity}}
    \begin{tabularx}{\textwidth}{Xcc}
        \toprule
            Kernel & $\tildePhi{\omega}(X)^\adjoint$ & $\tildePhi{\omega}(X)$
            \\
        \midrule
            Decomposable & $O\left((p'D+p'p)N\right)$ &
            $O\left((pN+p'p)D\right)$ \\
            Curl-free & $O\left(pND\right)$ & $O\left(pND\right)$ \\
            Divergence-free & $O\left((p^2+pN)D\right)$ &
            $O\left((p^2+pN)D\right)$ \\
        \bottomrule
    \end{tabularx}
\end{table}
\paragraph{}
A quick experiment shows the advantage of seeing the decomposable kernel as a
linear operator rather than a matrix. We draw $N=100$ points $(x_i)_{i=1}^N$ in
the interval $(0,1)^{20}$ and use a decomposable kernel with matrix
$\Gamma=BB^\transpose \in\mathcal{M}_{p,p}(\mathbb{R})$ where
$B\in\mathcal{M}_{p,p}(\mathbb{R})$ is a random matrix with coefficients drawn
uniformly in $(0,1)$. We compute $\tildePhi{\omega}(x)^\transpose \theta$ for
all $x_i$'s, where $\theta\in\mathcal{M}_{2D,1}(\mathbb{R})$, $D=100$, with the
implementation \texttt{Ef\-fi\-cient\-De\-com\-po\-sa\-ble\-Gaus\-sian\-ORFF},
\cref{eq:phi_transpose_efficient}, and
\texttt{Na\-ive\-De\-com\-po\-sa\-ble\-Gaus\-sian\-ORFF},
\cref{eq:matrix_decomposable_orff}. The coefficients of $\theta$ were drawn at
random uniformly in $(0,1)$. We report the execution time in
\cref{fig:efficient_decomposable_gaussian} for different values of $p$, $1\le
p\le100$.
\begin{pycode}[efficient_linop]
sys.path.append('./src/')
import efficient_decomposable_gaussian

efficient_decomposable_gaussian.main()
\end{pycode}
\begin{figure}[htb]
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./efficient_decomposable_gaussian.pgf}}')}
    \caption[Efficient decomposable gaussian \acs{ORFF}]{Efficient decomposable
    gaussian ORFF (lower is better).}
    \label{fig:efficient_decomposable_gaussian}
\end{figure}
The left plot reports the execution time in seconds of the construction of the
feature. The middle plot reports the execution time of
$\tildePhi{\omega}(x)^\transpose \theta$, and the right plot the memory used in
bytes  to store $\tildePhi{\omega}(x)$ for all $x_i$'s. We averaged the results
over ten runs. Full code is given in
\cref{code:efficient_decomposable_gaussian}.

\subsection{Curl-free kernel}
We use the unbounded \acs{ORFF} map presented in
\cref{eq:unbounded_curl_free_orff}. We draw $N=1000$ points $(x_i)_{i=1}^N$ in
the interval $(0,1)^{p}$ and use a curl-free kernel. We compute
$\tildePhi{\omega}(x)^\transpose \theta$ for all $x_i$'s, where
$\theta\in\mathcal{M}_{2D,1}(\mathbb{R})$, $D=500$, with the matrix
implementation and the \texttt{LinearOperator} implementation. The coefficients
of $\theta$ were drawn at random uniformly in $(0,1)$. We report the execution
time in \cref{fig:efficient_curlfree_gaussian} for different values of $p$,
$1\le p\le100$.
\begin{pycode}[efficient_linop]
sys.path.append('./src/')
import efficient_curlfree_gaussian

efficient_curlfree_gaussian.main()
\end{pycode}
\begin{figure}[h]
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./efficient_curlfree_gaussian.pgf}}')}
    \caption[Efficient curl-free gaussian \acs{ORFF}]{Efficient curl-free
    gaussian ORFF (lower is better).}
    \label{fig:efficient_curlfree_gaussian}
\end{figure}
The left plot reports the execution time in seconds of the construction of the
features. The middle plot reports the execution time of
$\tildePhi{\omega}(x)^\transpose \theta$, and the right plot the memory used in
bytes  to store $\tildePhi{\omega}(x)$ for all $x_i$'s. We averaged the results
over fifty runs. Full code is given in \cref{code:efficient_curlfree_gaussian}.
As we can see the linear-operator implementation is one order of magnitude
slower than its matrix counterpart. However it uses considerably less memory.

\subsection{Divergence-free kernel}
We use the unbounded \acs{ORFF} map presented in
\cref{eq:unbounded_div_free_orff}. We draw $N=100$ points $(x_i)_{i=1}^N$ in
the interval $(0,1)^{p}$ and use a curl-free kernel. We compute
$\tildePhi{\omega}(x)^\transpose \theta$ for all $x_i$'s, where
$\theta\in\mathcal{M}_{2Dp,1}(\mathbb{R})$, $D=100$, with the matrix
implementation and the \texttt{LinearOperator} implementation. The coefficients
of $\theta$ were drawn at random uniformly in $(0,1)$. We report the execution
time in \cref{fig:efficient_curlfree_gaussian} for different values of $p$,
$1\le p\le100$.
\begin{pycode}[efficient_linop]
sys.path.append('./src/')
import efficient_divfree_gaussian

efficient_divfree_gaussian.main()
\end{pycode}
\begin{figure}[h]
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./efficient_divfree_gaussian.pgf}}')}
    \caption[Efficient divergence-free gaussian \acs{ORFF}]{Efficient
    divergence-free gaussian ORFF (lower is better).}
    \label{fig:efficient_divfree_gaussian}
\end{figure}
The left plot reports the execution time in seconds of the construction of the
feature. The middle plot reports the execution time of
$\tildePhi{\omega}(x)^\transpose \theta$, and the right plot the memory used in
bytes  to store $\tildePhi{\omega}(x)$ for all $x_i$'s. We averaged the results
over ten runs. Full code is given in \cref{code:efficient_divfree_gaussian}. We
draw the same conclusions as the curl-free kernel.

\subsection{Iterative solvers using linear operators}
We have shown in this section that viewing the \acl{ORFF} maps as linear
operator rather than staking matrices leads to more efficient computations.


% Acknowledgements should go at the end, before appendices and references
\acks{Maxime Sangnier, Markus Heinonen.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}\label{app:theorem}

\bibliography{jmlr-orff}

\end{document}

