\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf
\usepackage[abbrvbib]{jmlr2e}
\usepackage{pythontex}
\usepackage{acro}
\usepackage{ragged2e}
\usepackage{cmap} % copy-paste pdf
\usepackage{microtype}
\usepackage{dirtytalk}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{fancyvrb}

\VerbatimFootnotes

\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{cd}
\usetikzlibrary{matrix}
\usetikzlibrary{calc}
\makeatletter
\tikzset{%
  column sep/.code=\def\pgfmatrixcolumnsep{\pgf@matrix@xscale*(#1)},
  row sep/.code   =\def\pgfmatrixrowsep{\pgf@matrix@yscale*(#1)},
  matrix xscale/.code=%
    \pgfmathsetmacro\pgf@matrix@xscale{\pgf@matrix@xscale*(#1)},
  matrix yscale/.code=%
    \pgfmathsetmacro\pgf@matrix@yscale{\pgf@matrix@yscale*(#1)},
  matrix scale/.style={/tikz/matrix xscale={#1},/tikz/matrix yscale={#1}}}
\def\pgf@matrix@xscale{1}
\def\pgf@matrix@yscale{1}
\makeatother

\usepackage[american]{babel}

\usepackage{braket}
\usepackage{colonequals}
\usepackage{thmtools}
\usepackage{nameref, cleveref}
\usepackage{mathtools}
\usepackage[algo2e,ruled,linesnumbered]{algorithm2e}
\usepackage{breqn}
\usepackage{commands}

\makeatletter
\let\cref@old@eq@setnumberOld\eq@setnumber
\def\eq@setnumber{%
\cref@old@eq@setnumberOld%
\cref@constructprefix{equation}{\cref@result}%
\protected@xdef\cref@currentlabel{%
[equation][\arabic{equation}][\cref@result]\p@equation\eq@number}}
\makeatother

\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{ltablex}
\usepackage{threeparttablex}
\setlength{\extrarowheight}{3pt} % Increase table row height
\newcommand{\tableheadline}[1]{\multicolumn{1}{c}{\spacedlowsmallcaps{#1}}}
\newcommand{\myfloatalign}{\centering} % To be used with each float for alignment
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{caption}

\crefname{algocfline}{Algorithm}{Algorithms}
\Crefname{algocfline}{Algorithm}{Algorithms}
\crefname{algocf}{Algorithm}{Algorithm}
\Crefname{algocf}{Algorithm}{Algorithms}
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{chapter}{Chapter}{Chapters}
\Crefname{chapter}{Chapter}{Chapters}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}
\crefname{subsection}{Subsection}{Subsections}
\Crefname{subsection}{Subsection}{Subsections}
\crefname{theorem}{Theorem}{Theorems}
\Crefname{theorem}{Theorem}{Theorems}
\crefname{proposition}{Proposition}{Propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{lemma}{Lemma}{Lemmas}
\Crefname{lemma}{Lemma}{Lemmas}
\crefname{corollary}{Corollary}{Corollaries}
\Crefname{corollary}{corollary}{Corollaries}
\crefname{equation}{Equation}{Equations}
\Crefname{equation}{Equation}{Equations}
\crefname{remark}{Remark}{Remarks}
\Crefname{remark}{Remark}{Remarks}
\crefname{example}{Example}{Examples}
\Crefname{example}{Example}{Examples}
\creflabelformat{equation}{#2\textup{#1}#3}

% Heading arguments are {volume}{year}{pages}{submitted}{published}
%     {author-full-names}

\jmlrheading{1}{2000}{1--48}{4/00}{10/00}{brault00a}
    {Romain Brault and Florence d'Alch\'e-Buc}

% Short headings should be running head and authors last names

\ShortHeadings{RFFs for OVKs}{Brault and d'Alch\'e-Buc}
\firstpageno{1}

\sloppy

\begin{document}

\title{Random Fourier Features for Operator-Valued Kernels}

\author{\name{}Brault Romain
       \email~romain.brault@telecom-paristech.fr \\
       \addr~LTCI\\
       T\'el\'ecom ParisTech\\
       Paris, 46 rue Barrault, France \\
       Universit\'e Paris-Saclay \\
       \AND%
       \name{}Florence d'Alch\'e-Buc
       \email~florence.dalche@telecom-paristech.fr \\
       \addr~LTCI\\
       T\'el\'ecom ParisTech\\
       Paris, 46 rue Barrault, France \\
       Universit\'e Paris-Saclay}

\editor{Francis Bach}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
    Many problems in Machine Learning can be cast into
    vector-valued functions approximation. Operator-Valued Kernels
    \emph{\acl{OVK}s} and vector-valued Reproducing Kernel Hilbert Spaces
    provide a theoretical and practical framework to address that issue,
    extending nicely the well-known setting of scalar-valued kernels.
    However large scale applications are usually not affordable with these
    tools that require an important computational power along with a large
    memory capacity. In this paper, we propose and study scalable methods
    to perform regression with \emph{\acl{OVK}s}. To achieve this goal, we
    extend Random Fourier Features, an approximation technique originally
    introduced for scalar-valued kernels, to \emph{\acl{OVK}s}. The idea is
    to take advantage of an approximated operator-valued feature map in
    order to come up with a linear model in a finite-dimensional space.
\end{abstract}

\begin{keywords}
    Random Fourier Feature, Operator-Valued Kernel
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
This paper is dedicated to the definition of a general and flexible approach to
learn vector-valued functions together with an efficient implementation of the
learning algorithms. To achieve this goal, we study shallow architectures,
namely the product of a (nonlinear) operator-valued feature
$\widetilde{\Phi}(x)$ and a parameter vector $\theta$ such that
$\widetilde{f}(x) = {\widetilde{\Phi}(x)}^* \theta$, and combine two appealing
methodologies: Operator-Valued Kernel Regression and Random Fourier Features.
\paragraph{}
Operator-Valued Kernels \citep{Micchelli2005,Carmeli2010,Kadri_aistat10,
Brouard2011,Alvarez2012} extend the classic scalar-valued kernels to functions
with values in some \emph{output} Hilbert space. As in the scalar case,
\acfp{OVK} are used to build Reproducing Kernel Hilbert Spaces (\acs{RKHS}) in
which representer theorems apply as for ridge regression or other appropriate
loss functional. In these cases, learning a model in the \acs{RKHS} boils down
to learning a function of the form $f(x)=\sum_{i=1}^N K(x,x_i)\alpha_i$ where
$x_1, \ldots, x_N$ are the training input data and each $\alpha_i, i=1, \ldots,
N$ is a vector of the output space $\mathcal{Y}$, and each $K(x,x_i)$ is an
operator on $\mathcal{Y}$.
\paragraph{}
However, \acsp{OVK} suffer from the same drawbacks as classic
(sca\-lar-va\-lued) kernel machines: they scale poorly to large datasets
because they are exceedingly demanding in terms of memory and computations. We
propose to approximate OVKs by extending a methodology called \acfp{RFF}
\citep{Rahimi2007, Le2013, Yang2015, sriper2015, Bach2015, sutherland2015,
rudi2016generalization} so far developed to speed up scalar-valued kernel
machines. The \acs{RFF} approach linearizes a shift-invariant kernel model by
generating explicitly an approximated feature map $\tilde{\phi}$. \acsp{RFF}
has been shown to be efficient on large datasets \citep{rudi2016generalization}
and has been further improved by efficient matrix computations such as
\citep[``FastFood'']{Le2013} and \citep[``SORF'']{felix2016orthogonal}, which
are considered as the best large scale implementations of kernel methods, along
with Nystr\"om approaches proposed in \citet{drineas2005nystrom}. Moreover
thanks to \acsp{RFF}, kernel methods have been proved to be competitive with
deep architectures \citep{lu2014scale, dai2014scalable, yang2015deep}.

\subsection{Outline and contributions}
The paper is structured as follow. In \cref{sec:background} we recall briefly
how to obtain \acp{RFF} for scalar-valued kernels and list the state of the
art implementation of \acp{RFF} for large scale kernel learning. Then we define
properly \aclp{OVK}, give some important theorems and properties used
throughout this paper before given a non exhaustive list of problem tackled
with \acsp{OVK}.
\paragraph{}
Then we move on to our contributions. In \cref{sec:ORFF_construction} we
propose an \acs{RFF} construction from $\mathcal{Y}$-Mercer shift invariant
\acs{OVK} that we call \acf{ORFF}. Then we study the structure of a random
feature corresponding to an \acs{OVK} (without having to specify the target
kernel). Eventually we use the framework used to construct \acsp{ORFF} to study 
the regularization properties of \acsp{OVK} in terms of \acl{FT}.
\paragraph{}
In \cref{sec:consistency_of_the_ORFF_estimator} we assess theoretically the
quality of our \acs{ORFF}: we show that the stochastic \acs{ORFF} estimator
converges with high probability toward the target kernel and derive convergence
rates. We also give a bound on the variance of the approximated \acs{OVK}
constructed from the corresponding \acs{ORFF}.
\paragraph{}
In \cref{sec:learning_with_operator-valued_random-fourier_features} we focus on
Ridge regression with \acsp{OVK}. First we study the relationship between
finding a minimizer in the \acs{vv-RKHS} induce by a given \acs{OVK} and the
feature induced by the corresponding \acs{ORFF}. Then we define a gradient
based algorithm to tackle Ridge regression with \acs{ORFF}, show how to
obtain an efficient implementation and study its complexity.
\paragraph{}
Eventually we end this paper by some numerical experiments in
\cref{sec:num_exp} on toy and real datasets before giving a general conlusion
in  \cref{sec:conclusion}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}
\label{sec:background}
%In this section we summarize briefly important notions used throughout this
%document. It is mainly based on books and lecture notes of
%\citet{kurdila2006convex,cotaescu2016elements}.

%\subsection{Notations}
%\label{sec:notations}
%We note $\mathbb{K}$ any Abelian field and call its elements scalars.
%$\mathbb{R}$ is the Abelian field of real numbers and $\mathbb{C}$ is the
%Abelian field of complex numbers. The unit pure imaginary number
%$\sqrt{-1}\in\mathbb{C}$ is denoted $\iu$ and the Euler constant
%$\exp(1)\in\mathbb{R}$ is denoted $\ec$.  $\mathbb{N}$ represents the set of
%natural numbers and $\mathbb{N}_n$, $n\in\mathbb{N}$ the set of natural numbers
%smaller or equal to $n$. For any space $\mathcal{S}$, $\mathcal{S}^d$,
%$d\in\mathbb{N}$ represents the Cartesian product space $\mathcal{S}^d =
%\mathcal{S}\times\cdots\times\mathcal{S}$. For any two algebraic structures
%$\mathcal{S}$ and $\mathcal{S}'$ we write $\mathcal{S}\cong\mathcal{S}'$ if
%there exist an isomorphism between these two structures. If $a+\iu b = x \in
%\mathbb{C}$ then $\conj{x}=a-\iu b\in\mathbb{C}$ denotes the complex conjugate.
%By extension if $x\in\mathbb{R}$, $\conj{x}=x\in\mathbb{R}$.

%\subsubsection{Topology and continuity}
%In order to define a proper notion of continuity, we focus on topological
%spaces. A topological space is a pair of sets $(\mathcal{X},\mathcal{T}_x)$
%where $\mathcal{X}$ describes the points considered, and $\mathcal{T}_x$
%describes the possible neighbourhoods. The standard axioms of topology suppose
%that $\mathcal{T}_x\subseteq{\mathcal{P}(\mathcal{X})}$ is a collection of
%subsets of $\mathcal{X}$ such that the empty set and $\mathcal{X}$ itself
%belongs to $\mathcal{T}_x$, any (finite or infinite) union of members of
%$\mathcal{T}_x$ still belongs to $\mathcal{T}_x$ and the intersection of any
%finite number of members of $\mathcal{T}_x$ still belongs to $\mathcal{T}_x$.
%The elements of $\mathcal{T}_x$ are called open sets and the collection
%$\mathcal{T}_x$ is a topology on $\mathcal{X}$. If
%$(\mathcal{X},\mathcal{T}_x)$ and $(\mathcal{Y},\mathcal{T}_y)$ are topological
%spaces, a function $f$ is said to be continuous if for every open set
%$\mathcal{V}\in \mathcal{T}_y$, the inverse image $f^{-1}(\mathcal{V}) = \Set{
%x \in \mathcal{X} | f ( x ) \in \mathcal{V} }$ is an open subset of
%$\mathcal{T}_x$. Since the notion of continuity depends on open sets, it
%depends on the topology of the spaces $\mathcal{X}$ and $\mathcal{Y}$.
%\paragraph{}
%If $(\mathcal{X},\mathcal{T}_{x})$ is a topological space and $x$ is a point in
%$\mathcal{X}$, a neighbourhood of $x$ is a subset $\mathcal{V}$ of
%$\mathcal{X}$ that includes an \emph{open} set $\mathcal{U}$ containing $x$. A
%topological space $\mathcal{X}$ is said to be Hausdorff (T2) when all distinct
%points in $\mathcal{X}$ are pairwise neighbourhood-separable. \acs{ie}~if there
%exists a neighbourhood $\mathcal{U}$ of $x$ and a neighbourhood $\mathcal{V}$
%of $y$ such that $\mathcal{U}$ and $\mathcal{V}$ are disjoint. It implies the
%uniqueness of limits of sequences and existence of nets used throughout this
%thesis. Therefore in the whole document we always assume that a topological
%space $\mathcal{X}$ is Haussdorff.
%\paragraph{}
%A topological space is said to be second countable if it has a countable base.
%Every second-countable space is separable. The reverse implications do not
%hold). A space is metrisable if and only if it is second countable.
%\paragraph{}
%A topological space is said to be separable if there exists a sequence
%$(x_n)_{n\in\mathbb{N}^*}$ of elements of $\mathcal{X}$ such that every
%nonempty open subsets of the space contains at least one element of the
%sequence. Separability plays an important role in numerical analysis because
%many theorems have only constructive proofs for separable spaces. Such
%constructive proofs can be turned into algorithms which is the primary goal of
%this work. In this document we also assume that any topological space is
%separable if there is no specific mention of the contrary. Moreover we recall
%that a Hilbert space is separable if and only if it has a countable orthonormal
%basis (Hence separable Hilbert spaces are second countable). Hence an operator
%between two separable Hilbert spaces can be written as an infinite dimensional
%matrix. In some cases we also introduce \emph{Polish spaces} which are
%separable topological spaces $\mathcal{X}$ that have at least one metric $d$
%such that $(\mathcal{X}, d)$ is complete. Then $d$ induces the topology
%$\mathcal{T}_x$ of $\mathcal{X}$. As metrisable spaces, Polish spaces are
%always second countable. Moreover every second countable locally compact
%Hausdorff space is a Polish space and every separable Banach space is a Polish
%space.
%\paragraph{}
%If $\mathcal{X}$ and $\mathcal{Y}$ are two topological spaces, we denote by
%$\mathcal{F}(\mathcal{X};\mathcal{Y})$ the topological vector space of
%functions $f:\mathcal{X}\to\mathcal{Y}$ and
%$\mathcal{C}(\mathcal{X};\mathcal{Y}) \subset
%\mathcal{F}(\mathcal{X};\mathcal{Y})$ the subspace of continuous functions,
%endowed with the product topology (topology of pointwise convergence).
%\subsubsection{Measure theory}
%A $\sigma$-algebra on $\mathcal{X}$ is a set
%$\mathcal{M}\subseteq\mathcal{P}(\mathcal{X})$ of subsets of $\mathcal{X}$,
%containing the empty set, which is closed under taking complements and
%countable unions. A pair $(\mathcal{X},\mathcal{M})$ where $\mathcal{X}$ is a
%set and $\mathcal{M}$ is a $\sigma$-algebra is called a measure space. The
%Borel $\sigma$-algebra $\mathcal{B}(\mathcal{X})$ is a $\sigma$-algebra
%generated by the open sets of $\mathcal{X}$. A measure on a measurable space
%$(\mathcal{X},\mathcal{B}(\mathcal{X}))$ is a map $\mu: \mathcal{B}(X) \to
%\overline{\mathbb{R}}_+$ which is zero on the empty set and countably additive,
%\acs{ie}~for any subset $(\mathcal{Z}_n)_{n\in\mathbb{N}}$ is a sequence of
%pairwise disjoint measurable sets,
%\begin{dmath*}
    %\mu\left(\bigcup_{n\in\mathbb{N}}\mathcal{Z}_n\right) =
    %\sum_{n\in\mathbb{N}}\mu(\mathcal{Z}_n).
%\end{dmath*}
%We note $\mathcal{N}(m, \sigma)$ the Gaussian distribution with
%mean $m\in\mathbb{R}$ and variance $\sigma^2\in\mathbb{R}$. $\mathcal{U}(a, b)$
%is the uniform distribution with support $(a, b)$ and $\mathcal{S}(m, \sigma)$
%is the hyperbolic secant distribution with mean $m$ and variance $\sigma^2$.

%\subsubsection{Vector spaces, linear operators and matrices}
%Given any vector space $\mathcal{H}$ over an Abelian field $\mathbb{K}$, the
%(continuous) dual space $\mathcal{H}^\adjoint$
%is defined as the set of all \emph{continuous} linear functionals $x^*:
%\mathcal{H} \to \mathbb{K}$.
%\paragraph{}
%Let $\mathcal{H}_1$ and $\mathcal{H}_2$ be two vector spaces.  We call operator
%any linear function from $\mathcal{H}_1$ to $\mathcal{H}_2$.
%We set $\mathcal{L}(\mathcal{H}_1;\mathcal{H}_2)$ to be the space of
%\emph{continuous} (linear) operators from $\mathcal{H}_1$ to $\mathcal{H}_2$.
%The vector space $\mathcal{H}_1$ is called the domain, noted $\Dom$ and
%$\mathcal{H}_2$ the codomain. We use the shortcut notation
%$\mathcal{L}(\mathcal{H})=\mathcal{L}(\mathcal{H}; \mathcal{H})$.  The
%transpose (or dual) of an operator $W:~\mathcal{H}_1\to\mathcal{H}_2$ is
%defined as $W^\transpose :\mathcal{H}_2^\adjoint \to \mathcal{H}_1^\adjoint$
%such that $W^\transpose :x^\adjoint\mapsto x^\adjoint(W)$.
%\paragraph{}
%Let $\mathcal{H}_1$ and $\mathcal{H}_2$ be two Hilbert spaces. The adjoint of
%an operator $W:\mathcal{H}_1\to\mathcal{H}_2$ is the unique mapping
%$W^\adjoint:\mathcal{H}_2\to\mathcal{H}_1$ such that $\inner{W^\adjoint x,
%z}_{\mathcal{H}_1}=\inner{x, Wz}_{\mathcal{H}_2}$ for all
%$x\in\Dom(W^\adjoint)$, $z\in\Dom(W)$. Its existence is guaranteed by Riesz's
%representation theorem. An operator
%$W:\Dom(W)\subseteq\mathcal{H}\to\mathcal{H}$ is said to be symmetric when
%$W^\adjoint=W$, and self-adjoint when $W$ is bounded, symmetric,
%$\Dom(W^\adjoint) = \Dom(W)$ and $\Dom(W)$ is dense in $\mathcal{H}$. If $W$ is
%bounded, symmetric and $\Dom(W)=\mathcal{H}$ then $W$ is self-adjoint.
%Interestingly if $\mathcal{H}_1$ and $\mathcal{H}_2$ are normed vector spaces,
%they can be viewed as topological vector spaces, and the notion of continuity
%coincides with that of boundedness. We recall that the induced norm of a linear
%operator is given by
%\begin{dmath*}
    %\norm{W}_{\mathcal{H}_1,\mathcal{H}_2} = \sup_{x\neq 0}
    %\frac{\norm{W x}_{\mathcal{H}_2}}{\norm{x}_{\mathcal{H}_1}}.
%\end{dmath*}
%If $W\in\mathcal{L}(\mathcal{H}_1, \mathcal{H}_2)$
%\begin{dmath*}
    %\Ker W=\Set{x\in\Dom(W) | W x = 0}
%\end{dmath*}
%denotes the kernel (nullspace), which is a vector subspace of the domain and
%\begin{dmath*}
    %\Ima W = \Set{y\in\mathcal{H}_2 | y =Wx,\enskip x \in \Dom(W)}
%\end{dmath*}
%the image (range) which is a vector subspace of the codomain $\mathcal{H}_2$.
%\paragraph{}
%If $\mathcal{H}$ is an Hilbert space on a field $\mathbb{K}$ we denote its
%scalar product by $\inner{\cdot,\cdot}_{\mathcal{H}}$ and its norm by
%$\norm{\cdot}_{\mathcal{H}}$. When the base field of $\mathcal{H}$ is
%$\mathbb{R}$, $\inner{\cdot,\cdot}_{\mathcal{H}}$ is a \emph{bilinear} form.
%When the base field of $\mathcal{H}$ is $\mathbb{C}$,
%$\inner{\cdot,\cdot}_{\mathcal{H}}$ is a \emph{sesquilinear} form.
%Let $\mathcal{H}$ be a \emph{separable} Hilbert space and let
%$(e_i)_{i\in\mathbb{N}^*}$ be a basis of $\mathcal{H}$. We call
%$(e_i^\adjoint)_{i\in\mathbb{N}^*}$ the dual basis of $\mathcal{H}$, the basis
%of $\mathcal{H}^\adjoint$ such that for all $i$, $j\in\mathbb{N}^*$,
%$e_i^\adjoint(e_j)=\inner{e_i, e_j}_{\mathcal{H}}=\delta_{ij}$. In the whole
%document we consider that $\mathcal{H}^\adjoint$ is always equipped with the
%dual basis of $\mathcal{H}$.  For a vector $x\in\mathcal{H}$ with a basis
%$(e_i)_{i\in\mathbb{N}^*}$ we write $x_i=e_i^\adjoint(x)$. For a linear
%operator $W:\mathcal{H}_1\to\mathcal{H}_2$ where $\mathcal{H}_1$ and
%$\mathcal{H}_2$ are Hilbert spaces with respective basis
%$(e_i)_{i\in\mathbb{N}^*}$ and $(e'_j)_{j\in\mathbb{N}^*}$, we note $W_i=We_i$
%and $W_{ij}=e_j^\adjoint(We_i)$. Eventually given two separable Hilbert spaces
%$\mathcal{H}_1$ and $\mathcal{H}_2$, an operator
%$W:\mathcal{H}_1\to\mathcal{H}_2$, $(e_i)_{i\in\mathbb{N}^*}$ a basis of
%$\mathcal{H}_1$ and $(e'_i)_{i\in\mathbb{N}^*}$ a basis of $\mathcal{H}_2$ we
%have
%\begin{dmath*}
    %(W^\transpose)_{ij}=e_j^{\adjoint\adjoint}W^\transpose
    %{e'}_i^\adjoint\hiderel{=} e_j^{\adjoint\adjoint}{e'}_i^\adjoint W
    %\hiderel{=} {e'}_i^\adjoint W e_j \hiderel{=} W_{ji}.
%\end{dmath*}
%\paragraph{}
%We call matrix $M$ of size $(m,n)\in\mathbb{N}^2$ on an Abelian field
%$\mathbb{K}$ a collection of elements $M=(m_{ij})_{1\le i\le m, 1\le j \le n}$,
%$m_{ij}\in\mathbb{K}$. We note $\mathcal{M}_{m,n}(\mathbb{K})$ the vector space
%of all matrices. If $\mathcal{H}_1$ and $\mathcal{H}_2$ are two separable
%Hilbert spaces on an Abelian field $\mathbb{K}$, any linear operator
%$L\in\mathcal{L}(\mathcal{H}_1;\mathcal{H}_2)$ can be viewed as a (potentially
%infinite) matrix. Let $n=\dim(\mathcal{H}_1)$, $m=\dim(\mathcal{H}_2)$ and let
%$B=(e_i)_{i=1}^{n}$ and $C=(e'_i)_{i=1}^{m}$ be the respective bases of
%$\mathcal{H}_1$ and $\mathcal{H}_2$. We note $\text{mat}_{B, C}:
%\mathcal{L}(\mathcal{H}_1;\mathcal{H}_2) \to \mathcal{M}_{m,n}(\mathbb{K})$
%such that $M=\text{mat}_{B, C}(L)=({e'}_j^\adjoint L e_i)_{1\le i\le n, 1\le j
%\le m}\in\mathcal{M}_{m,n}(\mathbb{K})$. Let
%$M_1\in\mathcal{M}_{m,n}(\mathbb{K})$ and
%$M_2\in\mathcal{M}_{n,l}(\mathbb{K})$. The product between two matrices is
%written $M_1M_2\in\mathcal{M}_{m,l}(\mathbb{K})$ and obey $(M_1M_2)_{ij} =
%\sum_{k=1}^n M_{ik}M_{kj}$. Given two linear operator
%$L_1\in\mathcal{L}(\mathcal{H}_1;\mathcal{H}_2)$ and
%$L2\in\mathcal{L}(\mathcal{H}_2;\mathcal{H}_3)$ we have
%$L_1L_2\in\mathcal{L}(\mathcal{H}_1;\mathcal{H}_3)$ and i
%\begin{dmath*}
    %\text{mat}_{B, D}(L_1L_2)=\text{mat}_{B, C}(L_1)\text{mat}_{C, D}(L_2).
%\end{dmath*}
%The operator $\text{mat}_{B, C}$ is a vector space isomorphism allowing us to
%identify $\mathcal{L}(\mathcal{H}_1;\mathcal{H}_2)$ with
%$\mathcal{M}_{mn}(\mathbb{K})$ where $n=\dim(\mathcal{H}_1)$ and
%$m=\dim(\mathcal{H}_2)$.
Notations used throughout this paper are summarized in
\cref{table:notations1}.
\begin{table}
    \centering
    \caption{Mathematical symbols and their signification (part 1).
    \label{table:notations1}}
    \begin{tabularx}{\textwidth}{cX}
        \toprule
            Symbol & \multicolumn{1}{c}{Meaning} \\
        \cmidrule{1-2}
        \endhead
            %$\colonequals$ & Equal by definition. \\
            %$\mathbb{N}$ & The semi-group of natural numbers. \\
            %$\mathbb{K}$ & Any non-discrete Abelian field endowed with an
            %absolute value. Elements of $\mathbb{K}$ are called scalars. \\
            %$\mathbb{R}$ & The Abelian field of real numbers. \\
            %$\mathbb{C}$ & The Abelian field of complex numbers. \\
            %$\mathbb{U}$ & The circle group of complex numbers with unit
            %module. \\
            %$\iu \in\mathbb{C}$ & Unit pure imaginary number
            %$\iu^2\colonequals-1$.  \\
            %$\ec \in\mathbb{R}$ & Euler constant. \\
            $e \in \mathcal{X}$ &  The neutral element of the group
            $\mathcal{X}$. \\
            $\delta_{ij}$ & Kronecker delta function. $\delta_{ij}=0$ if $i
            \neq j$, $1$ otherwise. \\
            %$\inner{\cdot,\cdot}_2$ & Euclidean inner product. \\
            %$\norm{\cdot}_2$ & Euclidean norm. \\
            %$\mathcal{X}$ & Input space. \\
            $\dual{\mathcal{X}}$ & The Pontryagin dual of $\mathcal{X}$ when
            $\mathcal{X}$ is a \acs{LCA} group. \\
            %$\mathcal{Y}$ & Output space (Hilbert space). \\
            %$\mathcal{H}$ & Feature space (Hilbert space).  \\
            $\inner{\cdot,\cdot}_{\mathcal{Y}}$ & The canonical inner
            product of the Hilbert space $\mathcal{Y}$. \\
            $\norm{\cdot}_{\mathcal{Y}}$ & The canonical norm induced by the
            inner product of the Hilbert space $\mathcal{Y}$. \\
            $\mathcal{F}(\mathcal{X};\mathcal{Y})$ & Topological vector space
            of functions from $\mathcal{X}$ to $\mathcal{Y}$. \\
            $\mathcal{C}(\mathcal{X};\mathcal{Y})$ & The topological vector
            subspace of $\mathcal{F}$ of continuous functions from
            $\mathcal{X}$ to $\mathcal{Y}$. \\
            $\mathcal{L}(\mathcal{H};\mathcal{Y})$ & The space bounded linear
            operator from a Hilbert space $\mathcal{H}$ to a
            Hilbert space $\mathcal{Y}$. \\
            $\norm{\cdot}_{\mathcal{Y},\mathcal{Y}'}$ & The operator norm
            $\norm{\Gamma}_{\mathcal{Y}, \mathcal{Y'}} =
            \sup_{\norm{y}_{\mathcal{Y}}=1}\norm{\Gamma y}_{\mathcal{Y}'}$ for
            all $\Gamma\in\mathcal{L}(\mathcal{Y},\mathcal{Y'})$ \\
            $\mathcal{M}_{m,n}(\mathbb{K})$ & The space of matrices of size
            $(m,n)$. \\
            $\mathcal{L}(\mathcal{Y})$ & The space of bounded linear operator
            from a Hilbert space $\mathcal{Y}$ to itself. \\
            $\mathcal{L}_{+}(\mathcal{Y})$ & The space of non-negative bounded
            linear operator from a Hilbert space $\mathcal{H}$ to itself. \\
            $\mathcal{B}(\mathcal{X})$ & Borel $\sigma$-algebra on a
            topological space $\mathcal{X}$. \\
            %$\mu(\mathcal{X})$ & A scalar positive measure of $\mathcal{X}$. \\
            $\Leb(\mathcal{X})$ & The Lebesgue measure of $\mathcal{X}$. \\
            $\Haar(\mathcal{X})$ & A Haar measure of $\mathcal{X}$. \\
            $\probability_{\mu, \rho}(\mathcal{X})$ & A probability measure of
            $\mathcal{X}$ whose Radon-Nikodym derivative (density) with respect
            to the measure $\mu$ is $\rho$. \\
            $\FT{\cdot}$ & The \acl{FT} operator. \\
            %$\IFT{\cdot}$ & The \acl{IFT} operator. \\
            %$\esssup$ & The essential supremum. \\
            %$L^p(\mathcal{X}, \mu)$ & The Banach space of
            %$\abs{\cdot}^p$-integrable function from
            %$(\mathcal{X},\mathcal{B}(\mathcal{X}), \mu)$ to $\mathbb{C}$ for
            %$p\in\mathbb{R}_+$. \\
            $L^p(\mathcal{X}, \mu;\mathcal{Y})$ & The Banach space of
            $\norm{\cdot}_{\mathcal{Y}}^p$ (Bochner)-integrable function from
            $(\mathcal{X},\mathcal{B}(\mathcal{X}), \mu)$ to $\mathcal{Y}$ for
            $p\in\mathbb{R}_+$. $L^p(\mathcal{X},\mu,\mathbb{R}) \colonequals
            L^p(\mathcal{X},\mu)$ and
            $L^p(\mathcal{X},\mu,\mathbb{R})=L^p(\mathcal{X}, \mu)$. \\
            $\Vect_{j=1}^D x_i$ & The direct sum of $D\in\mathbb{N}$ vectors
            $x_i$'s in the Hilbert spaces $\mathcal{H}_i$. By definition
            $\inner{\Vect_{j=1}^D x_j, \Vect_{j=1}^D z_j} = \sum_{j=1}^D
            \inner{x_j, z_j}_{\mathcal{H}_i}$. \\
            $\norm{\cdot}_p$ & The $L^p(\mathcal{X}, \mu, \mathcal{Y})$ norm.
            $\norm{f}_p^p\colonequals \int_{\mathcal{X}}
            \norm{f(x)}_{\mathcal{Y}}^p d\mu(x)$.  When
            $\mathcal{X}=\mathbb{N}^*$, $\mathcal{Y}\subseteq \mathbb{R}$ and
            $\mu$ is the counting measure and $p=2$ it coincide with the
            Euclidean norm $\norm{\cdot}_2$ for finite dimensional vectors. \\
            $\norm{\cdot}_{\infty}$ & The uniform norm $\norm{f}_{\infty}=
            \esssup \set{\norm{f(x)}_{\mathcal{Y}} |
            x\in\mathcal{X}}=\lim_{p\to\infty}\norm{f}_p$. \\
            %${}^\transpose$ & The transpose operator of a linear operator. \\
            %${}^\adjoint$ & The adjoint operator of a linear operator. \\
            $\abs{\Gamma}$ & The absolute value of the linear operator
            $\Gamma\in\mathcal{L}(\mathcal{Y})$, \acs{ie}
            $\abs{\Gamma}^2=\Gamma^{\adjoint}\Gamma$. \\
            $\Tr\left[\Gamma\right]$ & The trace of a linear operator
            $\Gamma\in\mathcal{L}(\mathcal{Y})$. \\
            %$\sigma(\Gamma)$ & The spectrum of the bounded linear operator
            %$Gamma\in\mathcal{L}(\mathcal{Y})$ where $\mathcal{Y}$ is a Hilbert
            %space, \acs{ie}~$\sigma(\Gamma)=\Set{\lambda\in\mathbb{C} |
            %\nexists s, s(\lambda e - \Gamma) = e}$. \\
            %$\lambda_i(\Gamma)$ & The $i$-th eigenvalue of
            %$\Gamma\in\mathcal{L}(\mathcal{Y})$, ranked by increasing modulus,
            %where $\mathcal{Y}$ is a \emph{separable} Hilbert space and
            %$i\in\mathbb{N}^*$. \\
            %$\rho(\Gamma)$ & The spectral radius of the linear operator
            %$\Gamma$ \acs{ie}~$\rho(\Gamma)=\sup\set{\abs{\lambda} | \lambda
            %\in \sigma(\Gamma)}$. \\
            $\norm{\cdot}_{\sigma, p}$ & The Schatten $p$-norm,
            $\norm{\Gamma}_{\sigma,
            p}^p=\Tr\left[\abs{\Gamma}^p\right]$ for
            $\Gamma\in\mathcal{L}(\mathcal{Y})$, where $\mathcal{Y}$ is a
            Hilbert space. Note that $\norm{\Gamma}_{\sigma,\infty} =
            \rho(\Gamma) \le \norm{\Gamma}_{\mathcal{Y},\mathcal{Y}}$.  \\
            $\succcurlyeq$ & \say{Greater than} in the Loewner partial order of
            operators. $\Gamma_1 \succcurlyeq \Gamma_2$ if $\sigma(\Gamma_1 -
            \Gamma_2) \subseteq \mathbb{R}_+$. \\
            %$\bar{\mathbb{R}}$ & The one point compacification of the real
            %line $\mathbb{R} \cup \Set{\infty}$. \\
            $\cong$ & Given two sets $\mathcal{X}$ and $\mathcal{Y}$,
            $\mathcal{X} \cong \mathcal{Y}$ if there exists an isomorphism
            $\phi:\mathcal{X}\to\mathcal{Y}$. \\
        \bottomrule
    \end{tabularx}
\end{table}
%\begin{table}[t]
    %\centering
    %\caption{Mathematical symbols and their signification (part 2).
    %\label{table:notations2}}
    %\begin{tabularx}{\textwidth}{cX}
        %\toprule
            %Symbol & \multicolumn{1}{c}{Meaning} \\
        %\cmidrule{1-2}
        %\endhead
        %\bottomrule
    %\end{tabularx}
%\end{table}
%\subsection{Introduction to kernel methods}\label{subsec:kernels}
%\subsubsection{Kernels and Reproducing Kernel Hilbert Spaces}
%The idea of kernel methods
%\citep{Aronszajn1950,KIMELDORF1971,boser1992training,
%Berlinet2003,Shawe-TaylorBook} is to work in a subset of the set of all
%functions, namely a \acf{RKHS}, associated to a well chosen positive
%semi-definite and symmetric function (\emph{a kernel}).
%\paragraph{}
%\begin{definition}[Positive-definite kernels]
    %Let $\mathcal{X}$ be a locally compact second countable topological space.
    %A kernel $k:\mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is said to be
    %\acf{PSD} if for any $(x_1, \ldots, x_N) \in \mathcal{X}^N$, the (Gram)
    %matrix
    %\begin{dmath*}
        %\mathbf{K} =
        %\begin{pmatrix}
            %k(x_i,x_j)
        %\end{pmatrix}_{i = 1, j = 1}^{i = N, j = N} \hiderel{\in}
        %\mathcal{M}_{N, N}(\mathbb{R})
    %\end{dmath*}
    %is \acf{SPSD}\footnote{Note that for historical reasons valid kernels are
    %called \say{positive-definite kernels}, although for any sequences of
    %points the corresponding Gram matrix needs only to be (symmetric) Positive
    %Semi-Definite \citep{fukumizu2008elements}.}.
%\end{definition}
%The following proposition gives sufficient conditions to obtain a \acs{SPSD}
%matrix.
%\begin{proposition}[\acs{SPSD} matrix]
    %$K$ is SPSD if it is symmetric and one of the following conditions holds:
    %\begin{itemize}
        %\item The eigenvalues of $\mathbf{K}$ are non-negative
        %\item for any column vector $c= (c_1, \ldots, c_N)^\transpose \in
        %\mathcal{M}_{N, 1}(\mathbb{R})$,
        %\begin{dmath*}
            %c^\transpose\mathbf{K}c=\sum_{i,j=1}^N c_ic_jK(x_i,x_i)
            %\hiderel{\geq} 0
        %\end{dmath*}
    %\end{itemize}
%\end{proposition}
%One of the most important property of \acs{PD} kernels \citep{Mohri2012} is
%that a \acs{PD} kernel defines a unique \acs{RKHS}. Note that the converse is
%also true.
%\begin{theorem}[\citet{Aronszajn1950}]
    %Suppose $k$ is a symmetric, positive-definite kernel on a set
    %$\mathcal{X}$. Then there is a unique Hilbert space of functions
    %$\mathcal{H}$ on $\mathcal{X}$ for which $k$ is a reproducing kernel,
    %\acs{ie}
    %\begin{dgroup}
        %\begin{dmath}\label{eq:reproducing-prop}
            %\forall x \in \mathcal{X}, k(\cdot, x) \hiderel{\in} \mathcal{H}
        %\end{dmath}
        %\begin{dmath}
            %\forall h \in \mathcal{H}, \forall x \hiderel{\in} \mathcal{X},
            %h(x) \hiderel{=} \inner{h,k(\cdot,x)}_{\mathcal{H}}.
        %\end{dmath}
    %\end{dgroup}
    %$\mathcal{H}$ is called a reproducing kernel Hilbert space (\acl{RKHS})
    %associated to $k$, and will be denoted, $\mathcal{H}_k$.
%\end{theorem}
%Another way to use Aronszajn's results is to state the feature map property for
%the \acs{PD} kernels.
%\begin{proposition}[Feature map]
     %Suppose $k$ is a symmetric, positive-definite kernel on a set
     %$\mathcal{X}$. Then, there exists a Hilbert space $\mathcal{H}$ and a
     %mapping $\phi$ from $\mathcal{X}$ to $\mathcal{H}$ such that:
    %\begin{dmath*}
        %\forall x, x' \hiderel{\in} \mathcal{X},~
        %k(x,x')\hiderel{=}\inner{\phi(x),\phi(x')}_{\mathcal{H}}.
    %\end{dmath*}
    %The mapping $\phi$ is called a \emph{feature map} and $\mathcal{H}$, a
    %feature space.
%\end{proposition}
%\begin{remark}
    %Aronszajn's theorem tells us that there always exists at least one feature
    %map called \emph{canonical feature map}, with associated feature space
    %called the \acl{RKHS} $\mathcal{H}=\mathcal{H}_k$ (associated with $k$),
    %\begin{dmath*}
        %\phi(x)= k(\cdot, x)
    %\end{dmath*}
    %There exists several pairs of feature maps and features spaces for a given
    %kernel $k$.
%\end{remark}
%\subsubsection{Learning in Reproducing Kernel Hilbert Spaces}
%\label{subsubsec:learning_in_rkhs}
%We place ourself in the context of supervised statistical learning
%\citep{vapnik1998statistical}, where the goal is to minimize a quantity called
%the true risk over a class of function which in our case is a \acs{RKHS}. We
%suppose that we are given a local loss function
%$L:\mathcal{X}\times\mathcal{H}_k\times \mathcal{Y} \to \mathbb{R}_+$ and that
%our training samples $(x_i, y_i)$ are \ac{iid} random variables following the
%probability distribution $\probability(X, Y)$. Then we define the true risk as
%\begin{dmath*}
    %\mathfrak{R}(f)=\expectation_{X, Y} L(X, f, Y)\condition{$X,
    %Y\sim\probability(X, Y)$}.
%\end{dmath*}
%However since the probability distribution $\probability(X, Y)$ is usually
%unknown we define and minimize its empirical counterpart called the empirical
%risk.
%\begin{dmath*}
    %\label{eq:empirical_risk}
    %\mathfrak{R}_{emp}(f, \seq{s}) = \frac{1}{N}\sum_{i=1}^NL(x_i, f, y_i).
%\end{dmath*}
%In this context, a fair question is how to pick-up functions in a space
%$\mathcal{H}_k$ with inifinitely many elements, that minimize the empirical
%risk (\cref{eq:empirical_risk}), in polynomial time? The answer comes from the
%regularization and interpolation theory. To limit the size of the space in
%which we search for the function minimizing the empirical risk we add a
%regularization term to the empirical risk.
%\begin{dmath*}
    %\mathfrak{R}_{\lambda}(f, \seq{s}) = \mathfrak{R}_{\text{emp}}(f, \seq{s})
    %+ \frac{\lambda}{2} \norm{f}_{\mathcal{H}_k}^2
    %= \frac{1}{N} \sum_{i=1}^N L\left(x_i, f,
    %y_i\right) + \frac{\lambda}{2}\norm{f}_{\mathcal{H}_k}^2
%\end{dmath*}
%and we minimize $\mathfrak{R}_{\lambda}$ with respect to $f$ for a given
%$\seq{s}$ instead of $\mathfrak{R}_{\text{emp}}$. Then the representer theorem
%(also called minimal norm interpolation theorem) states the following.
%\begin{theorem}[Representer theorem, \citet{Wahba90}]
    %If $f_{\seq{s}}$ is a solution of
    %\begin{dmath*}
        %\argmin_{f\in\mathcal{H}_k} \mathfrak{R}_{\lambda}(f, \seq{s}),
    %\end{dmath*}
    %where $\lambda > 0$ then $f_{\seq{s}}=\sum_{i=1}^N k(\cdot, x_i) \alpha_i$.
%\end{theorem}
%We note the vector $\boldsymbol{\alpha} = (\alpha_i)_{i=1}^N$ and the matrix
%$\mathbf{K}=(k(x_i, x_k))_{i, k = 1}^N$. Because of the representer theorem,
%stating that a solution of the empirical risk minimization is a linear
%combination of kernel evaluations weighted by a vector $\boldsymbol{\alpha}$,
%with mild abuse of notation we identify the function $f\in\mathcal{H}_k$ with
%the vector $\boldsymbol{\alpha}\in\mathbb{R}^N$.  Thus we rewrite the loss
%$L(x, f, y)$ as $L(x, \boldsymbol{\alpha}, y)$. Then we can rewrite
%\begin{dmath*}
    %\mathfrak{R}_{\lambda}(\boldsymbol{\alpha}, \seq{s}) =
    %\frac{1}{N}
    %\sum_{i=1}^NL(x_i, \boldsymbol{\alpha}, y_i) +
    %\frac{\lambda}{2} \inner{\boldsymbol{\alpha},
    %\mathbf{K}\boldsymbol{\alpha}}_2,
%\end{dmath*}
%and $f(x_i) = (\mathbf{K}\boldsymbol{\alpha})_i$ for any $x_i$ in the
%training set. For instance if we choose $L(x, f,
%y)=\frac{1}{2}\abs{f(x)-y}^2$ to be the least square loss, then
%\begin{dmath*}
    %L(x_i, \boldsymbol{\alpha}, y_i) =
    %\frac{1}{2}\abs{(\mathbf{K}\boldsymbol{\alpha})_i-y_i}^2.
%\end{dmath*}
%In this case $L$ is convex in $\boldsymbol{\alpha}$, thus it is possible to
%derive a polynomial time (in $N$) algorithm minimizing $\mathfrak{R}_{\lambda}$
%for the least square loss, which is called \emph{kernel Ridge regression}:
%\begin{dmath}
    %\label{eq:ridge_regression}
    %\mathfrak{R}_{\lambda}(\boldsymbol{\alpha}, \seq{s}) =
    %\frac{1}{2N}\norm{\mathbf{K}\boldsymbol{\alpha} - (y_i)_{i=1}^N}_2^2 +
    %\frac{\lambda}{2} \inner{\boldsymbol{\alpha},
    %\mathbf{K}\boldsymbol{\alpha}}_2.
%\end{dmath}
%As a result of the representer theorem we see that we search a minimizer over
%$\boldsymbol{\alpha}\in\mathbb{R}^N$ instead of $f\in\mathcal{H}_k$. By strict
%convexity and coercivity of $\mathfrak{R}_{\lambda}$, and because $\mathbf{K} +
%\lambda I_N$ is invertible\footnote{Note that although $\mathbf{K} + \lambda
%I_N$ is always invertible if $\lambda>0$, choosing a too small value of
%$\lambda$ can leads to an ill-conditioned system if the eigenvalues of
%$\mathbf{K}+\lambda I_N$ are too small.} for any $\lambda > 0$, the unique
%solution is $\alpha_{\seq{s}} = \argmin_{\boldsymbol{\alpha}\in\mathbb{R}^N}
%\mathfrak{R}_{\lambda}(\boldsymbol{\alpha},\seq{s}) = (\mathbf{K}/N + \lambda
%I_N)^{-1}(y_i)_{i=1}^N$. This is an $O_t\left(N^3\right)$ algorithm.
%\paragraph{}
%Another way of describing positive-definite kernels and \acs{RKHS} consists in
%defining a \emph{feature map} $\phi:\mathcal{X}\to\mathcal{H}$ where
%$\mathcal{H}$ is a Hilbert space.  Then any function in $\mathcal{H}_k$ can be
%written $f(x)=\inner{\phi(x), \theta}_{\mathcal{H}}$ In a nutshell the function
%$\phi$ is called feature map because it \say{extracts characteristic elements
%from a vector}. Usually a feature map takes a vector in an input space with low
%dimension and maps it to a potentially infinite dimensional Hilbert space. Put
%it differently, any function in $\mathcal{H}_k$ is the composition of linear
%functional $\theta^\adjoint$ with a non linear feature map $\phi$. Thus if the
%feature map $\phi$ is fixed (which is equivalent to fixing the kernel), it is
%possible to \say{learn} with a linear class of functions $\theta\in\mathcal{H}$
%(see \cref{fig:feature_map}).
%\begin{figure}
    %%\centering\resizebox{\textwidth}{!}{%
    %%\begin{tikzpicture}
        %%\node[inner sep=0pt] (input) at (0,0)
            %%{\includegraphics[width=.35\textwidth]{./gfx/input.eps}};
        %%\node[inner sep=0pt] (feature) at (5,-6)
            %%{\includegraphics[width=.35\textwidth]{./gfx/feature.eps}};
        %%\draw[->,thick] (input.east) -- (feature.west)
            %%node[midway,fill=white] {$\phi:\mathcal{X} \to \mathcal{H}$};
    %%\end{tikzpicture}}
    %\centering
    %\begin{tabular}{c}
        %\includegraphics[valign=m, width=.5\textheight]{./gfx/input.eps} \\
        %$\xdownarrow{1cm} \phi: \enskip \mathcal{X} = \mathbb{R}^2 \to
        %\mathcal{H} = \mathbb{R}^3$ \\
        %\includegraphics[valign=m, width=.5\textheight]{./gfx/feature.eps}
    %\end{tabular}
    %\caption[A scalar-valued feature map]{We map the two circles in
    %$\mathbb{R}^2$ to $\mathbb{R}^3$. In $\mathbb{R}^3$ it is now possible to
    %separate the circles with a linear functional: a plane. We used the feature
    %map \\ $\phi(x) = 0.82 \begin{pmatrix} \cos(1.76 x_1 + 2.24 x_2 + 2.75) \\
    %\cos(0.40 x_1 + 1.87 x_2 + 5.6) \\ \cos(0.98 x_1 - 0.98 x_2 + 6.05)
    %\end{pmatrix}$. \\
    %Here $\phi:\mathbb{R}^2\to\mathbb{R}^3$ has been chosen
    %as a realization of an \acs{RFF} map (see \cref{eq:rff2}). A \say{cleaner}
    %feature map adapted to this problem could have been \\
    %$\phi(x)=\begin{pmatrix} x_1 \\ x_2 \\ x_1^2 + x_2^2 \end{pmatrix}$.
    %\label{fig:feature_map}}
%\end{figure}
%If we note
%\begin{dmath*}
    %\boldsymbol{\phi} =
    %\begin{pmatrix}
        %\phi(x_1) & \dots & \phi(x_N)
    %\end{pmatrix}
%\end{dmath*}
%the \say{matrix} where each column represents the feature map evaluated at the
%point $x_i$ with $1 \le i \le N$, the regularized risk minimization with the
%least square loss reads
%\begin{dmath*}
    %\mathfrak{R}_{\lambda}(\theta, \seq{s}) =
    %\frac{1}{2N}\norm{\boldsymbol{\phi}^\transpose \theta - (y_i)_{i=1}^N
    %}_{2}^2 + \frac{\lambda}{2}\norm{\theta}_2^2.
%\end{dmath*}
%and the unique solution is $\theta_{\seq{s}} =
%\left(\boldsymbol{\phi}\boldsymbol{\phi}^\transpose/N + \lambda
%I_{\mathcal{H}}\right)^{-1}\boldsymbol{\phi}$. This is an
%\begin{dmath*}
    %O_t\left( \dim(\mathcal{H})^2(N + \dim{\mathcal{H}}) \right).
%\end{dmath*}
%time complexity algorithm.  This algorithm seems more appealing than its kernel
%counterpart when many data are given since once the space $\mathcal{H}$ has
%been fixed, the algorithm is linear in the number of training points. However
%many questions remains. First although it is possible to design a feature map
%\emph{ex nihilo}, can we design systematically a feature map from a kernel? For
%some kernels (\acs{eg} the Gaussian kernel) it is well known that the Hilbert
%space corresponding to it has dimension $\dim(\mathcal{H}) = \infty$. Is it
%possible to find an approximation of the kernel such that $\dim(\mathcal{H}) <
%\infty$? If such a construction is possible and we know that $N$ training data
%are available, is it possible to have a sufficiently good approximation%
%\footnote{When $\dim(\mathcal{H}) \ge N$ then is it is better to use the kernel
%algorithm than the feature algorithm. This is called the kernel trick.} with
%$\dim(\mathcal{H}) \ll N$?
%\subsection{Towards large scale learning with kernels}
%Motivated by large scale applications, different methodologies have been
%proposed to approximate kernels and feature maps. This subsection briefly
%reminds the main approaches based on  Random Fourier Features and Nystr\"om
%techniques. Notice that another line of research concerns online learning
%method such as \acs{NORMA} developed in \cite{kivinen2004online}, later
%extended to the operator-valued kernel case by \citet{audiffren2013online}.  We
%start with the seminal work of \citet{Rahimi2007} who show that given a
%continuous shift-invariant kernel ($\forall x, z, t \in \mathcal{X}$, $k(x + t,
%z + t) = k(x, z)$), it is possible to obtain a feature map called \acs{RFF}
%that approximate the given kernel.
%\subsubsection{Random Fourier Feature maps}
\subsection{Random Fourier Feature maps}
\aclp{RFF} methodology introduced  by \citet{Rahimi2007} provides a
way to scale up kernel methods when kernels are Mercer and
\emph{translation-invariant}.  We view the input space $\mathcal{X}$ as a group
endowed with the addition. Extensions to other group laws such as
\citet{li2010random} are described in \cref{subsubsec:skewedchi2} within the
general framework of operator-valued kernels.
\paragraph{}
Denote $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ a positive
definite kernel \citep{Aronszajn1950} on $\mathbb{R}^d$. A kernel $k$ is said
to be \emph{shift-invariant} or \emph{translation-invariant} for the addition
if for for all $(x,z,t) \in \left(\mathbb{R}^d\right)^3$ we have $k(x+t,z+t) =
k(x,z)$.  Then, we define $k_0: \mathbb{R}^d \to \mathbb{R}$ the function such
that $k(x,z)= k_0(x-z)$. $k_0$ is called the \emph{signature} of kernel $k$. If
$k_0$ is a continuous function we call the kernel \say{Mercer}. Then, Bochner's
theorem \citep{folland1994course} is the theoretical result that leads to the
Random Fourier Features.
\begin{theorem}[Bochner's theorem]\label{th:bochner-scalar}
    Any continuous positive-definite function (\acs{eg} a Mercer kernel) is the
    \acl{FT} of a bounded non-negative Borel measure.
\end{theorem}
It implies that any positive-definite, continuous and shift-invariant kernel
$k$, have a continuous and positive-definite signature $k_0$, which is the
\acl{FT} $\mathcal{F}$ of a non-negative measure $\mu$. We therefore have the
$k(x,z)=k_0(x-z) \hiderel{=} \int_{\mathbb{R}^d} \exp(-\iu \inner{\omega,x -
z}) d\mu(\omega) \hiderel{=}\FT{k_0}(\omega)$.  Moreover $\mu = \IFT{k_0}$.
Without loss of generality, we assume that $\mu$ is a probability measure,
\acs{ie} $\int_{\mathbb{R}^d} d\mu(\omega)=1$ by renormalizing the kernel since
$\int_{\mathbb{R}^d}d\mu(\omega)= \int_{\mathbb{R}^d}\exp(-\iu \inner{\omega,
0})d\mu(\omega)\hiderel{=}k_0(0)$.  and we can write the kernel as an
expectation over a probability measure $\mu$.  For all $x$, $z\in\mathbb{R}^d$
\begin{dmath*}
    k_0(x-z) = \expectation_{\omega\sim\mu}\left[\exp(-\iu \inner{\omega,x -
    z})\right].
\end{dmath*}
Eventually, if $k$ is real valued we only write the real part, $k(x,z) =
\expectation_{\omega\sim\mu}[\cos \inner{\omega,x - z}] \hiderel{=}
\expectation_{\omega\sim\mu}[ \cos \inner{\omega,z} \cos \inner{\omega,x} +
\sin \inner{\omega,z} \sin \inner{\omega,x}]$.  Let $\Vect_{j=1}^D x_j$ denote
the $Dd$-length column vector obtained by stacking vectors $x_j \in
\mathbb{R}^d$.  The feature map $\widetilde{\phi}: \mathbb{R}^d \rightarrow
\mathbb{R}^{2D}$ defined as
\begin{dmath}
    \label{eq:rff}
    \widetilde{\phi}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos{\inner{x,\omega_j}} \\
        \sin{\inner{x,\omega_j}}
    \end{pmatrix}\condition{$\omega_j \hiderel{\sim} \IFT{k_0}$ \acs{iid}}
\end{dmath}
is called a \emph{Random Fourier Feature} (map). Each $\omega_{j}, j=1, \ldots,
D$ is independently and identically sampled from the inverse Fourier transform
$\mu$ of $k_0$. This Random Fourier Feature map provides the following
Monte-Carlo estimator of the kernel: $\widetilde{k}(x, z) =
\widetilde{\phi}(x)^* \widetilde{\phi}(z)$. Using trigonometric identities,
\citet{Rahimi2007} showed that the same feature map can also be written
\begin{dmath}
    \label{eq:rff2}
    \tilde{\phi}(x)=\sqrt{\frac{2}{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos(\inner{x,\omega_j} + b_j)
    \end{pmatrix},
\end{dmath}
where $\omega_j \hiderel{\sim} \IFT{k_0}$, $b_j \sim \mathcal{U}(0, 2\pi)$
\acs{iid}.  The feature map defined by \cref{eq:rff} and \cref{eq:rff2} have
been compared in \citet{sutherland2015} where they give the condition under
wich \cref{eq:rff} has lower variance than \cref{eq:rff2}. For instance for the
Gaussian kernel, \cref{eq:rff} has always lower variance. In practice,
\cref{eq:rff2} is easier to program. In this paper we focus on random Fourier
feature of the form given in \cref{eq:rff}.
\paragraph{}
The dimension $D$ governs the precision of this approximation, whose uniform
convergence towards the target kernel can be found in \citet{Rahimi2007} and in
more recent papers with some refinements proposed in \citet{sutherland2015} and
\citet{sriper2015}.  Finally, it is important to notice that Random Fourier
Feature approach \emph{only} requires two steps before the application of a
learning algorithm: (1) define the inverse Fourier transform of the given
shift-invariant kernel, (2) compute the randomized feature map using the
spectral distribution $\mu$.  \citet{Rahimi2007} show that for the Gaussian
kernel $k_0(x-z) = \exp(-\gamma \norm{x - z}_2^2)$, the spectral distribution
$\mu$ is a Gaussian distribution. For the Laplacian kernel $k_0(x-z) =
\exp(-\gamma \norm{x - z}_1)$, the spectral distribution is a Cauchy
distribution.

\subsubsection{Extensions of the RFF method}
\paragraph{}
The seminal idea of \citet{Rahimi2007} has opened a large literature on random
features. Nowadays, many classes of kernels other than translation invariant
are now proved to have an efficient random feature representation.
\citet{kar2012random} proposed random feature maps for dot product kernels
(rotation invariant) and \citet{hamid2014compact} improved the rate of
convergence of the approximation error for such kernels by noticing that
feature maps for dot product kernels are usually low rank and may not utilize
the capacity of the projected feature  space  efficiently. \Citet{pham2013fast}
proposed fast random feature maps for polynomial kernels.
\paragraph{}
\Citet{li2010random} generalized the original \acs{RFF} of \citet{Rahimi2007}.
Instead of computing feature maps for shift-in\-va\-riant kernels on the
additive group $(\mathbb{R}^d, +)$, they used the generalized Fourier transform
on any locally compact abelian group to derive random features on the
multiplicative group $(\mathbb{R}^d, *)$. In the same spirit
\citet{yang2014random} noticed that an theorem equivalent to Bochner's theorem
exists on the semi-group $(\mathbb{R}_{>0}^d, +)$. From this they derived
\say{Random Laplace} features and used them to approximate kernels adapted to
learn on histograms.
\paragraph{}
To speed-up the convergence rate of the random features approximation,
\citet{yang2014quasi} proposed to sample the random variable from a quasi
Monte-Carlo sequence instead of \acs{iid}~random variables. \Citet{Le2013}
proposed the \say{Fastfood} algorithm to reduce the complexity of computing a
\acs{RFF} --using structured matrices and a fast Walsh-Hadarmard transform--
from $O_t(Dd)$ to $O_t(D\log(d))$. More recently \citet{felix2016orthogonal}
proposed also an algorithm \say{SORF} to compute Gaussian \acs{RFF} in
$O_t(D\log(d))$ but with better convergence rates than \say{Fastfood}
\citep{Le2013}.  \Citet{mukuta2016kernel} proposed a data dependent feature
map (comparable to the Nystro\"m method) by estimating the distribution of the
input data, and then finding the eigenfunction decomposition of Mercer's
integral operator associated to the kernel.
\paragraph{}
In the context of large scale learning and deep learning, \citet{lu2014scale}
showed that \acsp{RFF} can achieve performances comparable to deep-learning
methods by combining multiple kernel learning and composition of kernels along
with a scalable parallel implementation. \Citet{dai2014scalable} and
\citet{xie2015scale} combined \acsp{RFF} and stochastic gradient descent to
define an online learning algorithm called \say{Doubly stochastic gradient
descent} adapted to large scale learning. \Citet{yang2015deep} proposed and
studied the idea of replacing the last fully interconnected layer of a deep
convolutional neural network \citep{lecun1995convolutional} by the
\say{Fastfood} implementation of \acsp{RFF}.
\paragraph{}
Eventually \citet{Yang2015} introduced the algorithm \say{\`A la Carte}, based
on \say{Fastfood} which is able to learn the spectral distribution

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{On Operator-Valued Kernels}
\label{sec:background_on_operator-valued_kernels} We now introduce the theory
of \acf{vv-RKHS} that provides a flexible framework to study and learn
vector-valued functions. The fundations of the general theory of scalar kernels
is mostly due to \citet{Aronszajn1950}  and provides a unifying point of view
for the study of an important class of Hilbert spaces of real or complex valued
functions. It has been first applied in the theory of partial differential
equation. The theory of \acfp{OVK} which extends the scalar-valued kernel was
first developped by \citet{Pedrick57} in his Ph.~D Thesis. Since then it has
been successfully applied to machine learning by many authors. In particular we
introduce the notion of \aclp{OVK} following the propositions of
\citet{Micchelli2005,carmeli2006vector,Carmeli2010}.
\subsection{Definitions and properties}
\label{subsec:def_properties} In machine learning the goal is often to find a
function $f$ belonging to a class of functions
$\mathcal{F}(\mathcal{X};\mathcal{Y})$ that minimizes a criterion called the
true risk. The class of functions we consider are functions living in a Hilbert
space $\mathcal{H}\subset\mathcal{F}(\mathcal{X};\mathcal{Y})$. The
completeness allows to consider sequences of functions $f_n \in\mathcal{H}$
where the limit $f_n\to f$ is in $\mathcal{H}$. Moreover the existence of an
inner product gives rise to a norm and also makes $\mathcal{H}$ a metric space.
\paragraph{}
Among all these functions $f\in\mathcal{H}$, we consider a subset of functions
$f\in\mathcal{H}_K\subset\mathcal{H}$ such that the evaluation map
$\text{ev}_x:f\mapsto f(x)$ is bounded for all $x$. \acs{ie} such
that~$\norm{\text{ev}_x}_{\mathcal{H}_K}\le C_x\in\mathbb{R}$ for all $x$. For
scalar valued kernels the evaluation map is a linear functional. Thus by Riesz's
representation theorem there is an isomorphism between evaluating a function at
a point and an inner product: $f(x)=\text{ev}_x f = \inner{K_x, f}_K$. From
this we deduce the reproducing property $K(x,z)=\inner{K_x, K_z}_K$ which is
the cornerstone of many proofs in machine learning and functional analysis.
When dealing with vector-valued functions, the evaluation map $\text{ev}_x$ is
no longer a linear functional, since it is vector-valued. However, inspired by
the theory of scalar valued kernel, many authors showed that if the evaluation
map of functions with values in a Hilbert space $\mathcal{Y}$ is bounded, a
similar reproducing property can be obtained; namely $\inner{y',
K(x,z)y}=\inner{K_x y', K_z y}_K$ for all $y$, $y'\in\mathcal{Y}$. This
motivates the following definition of a \acf{vv-RKHS}.
\begin{definition}[\acl{vv-RKHS}~\citep{carmeli2006vector,Micchelli2005}]
    Let $\mathcal{Y}$ be a (real or complex) Hilbert space. A \acl{vv-RKHS} on
    a locally compact second countable topological space $\mathcal{X}$ is a
    Hilbert space $\mathcal{H}$ such that
    \begin{enumerate}
        \item the elements of $\mathcal{H}$ are functions from $\mathcal{X}$ to
        $\mathcal{Y}$ (\acs{ie}~$\mathcal{H} \subset \mathcal{F}(\mathcal{X},
        \mathcal{Y})$);
        \item for all $x\in\mathcal{X}$, there exists a positive constant $C_x$
        such that for all $f\in\mathcal{H}$ $\norm{f(x)}_{\mathcal{Y}}\le
        C_x\norm{f}_{\mathcal{H}}$.
    \end{enumerate}
\end{definition}
Throughout this section we show that a \ac{vv-RKHS} defines a unique
po\-si\-ti\-ve-de\-fi\-ni\-te function called \acf{OVK} and conversely an
\ac{OVK} uniquely defines a \ac{vv-RKHS}. The bijection between \acsp{OVK} and
\acsp{vv-RKHS} has been first proved by~\citet{Senkene73} in 1973. In this
introduction to \acsp{OVK} we follow the definitions and most recent proofs
of~\citet{Carmeli2010}.
%\begin{definition}[Positive-definite \acl{OVK} acting on a complex Hilbert
%space]
    %\label{def:reproducing_kernel}
    %Given $\mathcal{X}$ a locally compact second countable topological space
    %and  $\mathcal{Y}$ a complex Hilbert Space, a map
    %$K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ is called an
    %positive-definite \acl{OVK} kernel if
    %\begin{dmath}
        %\sum_{i,j=1}^N\inner{K(x_i,x_j)y_j,y_i}_{\mathcal{Y}}\ge 0,
    %\end{dmath}
    %for all $N\in\mathbb{N}$, for all sequences of points $(x_i)_{i=1}^N$ in
    %$\mathcal{X}^N$ and all sequences of points $(y_i)_{i=1}^N$ in
    %$\mathcal{Y}^N$. \label{def:ovk}
%\end{definition}
%If $\mathcal{Y}$ is a real Hilbert space, a positive-definite \acl{OVK} is
%always self-adjoint, \acs{ie}~$K(x,z)=K(z,x)^\adjoint$. This gives rise to the
%following definition of positive-definite \acl{OVK} acting on a real Hilbert
%space.
\begin{definition}[Positive-definite \acl{OVK}]
    \label{def:reproducing_kernel_real} Given $\mathcal{X}$ a locally compact
    second countable topological space and $\mathcal{Y}$ a real Hilbert Space,
    a map $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ is called
    a positive-definite \acl{OVK} kernel if $K(x,z)=K(z,x)^\adjoint$ and
    \begin{dmath}
        \sum_{i,j=1}^N\inner{K(x_i,x_j)y_j,y_i}_{\mathcal{Y}}\ge 0,
    \end{dmath}
    for all $N\in\mathbb{N}$, for all sequences of points $(x_i)_{i=1}^N$ in
    $\mathcal{X}^N$, and all sequences of points  $(y_i)_{i=1}^N$ in
    $\mathcal{Y}^N$. \label{def:ovk_real}
\end{definition}
As in the scalar case any \acl{vv-RKHS} defines a unique positive-definite
\acl{OVK} and conversely a positive-definite \acl{OVK} defines a unique
\acl{vv-RKHS}.
\begin{proposition}[\citep{carmeli2006vector}]
    \label{pr:unique_rkhs} Given a \acl{vv-RKHS} there is a unique
    positive-definite \acl{OVK}
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$.
\end{proposition}
iven $x\in\mathcal{X}$,
$K_x:\mathcal{Y}\to\mathcal{F}(\mathcal{X};\mathcal{Y})$ denotes the linear
operator whose action on a vector $y$ is the function
$K_xy\in\mathcal{F}(\mathcal{X};\mathcal{Y})$ defined for all $z\in\mathcal{X}$
by $K_x=\text{ev}_x^\adjoint$. As a consequence we have that
\begin{dmath}
    \label{eq:trivial_feature_op}
    K(x,z)y\hiderel{=}\text{ev}_x\text{ev}_z^\adjoint y\hiderel{=}K_x^\adjoint
    K_zy\hiderel{=}(K_zy)(x).
\end{dmath}
Some direct consequences follow from the definition.
\begin{enumerate}
    \item The kernel reproduces the value of a function $f\in\mathcal{H}$ at a
    point $x\in\mathcal{X}$ since for all $y\in\mathcal{Y}$ and
    $x\in\mathcal{X}$, $\text{ev}_x^\adjoint y=K_xy=K(\cdot,x)y$ such that
    $\label{eq:reproducing_prop} \inner{f(x),y}_{\mathcal{Y}}
    \hiderel{=}\inner{f,K(\cdot, x)y}_{\mathcal{H}}
    \hiderel{=}\inner{K_x^*f,y}_{\mathcal{Y}}$.
    \item For all $x\in\mathcal{X}$ and all $f\in\mathcal{H}$,
    $\norm{f(x)}_{\mathcal{Y}}\le
    \sqrt{\norm{K(x,x)}_{\mathcal{Y},\mathcal{Y}}}\norm{f}_{\mathcal{H}}$. This
    comes from the fact that $\norm{K_x}_{\mathcal{Y} ,\mathcal{H}} =
    \norm{K_x^\adjoint}_{\mathcal{H}, \mathcal{Y} }=
    \sqrt{\norm{K(x,x)}_{\mathcal{Y}, \mathcal{Y}}}$ and the operator norm is
    sub-multiplicative.
\end{enumerate}
Additionally given a positive-definite \acl{OVK}, it defines a unique
\ac{vv-RKHS}.
\begin{proposition}[\citep{carmeli2006vector}]
    Given a positive-definite \acl{OVK}
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$, there is a
    unique \acl{vv-RKHS} $\mathcal{H}$ on $\mathcal{X}$ with reproducing kernel
    $K$.
\end{proposition}
Since an positive-definite \acl{OVK} defines a unique \acf{vv-RKHS} and
conversely a \ac{vv-RKHS} defines a unique \acl{OVK}, we denote the Hilbert
space $\mathcal{H}$ endowed with the scalar product $\inner{\cdot,\cdot}$
respectively $\mathcal{H}_K$ and $\inner{\cdot,\cdot}_K$. From now we refer to
positive-definite \aclp{OVK} or reproducing \aclp{OVK} as \aclp{OVK}. As a
consequence, given $K$ an
\acl{OVK}, define $K_x=K(\cdot,x)$ we have
\begin{dgroup}
    \begin{dmath}
        \label{eq:kernel_operator_product}
        K(x,z)=K^\adjoint_x K_z \enskip\forall x,z\hiderel{\in}\mathcal{X}
    \end{dmath},
    \begin{dmath}
        \label{eq:span_RKHS}
        \mathcal{H}_K=\lspan\Set{K_x y | \forall
        x\hiderel{\in}\mathcal{X},\enskip\forall y\hiderel{\in}\mathcal{Y} }.
    \end{dmath}
\end{dgroup}
Where $\lspan$ is the closed span of a given set. Another way to describe
functions of $\mathcal{H}_K$ consists in using a suitable feature map.
\begin{proposition}[Feature Operator~\citep{Carmeli2010}]
    \label{pr:feature_operator} Let $\mathcal{H}$ be any Hilbert space and
    $\Phi:\mathcal{X}\to\mathcal{L}(\mathcal{Y};\mathcal{H})$, with $\Phi_x :=
    \Phi(x)$. Then the operator $W : \mathcal{H} \to \mathcal{F}(\mathcal{X};
    \mathcal{Y})$ defined for all $g \in\mathcal{H}$, and for all
    $x\in\mathcal{X}$ by $(W g)(x) = \Phi_x^\adjoint g$ is a partial isometry
    from $\mathcal{H}$ onto the \ac{vv-RKHS} $\mathcal{H}_K$ with reproducing
    kernel $K(x,z)=\Phi^\adjoint_x\Phi_z, \enskip \forall x,
    z\hiderel{\in}\mathcal{X}$.  $W^\adjoint W$ is the orthogonal projection
    onto $(\Ker W)^\perp = \lspan\Set{\Phi_x y | \forall
    x\hiderel{\in}\mathcal{X},\enskip\forall y\hiderel{\in}\mathcal{Y}}$.  Then
    $\label{eq:norm_relation_w} \norm{f}_K=\inf\Set{\norm{g}_{\mathcal{H}} |
    \forall g \in\mathcal{H},\enskip Wg=f}$.
\end{proposition}
In this work we mainly focus on the kernel functions inducing a \acs{vv-RKHS}
of continuous functions. Such kernel are named $\mathcal{Y}$-Mercer kernels and
generalize Mercer kernels.
\begin{definition}[$\mathcal{Y}$-Mercer kernel \citet{Carmeli2010}]
    A positive definite \acs{OVK}
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ is called
    $\mathcal{Y}$-Mercer if the associated \acs{vv-RKHS} $\mathcal{H}_K$ is a
    subspace of the space of continuous functions
    $\mathcal{C}(\mathcal{X};\mathcal{Y})$.
\end{definition}
\subsection{Examples of \aclp{OVK}}
\label{subsec:ovk-ex}
In this subsection we list some \acfp{OVK} that have been used successfully in
the litterature. We do not recall the proof that the following kernels are well
defined are refer the interrested reader to the respective authors original
work.
\paragraph{}
\acsp{OVK} have been first introduced in Machine Learning to solve multi-task
regression problems. Multi-task regression is encountered in many fields such
as structured classification when classes belong to a hierarchy for instance.
Instead of solving independently $p$ single output regression task, one would
like to take advantage of the relationships between output variables when
learning and making a decision.
\begin{proposition}[Decomposable kernel \citep{Micheli2013}]
    \label{dec-kernel}
    Let $\Gamma$ be a non-negative operator of $\mathcal{L}_+(\mathcal{Y})$.
    $K$ is said to be a \emph{decomposable}
    kernel\footnote{Some authors also refer to as \emph{separable} kernels.} if for
    all $(x,z) \in \mathcal{X}^2$, $K(x,z) \colonequals k(x,z)\Gamma$, where
    $k$ is a \emph{scalar} kernel.
\end{proposition}
When $\mathcal{Y}=\mathbb{R}^p$, the operator $\Gamma$ can be represented by a
matrix which can be interpreted as encoding the relationships between the
outputs coordinates.  If a graph coding for the proximity between tasks is
known, then it is shown in~\citet{Evgeniou2005,Baldassare2010,Alvarez2012} that
$\Gamma$ can be chosen equal to the pseudo inverse $L^{\dagger}$ of the graph
Laplacian such that the norm in $\mathcal{H}_K$ is a graph-regularizing penalty
for the outputs (tasks).  When no prior knowledge is available, $\Gamma$ can be
set to the empirical covariance of the output training data or learned with one
of the algorithms proposed in the literature~\citep{Dinuzzo2011, Sindhwani2013,
Lim2015}. Another interesting property of the decomposable kernel is its
universality (a kernel which may approximate an arbitrary continuous target
function uniformly on any compact subset of the input space). A reproducing
kernel $K$ is said \emph{universal} if the associated \ac{vv-RKHS}
$\mathcal{H}_K$ is \emph{dense} in the space of continuous functions
$\mathcal{C}(\mathcal{X},\mathcal{Y})$.  The conditions for a kernel to be
universal have been discussed in~\citet{caponnetto2008,Carmeli2010}. In
particular they show that a decomposable kernel is universal provided that the
scalar kernel $k$ is universal and the operator $\Gamma$ is injective.  Given
$(e_k)_{k=1}^p$ a basis of $\mathcal{Y}$, we recall here how the matrix
$\Gamma$ act as a regularizer between the components of the outputs $f_k =
\inner{f(\cdot), e_k}_{\mathcal{Y}}$ of a function $f\in\mathcal{H}_K$.  We
prove a generalized version of \cref{pr:kernel_reg} to any \acl{OVK} in
\cref{subsec:regularization_property}.
\begin{proposition}[Kernels and Regularizers~\citep{Alvarez2012}]
    \label{pr:kernel_reg}
    Let $K(x,z) \colonequals k(x,z)\Gamma$ for all $x$, $z\in\mathcal{X}$ be a
    decomposable kernel where $\Gamma$ is a matrix of size $p\times p$. Then
    for all $f\in\mathcal{H}_K$, $\norm{f}_K = \sum_{i,j=1}^p
    \left(\Gamma^\dagger\right)_{ij}\inner{f_i,f_j}_k$ where $f_i=\inner{f,e_i}$
    (resp $f_j=\inner{f,e_j}$), denotes the $i$-th (resp $j$-th) component of
    $f$.
\end{proposition}
\paragraph{}
Curl-free and divergence-free kernels provide an interesting application of
operator-valued kernels~\citep{Macedo2008, Baldassare2012, Micheli2013} to
\emph{vector field} learning, for which input and output spaces have the same
dimensions ($d=p$). Applications cover shape deformation
analysis \citep{Micheli2013} and magnetic fields
approximations \citep{Wahlstrom2013}. These kernels discussed in
\citep{Fuselier2006} allow encoding input-dependent similarities between
vector-fields. An illustration of a synthetic $2D$ curl-free and divergence
free fields are given respectively in \cref{fig:curl-field} and
\cref{fig:div-field}. To obain the curl-free field we took the gradient of
a mixture of five two dimensional Gaussians (since the gradient of a potential
is always curl-free). We generated the divergence-free field by taking the
orthogonal of the curl-free field.
\begin{figure}
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[trim=1.8cm 1cm 2cm 1cm,width=.8\textwidth,clip=true]{./gfx/curl_field.eps}
        \captionof{figure}{Synthetic $2D$ curl-free field \label{fig:curl-field}.}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[trim=1.8cm 1cm 2cm 1cm,width=.8\textwidth,clip=true]{./gfx/div_field.eps}
        \captionof{figure}{Synthetic $2D$ divergence-free field
        \label{fig:div-field}.}
    \end{minipage}
\end{figure}
\begin{proposition}[Curl-free and Div-free kernel \citep{Macedo2008}]
    \label{curl-div-free}
    Assume $\mathcal{X}=(\mathbb{R}^d, +)$ and $\mathcal{Y}=\mathbb{R}^p$ with
    $d=p$. The \emph{divergence-free} kernel is defined as
    $K^{div}(x,z)=K^{div}_0(\delta) \hiderel{=} (\nabla\nabla^\transpose  -
    \Delta I) k_0(\delta)$ and the \emph{curl-free} kernel as $K^{curl}(x,z)
    \hiderel{=} K_0^{curl}(\delta) =-\nabla\nabla^\transpose k_0(\delta)$,
    where $\nabla$ is the gradient operator
    %\footnote{See
    %\cref{subsec:gradient_methods} for a formal definition of the operator
    %$\nabla$.}
    , $\nabla\nabla^\transpose $ is the Hessian operator and $\Delta$ is the
    Laplacian operator.
\end{proposition}
\subsection{Shift-Invariant \acs{OVK} on \acs{LCA} groups}
The main subjects of interest of the present paper are shift-invariant
\acl{OVK}. When referring to a shift-invariant \ac{OVK}
$K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ we assume that
$\mathcal{X}$ is a locally compact second countable topological group with
identity $e$.
\begin{definition}[Shift-invariant \ac{OVK}]
    A reproducing \acl{OVK}
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ is called
    shift-invariant if for all $x$, $z$, $t\in\mathcal{X}$, $K(x\groupop t,
    z\groupop t) = K(x, z)$.
\end{definition}
A shift-invariant kernel can be characterized by a function of one variable
$K_e$ called the signature of $K$. Here $e$ denotes the neutral element of the
\ac{LCA} group $\mathcal{X}$ endowed with the binary group operation
$\groupop$.
\begin{proposition}[Kernel signature~\citep{Carmeli2010}]
    \label{pr:kernel_signature} Let
    $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ be a
    reproducing kernel. The following conditions are equivalents.
    \begin{enumerate}
        \item \label{pr:kernel_signature_1} $K$ is a positive-definite
        shift-invariant \acl{OVK}.
        \item \label{pr:kernel_signature_2} There
        is a positive-definite function
        $K_e:\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ such that
        $K(x,z)=K_e(\inv{z}\groupop x)$.
    \end{enumerate}
    If one of the above conditions is satisfied, then
    $\norm{K(x,x)}_{\mathcal{Y}}=\norm{K_e(e)}_{\mathcal{Y}}$, $\forall
    x\hiderel{\in}\mathcal{X}$.
\end{proposition}
The notation $K_e$ for the function of completely positive type associated with
the reproducing kernel $K$ is consistent with the definition given by
\cref{eq:trivial_feature_op} since for all $x\in\mathcal{X}$ and all
$y\in\mathcal{Y}$, $(K_ey)(x)=K_e(x)y$.  We recall that if $K$ is a
$\mathcal{Y}$-Mercer kernel, there is a function $K_e$ such that for all $x$
and $z\in\mathcal{X}$, $K(x, z)=K_e(x\groupop z^{-1})$. Then an \acs{OVK} $K$
is $\mathcal{Y}$-Mercer if and only if for all $y\in\mathcal{Y}$,
$K_e(\cdot)y\in\mathcal{C}(\mathcal{X};\mathcal{Y})$. In other words a
$\mathcal{Y}$-Mercer kernel is nothing but a functions whose signature is
continuous and positive definite \citep{Carmeli2010}, which fulfil the
conditions required for the \say{operator-valued} Bochner theorem to apply.
Note that if $K$ is a shift invariant $\mathcal{Y}$-Mercer kernel, then
$\mathcal{H}_K$ contains continuous \emph{bounded} functions
\citep{Carmeli2010}.
\subsection{Some applications of Operator-valued kernels}
We give here a non exhaustive list of works concerning \aclp{OVK}.  A good
review of \aclp{OVK} has been conducted in \citet{Alvarez2012}. For a
theoretical introduction to \acsp{OVK} the interested reader can refer to the
papers \citet{carmeli2006vector, caponnetto2008, Carmeli2010}. Generalization
bounds for \acs{OVK} have been studied in \citet{Sindhwani2013,
kadri2015operator,sangnier2016joint, maurer2016vector}.  Operator-valued Kernel
Regression has first been studied in the context of Ridge Regression and
Multi-task learning by \citet{Micchelli2005}.  Multi-task regression
\citep{micchelli2004kernels}  and structured multi-class classification
\citep{Dinuzzo2011,minh2013unifying,mroueh2012multiclass} are undoubtedly the
first target applications for working in \acl{vv-RKHS}.  \aclp{OVK} have been
shown useful to provide a general framework for structured output prediction
\citep{Brouard2011,Brouard2016_jmlr} with a link to Output Kernel Regression
\citep{Kadri_icml2013}. Beyond structured classification, other various
applications such as link prediction, drug activity prediction or recently
metabolite identification \citep{brouard2016fast} and  image colorization
\citep{ha2010image} have been developed.
\paragraph{}
\citet{Macedo2008, Baldassare2012} showed the interest of spectral algorithms
in Ridge regression and introduced vector field learning as a new multiple
output task in Machine Learning community. \citet{Wahlstrom2013} applied vector
field learning with \acs{OVK}-based Gaussian processes to the reconstruction of
magnetic fields (which are curl-free).  The works of
\citet{Kadri_aistat10,kadri2015operator} have been the precursors of
regression with functional values, opening a new avenue of applications.
Appropriate algorithms devoted to on-line learning have been also derived  by
\citet{audiffren2013online}.  Kernel learning was addressed at least in two
ways: first with using Multiple Kernel Learning in \citet{Kadri_nips2012} and
second, using various penalties, smooth ones in \citet{Dinuzzo2011,
ciliberto2015} for decomposable kernels and non smooth ones in
\citet{lim2015operator} using proximal methods in the case of decomposable and
transformable kernels.  Dynamical modeling was tackled in the context of
multivariate time series modelling in
\citet{lim2013okvar,Sindhwani2013,lim2015operator} and as a generalization of
Recursive Least Square Algorithm in \citet{amblard2015operator}.
\citet{sangnier2016joint} recently explored the minimization of a pinball loss
under regularizing constraints induced by a well chosen decomposable kernel in
order to handle joint quantile regression.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main contribution: Operator Random Fourier Features}
\label{sec:ORFF_construction}
We present in this section a construction methodology devoted to
shift-invariant $\mathcal{Y}$-Mercer operator-valued kernels defined on any
\acf{LCA} group, noted ($\mathcal{X}, \groupop)$, for some operation noted
$\groupop$. This allows us to use the general context of Pontryagin duality for
\acl{FT} of functions on \acs{LCA} groups. Building upon a generalization of
the celebrated Bochner's theorem for operator-valued measures, an
operator-valued kernel is seen as the \emph{\acl{FT}} of an operator-valued
positive measure. From that result, we extend the principle of \acs{RFF} for
scalar-valued kernels and derive a general methodology to build \acf{ORFF} when
operator-valued kernels are shift-invariant according to the chosen group
operation. Elements of this paper have been developped
in~\citet{brault2016random}.
\paragraph{}
We present a construction of feature maps called \acf{ORFF}, such that $f:
x\mapsto \tildePhi{\omega}(x)^\adjoint \theta$ is a continuous function that
maps an arbitrary \acs{LCA} group $\mathcal{X}$ as input space to an arbitrary
output Hilbert space $\mathcal{Y}$. First we define a functional \emph{Fourier
feature map}, and then propose a Monte-Carlo sampling from this feature map to
construct an approximation of a shift-invariant $\mathcal{Y}$-Mercer kernel.
Then, we prove the convergence of the kernel approximation
$\tilde{K}(x,z)=\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(z)$ with high
probability on \emph{compact} subsets of the \acs{LCA} $\mathcal{X}$.
Eventually we conclude with some numerical experiments.
\subsection{Theoretical study}
\label{sec:theoretical_study}
The following proposition of~\citet{Zhang2012, neeb1998operator} extends
Bochner's theorem to any shift-invariant $\mathcal{Y}$-Mercer kernel. We give a
short intoduction to \acs{LCA} groups and abstract harmonic analysis in
\cref{sec:abstract_harmonic}. For the sake of simplicity, the reader can take
$\overline{\pairing{x,\omega}}=\overline{\exp(\iu\inner{x,
\omega}_2})=\exp(-\iu\inner{x,\omega}_2)$, when $x\in\mathcal{X}=(\mathbb{R}^d,
+)$.
\begin{proposition}[Operator-valued Bochner's theorem~\citep{Zhang2012,
neeb1998operator}]
    \label{pr:operator_valued_bochner}
    If a function $K$ from $\mathcal{X} \times \mathcal{X}$ to $\mathcal{Y}$ is
    a shift-invariant $\mathcal{Y}$-Mercer kernel on $\mathcal{X}$, then there
    exists a unique positive projection-valued measure $\dual{Q}:
    \mathcal{B}(\mathcal{X}) \to
    \mathcal{L}_+(\mathcal{Y})$ such that for all $x$, $z \in \mathcal{X}$,
    \begin{dmath}
        \label{eq:bochner-gen}
        K(x, z) = \int_{\dual{\mathcal{X}}} \conj{\pairing{x \groupop \inv{z},
        \omega}} d\dual{Q}(\omega),
    \end{dmath}
    where $\dual{Q}$ belongs to the set of all the projection-valued measures
    of bounded variation on the $\sigma$-algebra of Borel subsets of
    $\dual{\mathcal{X}}$. Conversely, from any positive operator-valued measure
    $\dual{Q}$, a shift-invariant kernel $K$ can be defined by
    \cref{eq:bochner-gen}.
\end{proposition}
Although this theorem is central to the spectral decomposition of
shift-invariant $\mathcal{Y}$-Mercer \acs{OVK}, the following results proved
by~\citet{Carmeli2010} provides insights about this decomposition that are more
relevant in practice. It first gives the necessary conditions to build
shift-invariant $\mathcal{Y}$-Mercer kernel with a pair $(A, \dual{\mu})$ where
$A$ is an operator-valued function on $\dual{\mathcal{X}}$ and $\dual{\mu}$ is
a real-valued positive measure on $\dual{\mathcal{X}}$. Note that obviously
such a pair is not unique and the choice of this paper may have an impact on
theoretical properties as well as practical computations.  Secondly it also
states that any \acs{OVK} have such a spectral decomposition when $\mathcal{Y}$
is finite dimensional or $\mathcal{X}$.
\begin{proposition}[\citet{Carmeli2010}]
    \label{pr:mercer_kernel_bochner}
    Let $\dual{\mu}$ be a positive measure on
    $\mathcal{B}(\mathcal{\dual{\mathcal{X}}})$ and $A: \dual{\mathcal{X}}\to
    \mathcal{L}(\mathcal{Y})$ such that $\inner{A(\cdot)y,y'}\in
    L^1(\mathcal{X},\dual{\mu})$ for all $y,y'\in\mathcal{Y}$ and
    $A(\omega)\succcurlyeq 0$ for $\dual{\mu}$-almost all
    $\omega\in\dual{\mathcal{X}}$. Then, for all $\delta \in \mathcal{X}$,
    \begin{dmath}
        \label{eq:AK0}
        K_e(\delta)=\int_{\dual{\mathcal{X}}} \conj{\pairing{\delta, \omega}}
        A(\omega) d\dual{\mu}(\omega)
    \end{dmath}
    is the kernel signature of a shift-invariant $\mathcal{Y}$-Mercer kernel
    $K$ such that $K(x,z)=K_e(x \groupop \inv{z})$. The \acs{vv-RKHS}
    $\mathcal{H}_K$ is embed in
    $L^2(\dual{\mathcal{X}},\dual{\mu};\mathcal{Y}')$ by means of the feature
    operator
    \begin{dmath}
        \label{eq:feature_operator}
        (Wg)(x)=\int_{\mathcal{\dual{X}}} \conj{\pairing{x,\omega}} B(\omega)
        g(\omega) d\dual{\mu}(\omega),
    \end{dmath}
    Where $B(\omega)B(\omega)^\adjoint=A(\omega)$ and both integrals converge
    in the weak sense. If $\mathcal{Y}$ is finite dimensional or $\mathcal{X}$
    is compact, any shift-invariant kernel is of the above form for some pair
    $(A, \dual{\mu})$.
\end{proposition}
\paragraph{}
When $p=1$ one can always assume $A$ is reduced to the scalar $1$, $\dual{\mu}$
is still a bounded positive measure and we retrieve the Bochner theorem applied
to the scalar case (\cref{th:bochner-scalar}).
\paragraph{}
\Cref{pr:mercer_kernel_bochner} shows that a pair $(A,\dual{\mu})$ entirely
characterizes an \acs{OVK}. Namely a given measure $\dual{\mu}$ and a function
$A$ such that $\inner{y', A(.)y}\in L^1(\mathcal{X},\dual{\mu})$ for all $y$,
$y'\in\mathcal{Y}$ and $A(\omega)\succcurlyeq 0$ for $\dual{\mu}$-almost all
$\omega$, give rise to an \acs{OVK}. Since $(A,\dual{\mu})$ determine a unique
kernel we can write
$\mathcal{H}_{(A,\dual{\mu})}{\scriptstyle\implies}\mathcal{H}_K$ where $K$ is
defined as in \cref{eq:AK0}. However the converse is not true: Given a
$\mathcal{Y}$-Mercer shift invariant \acl{OVK}, there exist infinitely many
pairs $(A,\dual{\mu})$ that characterize an \acs{OVK}.
\paragraph{}
The main difference between \cref{eq:bochner-gen} and
\cref{eq:AK0} is that the first one characterizes an \acs{OVK}
by a unique \acf{POVM}, while the second one shows that the \acs{POVM} that
uniquely characterizes a $\mathcal{Y}$-Mercer \acs{OVK} has an operator-valued
density with respect to a \emph{scalar} measure $\dual{\mu}$; and that this
operator-valued density is not unique.
\paragraph{}
Finally \cref{pr:mercer_kernel_bochner} does not provide any
\emph{constructive} way to obtain the pair $(A,\dual{\mu})$ that characterizes
an \acs{OVK}. The following \cref{subsec:sufficient_conditions} is based on
another proposition of~\citeauthor{carmeli2006vector} and shows that if the
kernel signature $K_e(\delta)$ of an $\acs{OVK}$ is in $L^1$ then it is
possible to construct \emph{explicitly} a pair $(C,\dual{\Haar})$ from it.
Additionally, we show that we can always extract a scalar-valued
\emph{probability} density function from $C$ such that we obtain a pair
$(A,\probability_{\dual{\mu},\rho})$ where $\probability_{\dual{\mu},\rho}$ is
a \emph{probability} distribution absolutely continuous with respect to
$\dual{\mu}$ and with associated \ac{pdf}~$\rho$. Thus for all
$\mathcal{Z}\subset\mathcal{B}(\dual{\mathcal{X}})$,
$\probability_{\dual{\mu},\rho}(\mathcal{Z})=\int_{\mathcal{Z}}
\rho(\omega)d\dual{\mu}(\omega)$.  When the reference measure $\dual{\mu}$ is
the Lebesgue measure, we note
$\probability_{\dual{\mu},\rho}=\probability_\rho$. For any function
$f:\mathcal{X}\times\dual{\mathcal{X}}\times\mathcal{Y}\to\mathbb{R}$, we also
use the notation $\expectation_{\dual{\Haar}, \rho}\left[f(x, \omega, y)\right]
=\expectation_{\omega\sim\probability_{\dual{\Haar},\rho}}\left[f(x, \omega,
y)\right] \hiderel{=}\int_{\dual{\mathcal{X}}} f(x, \omega, y)
d\probability_{\dual{\Haar}, \rho}(\omega) =\int_{\dual{\mathcal{X}}} f(x,
\omega, y)\rho(\omega) d\dual{\Haar}(\omega)$.  where the two last equalities
hold by the \say{law of the unconscious statistician} (change of variable
formula) and the fact that $\probability_{\dual{\Haar}, \rho}$ has density
$\rho$.

\subsubsection{Sufficient conditions of existence}
\label{subsec:sufficient_conditions}
While \cref{pr:mercer_kernel_bochner} gives some insights on how to build an
approximation of a $\mathcal{Y}$-Mercer kernel, we need a theorem that provides
an explicit construction of the pair $(A, \probability_{\dual{\mu},\rho})$ from
the kernel signature $K_e$. Proposition 14 in~\citet{Carmeli2010} gives the
solution, and also provides a sufficient condition for
\cref{pr:mercer_kernel_bochner} to apply.
\begin{proposition}[\citet{Carmeli2010}]
\label{pr:inverse_ovk_Fourier_decomposition}
    Let $K$ be a shift-invariant $\mathcal{Y}$-Mercer kernel of signature
    $K_e$.  Suppose that for all $z \in \mathcal{X}$ and for all $y$, $y'
    \in\mathcal{Y}$, the function $\inner{K_e(.)y,y'}_{\mathcal{Y}}\in
    L^1(\mathcal{X},\Haar)$ where $\mathcal{X}$ is endowed with the group law
    $\groupop$. Denote $C: \dual{X} \to \mathcal{L}(Y)$, the function defined
    for all $\omega \in \dual{\mathcal{X}}$ that satisfies for all $y$, $y'$ in
    $\mathcal{Y}$:
    \begin{dmath}\label{eq:CK0}
        \inner{y',C(\omega)y}_{\mathcal{Y}}= \int_{\mathcal{X}}
        \pairing{\delta, \omega}\inner{y',
        K_e(\delta)y}_{\mathcal{Y}}d\Haar(\delta) \hiderel{=} \IFT{\inner{y',
        K_e(\cdot)y}}_{\mathcal{Y}}(\omega).
    \end{dmath}
    Then
    \begin{enumerate}
        \item $C(\omega)$ is a bounded non-negative operator for all $\omega
        \in \dual{\mathcal{X}}$,
        \item $\inner{y, C(\cdot)y'}_{\mathcal{Y}}\in
        L^1\left(\dual{\mathcal{X}},\dual{\Haar}\right)$ for all
        $y,y'\in\mathcal{X}$,
        \item for all $\delta\in\mathcal{X}$ and for all $y$, $y'$ in
        $\mathcal{Y}$, $\inner{y', K_e(\delta)y}_{\mathcal{Y}}=
        \int_{\dual{\mathcal{X}}}\conj{\pairing{\delta,\omega}}\inner{y',
        C(\omega)y}_{\mathcal{Y}}d\dual{\Haar}(\omega) \hiderel{=}
        \FT{\inner{y', C(\cdot)y}_{\mathcal{Y}}}(\delta)$.
    \end{enumerate}
\end{proposition}
We found some confusion in the literature whether a kernel is the
\acl{FT} or \acl{IFT} of a measure. However \cref{lm:C_characterization}
clarifies the relation between the \acl{FT} and \acl{IFT} for a translation
invariant \acl{OVK}. Notice that in the real scalar case the \acl{FT} and
\acl{IFT} of a shift-invariant kernel are the same, while the difference is
significant for \acs{OVK}.  The following lemma is a direct consequence of the
definition of $C(\omega)$ as the \acl{FT} of the adjoint of $K_e$ and also
helps to simplify the definition of \acs{ORFF}.
\begin{lemma}
    \label{lm:C_characterization}
    Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer kernel
    such that for all $y$, $y'\in\mathcal{Y}$, $\inner{y',
    K_e(\cdot)y}_{\mathcal{Y}}\in L^1(\mathcal{X},\Haar)$ and let $\inner{y',
    C(\cdot)y}_{\mathcal{Y}}=\IFT{\inner{y', K_e(\cdot)y}_{\mathcal{Y}}}$.
    Then
    \begin{enumerate}
        \item \label{lm:C_characterization_1} $C(\omega)$ is self-adjoint and
        $C$ is even.
        \item \label{lm:C_characterization_2} $\IFT{\inner{y',
        K_e(\cdot)y}_{\mathcal{Y}}} = \FT{\inner{y',
        K_e(\cdot)y}_{\mathcal{Y}}}$.
        \item \label{lm:C_characterization_3} $K_e(\delta)$ is self-adjoint and
        $K_e$ is even.
    \end{enumerate}
\end{lemma}
While \cref{pr:inverse_ovk_Fourier_decomposition} gives an explicit form of the
operator $C(\omega)$ defined as the \acl{FT} of the kernel $K$, it is not
really convenient to work with the Haar measure $\dual{\Haar}$ on
$\mathcal{B}(\dual{\mathcal{X}})$. However it is easily possible to turn
$\dual{\Haar}$ into a probability measure to allow efficient integration over
an infinite domain.
\paragraph{}
The following proposition allows to build a spectral decomposition of a
shift-invariant $\mathcal{Y}$-Mercer kernel on a \acs{LCA} group $\mathcal{X}$
endowed with the group law $\groupop$ with respect to a scalar probability
measure, by extracting a scalar probability density function from $C$.
\begin{proposition}[Shift-invariant $\mathcal{Y}$-Mercer kernel spectral
decomposition]
\label{pr:spectral}
    Let $K_e$ be the signature of a shift-invariant $\mathcal{Y}$-Mercer
    kernel. If for all $y$, $y' \in\mathcal{Y}$, $\inner{K_e(.)y,y'}\in
    L^1(\mathcal{X},\Haar)$ then there exists a positive probability measure
    $\probability_{\dual{\Haar},\rho}$ and an operator-valued function $A$ an
    such that for all $y,$ $y'\in\mathcal{Y}$,
    \begin{dmath}
        \label{eq:expectation_spec} \inner{y', K_e(\delta)y}
        =\expectation_{\dual{\Haar},\rho}\left[\conj{\pairing{\delta,
        \omega}}\inner{y', A(\omega)y}\right],
    \end{dmath}
    with $\inner{y', A(\omega)y}\rho(\omega) = \FT{\inner{y',
    K_e(\cdot)y}}(\omega)$.  Moreover
    \begin{enumerate}
        \item for all $y,$ $y'\in\mathcal{Y}$, $\inner{A(.)y,y'}\in
        L^1(\dual{\mathcal{X}}, \probability_{\dual{\Haar},\rho})$,
        \item $A(\omega)$ is non-negative for
        $\probability_{\dual{\Haar},\rho}$-almost all
        $\omega\in\dual{\mathcal{X}}$,
        \item $A(\cdot)$ and $\rho(\cdot)$ are even functions.
    \end{enumerate}
\end{proposition}
\subsection{Examples of spectral decomposition}
\label{subsec:dec_examples}
In this section we give examples of spectral decomposition for various
$\mathcal{Y}$-Mercer kernels, based on \cref{pr:spectral}.

\subsubsection{Gaussian decomposable kernel}
\label{par:gaussian_dec} Recall that a decomposable $\mathbb{R}^p$-Mercer
introduced in \cref{dec-kernel} has the form $K(x,z)=k(x,z)\Gamma$, where
$k(x,z)$ is a scalar Mercer kernel and $\Gamma\in\mathcal{L}(\mathbb{R}^p)$ is
a non-negative operator. Let us focus on
$K^{dec,gauss}_e(\cdot)=k_e^{gauss}(\cdot)\Gamma$, the Gaussian decomposable
kernel where $K_e^{dec, gauss}$ and $k_e^{gauss}$ are respectively the
signature of $K$ and $k$ on the additive group $\mathcal{X}=(\mathbb{R}^d,+)$
-- $\acs{ie}~\delta=x-z$ and $e=0$. The well known Gaussian kernel is defined
for all $\delta\in\mathbb{R}^d$ as follows
$k^{\text{gauss}}_0(\delta)\hiderel{=}\exp\left(
-\sigma^{-2}\norm{\delta}^2_2\right)/2$ where $\sigma \in \mathbb{R}_{>0}$ is
an hyperparameter corresponding to the bandwith of the kernel. The
--Pontryagin-- dual group of $\mathcal{X}=(\mathbb{R}^d,+)$ is
$\dual{\mathcal{X}}\cong(\mathbb{R}^d,+)$ with the pairing
$\pairing{\delta,\omega}=\exp\left(\iu\inner{\delta,\omega}\right)$ where
$\delta$ and $\omega\in\mathbb{R}^d$. In this case the Haar measures on
$\mathcal{X}$ and $\dual{\mathcal{X}}$ are in both cases the Lebesgue measure.
However in order to have the property that $\IFT{\FT{f}}=f$ and
$\IFT{f}=\mathcal{R}\FT{f}$ one must normalize both measures by
$\sqrt{2\pi}^{-d}$, \acs{ie}~for all
$\mathcal{Z}\in\mathcal{B}\left(\mathbb{R}^d\right)$,
$\sqrt{2\pi}^{d}\Haar(\mathcal{Z}) = \Leb(\mathcal{Z})$ and
$\sqrt{2\pi}^{d}\dual{\Haar}(\mathcal{Z}) = \Leb(\mathcal{Z})$.  Then the
\acl{FT} on $(\mathbb{R}^d,+)$ is
\begin{dmath*}
    \FT{f}(\omega)
    =\int_{\mathbb{R}^d} \exp\left(-\iu\inner{\delta, \omega}\right) f(\delta)
    d\Haar(\delta)
    \hiderel{=}\int_{\mathbb{R}^d} \exp\left(-\iu\inner{\delta, \omega}\right)
    f(\delta) \frac{d\Leb(\delta)}{\sqrt{2\pi}^d}.
\end{dmath*}
Since $k^{\text{gauss}}_0\in L^1$ and $\Gamma$ is bounded, it is possible to
apply \cref{pr:spectral}, and obtain for all $y$ and $y'\in\mathcal{Y}$,
\begin{dmath*}
    \inner*{y',C^{dec,gauss}(\omega)y}
    =\FT{\inner*{y',K^{dec,gauss}_0(\cdot)y}}(\omega)
    \hiderel{=}\FT{k_0^{gauss}}(\omega)\inner*{y', \Gamma y}.
\end{dmath*}
Thus
\begin{dmath*}
    C^{dec,gauss}(\omega)
    =\int_{\mathbb{R}^d}\exp\left(-\iu\inner{\omega, \delta} -
    \frac{\norm{\delta}^2_2}{2\sigma^2}\right)
    \frac{d\Leb(\delta)}{\sqrt{2\pi}^d} \Gamma.
\end{dmath*}
Hence
\begin{dmath*}
    C^{dec,gauss}(\omega)
    =\underbrace{\frac{1}{\sqrt{2\pi\frac{1}{\sigma^2}}^d}\exp\left(
    -\frac{\sigma^2}{2}\norm{\omega}^2_2\right)\sqrt{2\pi}^d}_{\rho(\cdot)
    =\mathcal{N}(0,\sigma^{-2}I_d)\sqrt{2\pi}^d}\underbrace{\Gamma}_{A(\cdot)
    =\Gamma}.
\end{dmath*}
Therefore the canonical decomposition of $C^{dec,gauss}$ is
$A^{dec,gauss}(\omega)=\Gamma$ and
$\rho^{dec,gauss}=\mathcal{N}(0,\sigma^{-2}I_d)\sqrt{2\pi}^d$, where
$\mathcal{N}$ is the Gaussian probability distribution. Note that this
decomposition is done with respect to the \emph{normalized} Lebesgue measure
$\dual{\Haar}$, meaning that for all
$\mathcal{Z}\in\mathcal{B}(\dual{\mathcal{X}})$,
\begin{dmath*}
    \probability_{\dual{\Haar}, \mathcal{N}(0, \sigma^{-2} I_d)
    \sqrt{2\pi}^d}(\mathcal{Z})
    =\int_{\mathcal{Z}} \mathcal{N}(0, \sigma^{-2} I_d) \sqrt{2\pi}^d
    d\dual{\Haar}(\omega)
    =\int_{\dual{\mathcal{X}}}\mathcal{N}(0,\sigma^{-2}I_d)d\Leb(\omega)
    \hiderel{=}\probability_{\mathcal{N}(0,\sigma^{-2}I_d)}(\mathcal{Z}).
\end{dmath*}
Thus, the same decomposition with respect to the usual --non-normalized--
Lebesgue measure $\Leb$ yields
\begin{dgroup}
    \begin{dmath}
        A^{dec,gauss}(\cdot)=\Gamma
    \end{dmath}
    \begin{dmath}
        \rho^{dec,gauss}=\mathcal{N}(0,\sigma^{-2}I_d).
    \end{dmath}
\end{dgroup}
\subsubsection{Skewed-$\chi^2$ decomposable kernel}
\label{subsubsec:skewedchi2} The skewed-$\chi^2$ scalar kernel
\citep{li2010random}, useful for image processing, is defined on the \acs{LCA}
group $\mathcal{X}=(-c_k;+\infty)_{k=1}^d$, with $c_k\in\mathbb{R}_{>0}$ and
endowed with the group operation $\odot$. Let $(e_k)_{k=1}^d$ be the standard
basis of $\mathcal{X}$. The operator $\odot:
\mathcal{X}\times\mathcal{X}\to\mathcal{X}$ is defined by $x\odot z =
\left((x_k + c_k)(z_k + c_k) - c_k\right)_{k=1}^d$.  The identity element $e$
is $\left(1-c_k\right)_{k=1}^d$ since $(1-c) \odot x = x$. Thus the inverse
element $x^{-1}$ is $((x_k+c_k)^{-1} - c_k)_{k=1}^d$. The skewed-$\chi^2$
scalar kernel reads
\begin{dmath}
    k^{skewed}_{1-c}(\delta)
    =\prod_{k=1}^d\frac{2}{\sqrt{\delta_k+c_k}+\sqrt{\frac{1}{\delta_k+c_k}}}.
\end{dmath}
The dual of $\mathcal{X}$ is $\dual{\mathcal{X}}\cong\mathbb{R}^d$ with the
pairing $\pairing{\delta,\omega}
=\prod_{k=1}^d\exp\left(\iu\log(\delta_k+c_k)\omega_k\right)$.  The Haar
measure are defined for all $\mathcal{Z}\in\mathcal{B}((-c;+\infty)^d)$ and all
$\dual{\mathcal{Z}}\in\mathcal{B}(\mathbb{R}^d)$ by
$\sqrt{2\pi}^d\Haar(\mathcal{Z})
=\int_{\mathcal{Z}}\prod_{k=1}^d\frac{1}{z_k+c_k}d\Leb(z)$ and
$\sqrt{2\pi}^d\dual{\Haar}(\dual{\mathcal{Z}})=\Leb(\dual{\mathcal{Z}})$. Thus
the \acl{FT} is
\begin{dmath*}
    \FT{f}(\omega)
    =\int_{(-c;+\infty)^d}\prod_{k=1}^d\frac{\exp\left(-\iu
    \log(\delta_k+c_k)\omega_k\right)}{\delta_k +
    c_k}f(\delta)\frac{d\Leb(\delta)}{\sqrt{2\pi}^d}.
\end{dmath*}
Then, applying Fubini's theorem over product space, and the fact that each
dimension is independent
\begin{dmath*}
    \FT{k_0^{skewed}}(\omega)
    =\prod_{k=1}^d\int_{-c_k}^{+\infty}\frac{2\exp\left(-\iu
    \log(\delta_k+c_k)\omega_k\right)}{(\delta_k +
    c_k)\left(\sqrt{\delta_k + c_k} + \sqrt{\frac{1}{\delta_k + c_k}}\right)}
    \frac{d\Leb(\delta_k)}{\sqrt{2\pi}^d}.
\end{dmath*}
Making the change of variable $t_k=(\delta_k+c_k)^{-1}$ yields
\begin{dmath*}
    \FT{k_0^{skewed}}(\omega)
    = \prod_{k=1}^d\int_{-\infty}^{+\infty} \frac{2 \exp\left(-\iu t_k
    \omega_k\right)}{\exp\left(\frac{1}{2} t_k \right) + \exp\left(-\frac{1}{2}
    t_k \right)} \frac{d\Leb(t_k)}{\sqrt{2\pi}^d}
    \hiderel{=}\sqrt{2\pi}^d\prod_{k=1}^d\sech(\pi\omega_k).
\end{dmath*}
Since $k^{\text{skewed}}_{1-c}\in L^1$ and $\Gamma$ is bounded, it is possible
to apply \cref{pr:spectral}, and obtain
\begin{dmath*}
    C^{dec,skewed}(\omega)
    =\FT{k_{1-c}^{skewed}}(\omega)\Gamma
    \hiderel{=}\underbrace{\sqrt{2\pi}^d\prod_{k=1}^d\sech(\pi
    \omega_k)}_{\rho(\cdot) = \mathcal{S}(0, 2^{-1})^d\sqrt{2
    \pi}^d}\underbrace{\Gamma}_{A(\cdot)}.
\end{dmath*}
Hence the decomposition with respect to the usual --non-normalized-- Lebesgue
measure $\Leb$ yields
\begin{dgroup}
    \begin{dmath}
        A^{dec,skewed}(\cdot)=\Gamma
    \end{dmath}
    \begin{dmath}
        \rho^{dec,skewed}=\mathcal{S}\left(0,2^{-1}\right)^d.
    \end{dmath}
\end{dgroup}
\subsubsection{Curl-free Gaussian kernel}
The curl-free Gaussian kernel is defined as
$K^{curl,gauss}_0=-\nabla\nabla^\transpose k_0^{gauss}$. Here
$\mathcal{X}=(\mathbb{R}^d, +)$ so the setting is the same than
\cref{par:gaussian_dec}.
\begin{dmath*}
    C^{curl,gauss}(\omega)_{ij}=
    \FT{K^{curl,gauss}_{1-c}(\cdot)_{ij}}(\omega)
    %=\FT{-\frac{d^2}{d\delta_id\delta_j}k^{gauss}_0}(\omega)
    %=-(\iu\omega_i)(\iu\omega_j)\FT{k_0^{gauss}}(\omega)
    %=\omega_i\omega_j\FT{k_0^{gauss}}(\omega)
    =\sqrt{2\pi\frac{1}{\sigma^2}}^d\exp\left(
    -\frac{\sigma^2}{2}\norm{\omega}^2_2\right)\sqrt{2\pi}^d\omega_i\omega_j.
\end{dmath*}
Hence
\begin{dmath*}
    C^{curl,gauss}(\omega)
    =\underbrace{\frac{1}{\sqrt{2\pi\frac{1}{\sigma^2}}^d}\exp\left(
    -\frac{\sigma^2}{2} \norm{\omega}^2_2\right) \sqrt{2\pi}^d}_{\mu(\cdot)
    =\mathcal{N}(0 ,\sigma^{-2} I_d)\sqrt{2 \pi}^d}
    \underbrace{\omega\omega^\transpose
    }_{A(\omega)=\omega\omega^\transpose }.
\end{dmath*}
Here a canonical decomposition is
$A^{curl,gauss}(\omega)=\omega\omega^\transpose $ for all
$\omega\in\mathbb{R}^d$ and
$\mu^{curl,gauss}=\mathcal{N}(0,\sigma^{-2}I_d)\sqrt{2\pi}^d$ with respect to
the normalized Lebesgue measure $d\omega$. Again the decomposition with respect
to the usual --non-normalized-- Lebesgue measure is for all
$\omega\in\mathbb{R}^d$
\begin{dgroup}
    \begin{dmath}
        A^{curl,gauss}(\omega)=\omega\omega^\transpose
    \end{dmath}
    \begin{dmath}
        \mu^{curl,gauss}=\mathcal{N}(0,\sigma^{-2}I_d).
    \end{dmath}
\end{dgroup}
\subsubsection{Divergence-free kernel}
The divergence-free Gaussian kernel is defined as
$K^{div,gauss}_0=(\nabla\nabla^\transpose -\Delta)k_0^{gauss}$ on the group
$\mathcal{X}=(\mathbb{R}^d, +)$. The setting is the same than
\cref{par:gaussian_dec}. Hence
\begin{dmath*}
    C^{div,gauss}(\omega)_{ij}
    = \FT{K^{div,gauss}_0(\cdot)_{ij}}(\omega)
    %= \FT{\frac{\partial^2}{\partial \delta_i \partial \delta_j}k^{gauss}_0 -
    %\delta_{i=j} \sum_{k=1}^d \frac{\partial^2}{\partial \delta_k
    %\partial\delta_k} k^{gauss}_0}(\omega)
    %= \left(-(\iu \omega_i)(\iu \omega_j) - \delta_{i=j}\sum_{k=1}^d(\iu
    %\omega_k)^2\right) \FT{k_0^{gauss}}
    \hiderel{=}\left(\delta_{i=j}\sum_{k=1}^d \omega_k^2 - \omega_i
    \omega_j\right) \FT{k_0^{gauss}}(\omega).
\end{dmath*}
Hence
\begin{dmath*}
    C^{div,gauss}(\omega)
    = \underbrace{\frac{1}{\sqrt{2\pi\frac{1}{\sigma^2}}^d}\exp\left(
    -\frac{\sigma^2}{2} \norm{\omega}^2_2\right) \sqrt{2\pi}^d}_{\rho(\cdot) =
    \mathcal{N}(0, \sigma^{-2} I_d)\sqrt{2 \pi}^d}\underbrace{\left(I_d
    \norm{\omega}_2^2 - \omega \omega^\transpose
    \right)}_{A(\omega)=I_d\norm{\omega}_2^2-\omega\omega^\transpose }.
\end{dmath*}
Thus the canonical decomposition with respect to the normalized Lebesgue
measure is $A^{div,gauss}(\omega)=I_d\norm{\omega}_2^2-\omega\omega^\transpose
$ and the measure
    $\rho^{div,gauss}=\mathcal{N}(0,\sigma^{-2}I_d)\sqrt{2\pi}^d$.
The canonical decomposition with respect to the usual Lebesgue measure is
\begin{dgroup}
    \begin{dmath}
        A^{div,gauss}(\omega)=I_d\norm{\omega}_2^2-\omega\omega^\transpose
    \end{dmath}
    \begin{dmath}
        \rho^{div,gauss}=\mathcal{N}(0,\sigma^{-2}I_d).
    \end{dmath}
\end{dgroup}

\subsection{Operator-valued Random Fourier Features (ORFF)}
\label{sec:building_ORFF}
\subsubsection{Building Operator-valued Random Fourier Features}
As shown in \cref{pr:spectral} it is
always possible to find a pair $(A, \probability_{\dual{\Haar},\rho})$ from a
shift invariant $\mathcal{Y}$-Mercer \acl{OVK} $K_e$ such that
$\probability_{\dual{\Haar},\rho}$ is a probability measure, \acs{ie}
$\int_{\dual{\mathcal{X}}} \rho d\dual{\Haar}=1$ where $\rho$ is the density of
$\probability_{\dual{\Haar},\rho}$ and
$K_e(\delta)=\expectation_{\dual{\Haar},
\rho}{\conj{\pairing{\delta,\omega}}A(\omega)}$. In order to obtain an
approximation of $K$ from a decomposition $(A,
\probability_{\dual{\Haar},\rho})$ we turn our attention to a Monte-Carlo
estimation of the expectation in \cref{eq:expectation_spec} characterizing a
$\mathcal{Y}$-Mercer shift-invariant \acl{OVK}. % \paragraph{}
%  \probability_{\dual{\Haar},\rho}
\begin{proposition}
    \label{cr:ORFF-kernel} Let $K(x,z)$ be a shift-invariant
    $\mathcal{Y}$-Mercer kernel with signature $K_e$ such that for all $y$,
    $y'\in\mathcal{Y}$, $\inner{y', K_e(\cdot)y}\in L^1(\mathcal{X},\Haar)$.
    Then one can find a pair $(A, \probability_{\dual{\Haar},\rho})$ that
    satisfies \cref{pr:spectral}. \acs{ie} for
    $\probability_{\dual{\Haar},\rho}$-almost all $\omega$, and all $y,
    y'\in\mathcal{Y}$, $\inner{y, A(\omega)y'}\rho(\omega)=\FT{\inner{y',
    K_e(\cdot)y}}(\omega)$.  If $(\omega_j)_{j=1}^D$ be a sequence of
    $D\in\mathbb{N}^*$ \acs{iid}~random variables following the law
    $\probability_{\dual{\Haar},\rho}$ then the operator-valued function
    $\tilde{K}$ defined for $(x,z) \in \mathcal{X}\times \mathcal{X}$ as
    \begin{dmath*}
        \tilde{K}(x,z)= \frac{1}{D} \sum_{j=1}^D
        \conj{\pairing{x\groupop\inv{z},\omega_j}} A(\omega_j)
    \end{dmath*}
    is an approximation of $K$. \acs{ie} it satisfies for all $x$,
    $z\in\mathcal{X}$, $\tilde{K}(x, z) \converges{\acs{asurely}}{D\to\infty}
    K(x, z)$ in the weak operator topology, where $K$ is a $\mathcal{Y}$-Mercer
    \acs{OVK}.
\end{proposition}
Now, for efficient computations as motivated in the introduction, we are
interested in finding an approximated \emph{feature map} instead of a kernel
approximation. Indeed, an approximated feature map will allow to build linear
models in regression tasks. The following proposition deals with the feature
map construction.
\begin{proposition}
    \label{cr:ORFF-map-kernel} Assume the same conditions as
    \cref{cr:ORFF-kernel}. Moreover, if one can define $B: \dual{\mathcal{X}}
    \to \mathcal{L}(\mathcal{Y}',\mathcal{Y})$ such that for
    %such that for all
    %$y\in\mathcal{Y}$ and all $y' \in\mathcal{Y}'$,
    %%$\inner{y, B(\cdot)y'} \in
    %L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar},\rho})$,  and for
    $\probability_{\dual{\Haar},\rho}$-almost all $\omega$, and all $y,
    y'\in\mathcal{Y}$, $\inner{y, B(\omega)B(\omega)^\adjoint y'}\rho(\omega)=
    \inner{y, A(\omega)y'}\rho(\omega) \hiderel{=} \FT{\inner{y,
    K_e(\cdot)y'}}(\omega)$, then the function
    $\tildePhi{\omega}:\dual{\mathcal{X}} \to \mathcal{L}(\mathcal{Y},
    \Vect_{j=1}^D \mathcal{Y'})$ defined for all $y \in \mathcal{Y}$ as
    follows:
    \begin{dmath*}
        \tildePhi{\omega}(x)y
        = \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
        \omega_j}B(\omega_j)^\adjoint y \condition{$\omega_j \sim
        \probability_{\dual{\Haar},\rho}$ \ac{iid},}
    \end{dmath*}
    is an approximated feature map\footnote{\acs{ie}~it satisfies
    for all $x,z\in\mathcal{X}$, $\tildePhi{\omega}(x)^\adjoint
    \tildePhi{\omega}(z)\converges{\acs{asurely}}{D\to\infty}K(x,z)$ in the
    weak operator topology, where $K$ is a $\mathcal{Y}$-Mercer \acs{OVK}.} for
    the kernel $K$.
\end{proposition}
\begin{remark}
    We find a decomposition such that
    $A(\omega_j)=B(\omega_j)B(\omega_j)^\adjoint $ for all $j\in\mathbb{N}^*_D$
    either by exhibiting a closed-form or using a numerical decomposition.
    Such a decomposition always exists since $A(\omega)$ is positive
    semi-definite for all $\omega\in\dual{\mathcal{X}}$.
\end{remark}
Notice that an \acs{ORFF} map as defined in \cref{cr:ORFF-map-kernel} is also
the Monte-Carlo sampling of the corresponding functional Fourier feature map
$\Phi_x: \mathcal{Y} \to L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar},
\rho}; \mathcal{Y}' )$ as defined in \cref{pr:fourier_feature_map}.  Indeed,
for all $y\in\mathcal{Y}$ and all $x\in\mathcal{X}$, $\tildePhi{\omega}(x)y =
\Vect_{j=1}^D (\Phi_x y)(\omega_j) \condition{$\omega_j \sim
\probability_{\dual{\Haar}, \rho}$ \acs{iid}}$ \Cref{cr:ORFF-map-kernel} allows
us to define \cref{alg:ORFF_construction} for constructing \acs{ORFF} from an
operator valued kernel.
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\begin{center}
    \begin{algorithm2e}[t!]\label{alg:ORFF_construction}
        \SetAlgoLined
        \Input{$K(x, z)=K_e(\delta)$ a shift-invariant $\mathcal{Y}$-Mercer
        kernel on $(\mathcal{X}, \groupop)$ such that $\forall
        y,y'\in\mathcal{Y},$ $\inner{y', K_e(\cdot)y}\in L^1(\mathbb{R}^d,
        \Haar)$ and $D$ the number of features.}
        \Output{A random feature $\tildePhi{\omega}(x)$ such that
        $\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(z) \approx K(x,z)$}
        \BlankLine
        Define the pairing $\pairing{x, \omega}$ from the \acs{LCA} group
        $(\mathcal{X}, \groupop)$\;
        Find a decomposition $(A,\probability_{\dual{\Haar},\rho})$ and $B$
        such that $B(\omega)B(\omega)^\adjoint \rho(\omega)=
        A(\omega)\rho(\omega)\hiderel{=}\IFT{K_e}(\omega)$\;
        \nl Draw $D$ \acs{iid} realizations $(\omega_j)_{j=1}^D$ from the
        probability distribution $\probability_{\dual{\Haar},\rho}$\;
        \nl \Return
        $\begin{cases}
            \tildePhi{\omega}(x) \in \mathcal{L}(\mathcal{Y}, \tildeH{\omega})
            &: y \mapsto \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
            \omega_j}B(\omega_j)^\adjoint y \\
            \tildePhi{\omega}(x)^\adjoint \in\mathcal{L}(\tildeH{\omega},
            \mathcal{Y}) &: \theta \mapsto \frac{1}{\sqrt{D}} \sum_{j=1}^D
            \pairing{x, \omega_j}B(\omega_j)\theta_j
        \end{cases}$\;
        \caption{Construction of \acs{ORFF} from \acs{OVK}}
    \end{algorithm2e}
\end{center}
\paragraph{}
We give a numerical illustration of different $\tilde{K}$ built from different
\acs{iid} realization $(\omega_j)_{j=1}^D$,
$\omega_j\sim\probability_{\dual{\Haar},\rho}$. In \cref{fig:not_Mercer}, we
represent the approximation of  a reference function (black line) defined as
$(y_1, y_2)^\transpose = f(x_i)=\sum_{j=1}^{250}\mathbf{K}_{ij}u_j$ where
$u_j\sim\mathcal{N}(0,I_2)$ and $K$ is a Gaussian decomposable kernel.  We took
$\Gamma=.5 I_2 +.5 1_2$ such that the outputs $y_1$ and $y_2$ share some
similarities.  We generated $250$ points equally separated on the segment
$(-1;1)$. The Gram matrix is then $\mathbf{K}_{ij}=\exp\left(-\frac{(x_i -
x_j)^2}{2(0.1)^2}\right)\Gamma \condition{for $i$, $j\in\mathbb{N}^*_{250}$.}$
We took $\Gamma=.5 I_2 +.5 1_2$ such that the outputs $y_1$ and $y_2$ share
some similarities.  We generated $250$ points equally separated on the segment
$(-1;1)$.
\begin{pycode}[not_mercer]
sys.path.append('./src/')
import not_mercer

not_mercer.main()
\end{pycode}
\begin{figure}[t]
    \pyc{print(r'\centering\resizebox{.85\textwidth}{!}{\input{./not_Mercer.pgf}}')}
    \caption[Approximation of a function in a vv-RKHS using different
    realizations of Operator Random Fourier Feature]{Approximation of a
    function in a VV-RKHS using different realizations of Operator Random
    Fourier Feature.
    Top row and bottom row correspond to two different realizations of
    $\tildeK{\omega}$, which are \emph{different} \acl{OVK}. However when $D$
    tends to infinity, the different realizations of $\tildeK{\omega}$ yield
    the same \acs{OVK}.}
    \label{fig:not_Mercer}
\end{figure}
Then we computed an approximate kernel matrix $\tilde{\mathbf{K}}\approx
\mathbf{K}$ for $25$ increasing values of $D$ ranging from $1$ to $10^4$. The
two graphs in \cref{fig:not_Mercer} on the top row shows that the more the
number of features increases the closer the model
$\widetilde{f}(x_i)=\sum_{j=1}^{250}\tilde{\mathbf{K}}_{ij}u_j$ is to $f$.
The bottom row shows the same experiment but for a different realization of
$\tilde{\mathbf{K}}$. When $D$ is small the curves of the bottom and top rows
are very dissimilar --and sine wave like-- while they both converge to $f$ when
$D$ increase.  We introduce a \emph{functional} feature map, we call
\emph{Fourier Feature map}, defined by the following proposition as a direct
consequence of \cref{pr:mercer_kernel_bochner}.
\begin{proposition}[Functional Fourier feature map]
    \label{pr:fourier_feature_map} Let $\mathcal{Y}$ and $\mathcal{Y}'$ be two
    Hilbert spaces. If there exists an operator-valued function
    $B:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y},\mathcal{Y}')$ such that
    for all $y$, $y'\in\mathcal{Y}$, $\inner{y, B(\omega)B(\omega)^\adjoint
    y'}_{\mathcal{Y}} =\inner{y', A(\omega)y}_{\mathcal{Y}}$
    $\dual{\mu}$-almost everywhere and $\inner{y', A(\cdot)y}\in
    L^1(\dual{\mathcal{X}},\dual{\mu})$ then the operator $\Phi_x$ defined for
    all $y$ in $\mathcal{Y}$ by $(\Phi_x
    y)(\omega)=\pairing{x,\omega}B(\omega)^\adjoint y$, is \emph{a feature
    map}\footnote{\acs{ie}~it satisfies for all $x$, $z \in \mathcal{X}$,
    $\Phi_x^\adjoint \Phi_z=K(x,z)$ where $K$ is a $\mathcal{Y}$-Mercer
    \acs{OVK}.} of some shift-invariant $\mathcal{Y}$-Mercer kernel $K$.
\end{proposition}
With this notation we have $\Phi: \mathcal{X} \to \mathcal{L}(\mathcal{Y};
L^2(\dual{\mathcal{X}}, \dual{\mu}; \mathcal{Y}'))$ such that $\Phi_x\in
\mathcal{L}(\mathcal{Y}; L^2(\dual{\mathcal{X}}, \dual{\mu}; \mathcal{Y}'))$
where $\Phi_x\colonequals\Phi(x)$.
\begin{figure}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \input{./gfx/feature_relationship.tikz}}
    \caption[Relationships between feature-maps.]{Relationships between
    feature-maps. For any realization of
    $\omega_j\sim\probability_{\dual{\Haar},\rho}$ \ac{iid},
    $\tildeH{\omega} = \Vect_{j=1}^D \mathcal{Y}'$.}
    \label{fig:rel_features}
\end{figure}
Notice that an \acs{ORFF} map as defined in \cref{cr:ORFF-map-kernel} is also
the Monte-Carlo sampling of the corresponding functional Fourier feature map
$\Phi_x: \mathcal{Y} \to L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar},
\rho}; \mathcal{Y}' )$ as defined in \cref{pr:fourier_feature_map}.  Indeed,
for all $y\in\mathcal{Y}$ and all $x\in\mathcal{X}$,
\begin{dmath*}
    \tildePhi{\omega}(x)y = \Vect_{j=1}^D (\Phi_x y)(\omega_j)
    \condition{$\omega_j \sim \probability_{\dual{\Haar}, \rho}$ \acs{iid}}
\end{dmath*}

\subsection{From Operator Random Fourier Feature maps to OVKs}
It is also interesting to notice that we can go the other way and define from
the general form of an \acl{ORFF}, an operator-valued kernel.
\begin{proposition}[Operator Random Fourier Feature map]
    \label{pr:ORFF-map} Let $\mathcal{Y}$ and $\mathcal{Y}'$ be two Hilbert
    spaces. If one defines an operator-valued function on the dual of a LCA
    group $\mathcal{X}$, $B: \dual{\mathcal{X}} \to
    \mathcal{L}(\mathcal{Y},\mathcal{Y}')$, and a probability measure
    $\probability_{\dual{\Haar},\rho}$ on $\mathcal{B}(\dual{\mathcal{X}})$,
    such that for all $y\in\mathcal{Y}$ and all $y'\in\mathcal{Y}'$, $\inner{y,
    B(\cdot)y'} \in L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar},\rho})$,
    then the operator-valued function $\tildePhi{\omega}: \mathcal{X}
    \hiderel{\to} \mathcal{L}\left(\mathcal{Y},
    \vect_{j=1}^D\mathcal{Y}'\right)$ defined for all $x \in \mathcal{X}$ and
    for all $y\in\mathcal{Y}$ by
    \begin{dmath}
        \label{eq:phitilde}
        \tildePhi{\omega}(x)y
        = \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
        \omega_j}B(\omega_j)^\adjoint y\condition{$\omega_j \sim
        \probability_{\dual{\Haar, \rho}}$, \acs{iid}, }
    \end{dmath}
    is an approximated feature map of some $\mathcal{Y}$-Mercer operator-valued
    kernel\footnote{\acs{ie}~it satisfies $\tildePhi{\omega}(x)^\adjoint
    \tildePhi{\omega}(z)\converges{\acs{asurely}}{D\to\infty}K(x,z)$ in the
    weak operator topology, where $K$ is a $\mathcal{Y}$-Mercer \acs{OVK}}.
\end{proposition}
The difference between \cref{pr:ORFF-map} and \cref{cr:ORFF-map-kernel} is that
in \cref{pr:ORFF-map} we do not assume that $A(\omega)$ and
$\probability_{\dual{Haar}, \rho}$ have been obtained from \cref{pr:spectral}.
We conclude by showing that any realization of an approximate feature map gives
a proper operator valued kernel. Hence we can always view $\tilde{K}(x,
z)=\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(z)$ ---where
$\tildePhi{\omega}$ is defined as in \cref{cr:ORFF-kernel} (construction from
an \acs{OVK}) or \cref{pr:ORFF-map}--- as a $\mathcal{Y}$-Mercer and thus apply
all the classic results of the \acl{OVK} theory on $\tilde{K}$.
\begin{proposition}
    \label{pr:orff_defines_kernel} Let $\seq{\omega}\in\dual{\mathcal{X}}^D$.
    If for all $y$, $y'\in\mathcal{Y}$ $\inner{y',
    \tildeK{\omega}_e\left(x\groupop z^{-1}\right)y}_{\mathcal{Y}}
    =\inner{\tildePhi{\omega}(x)y', \tildePhi{\omega}(z)y}_{\tildeH{\omega}}
    =\inner*{y', \frac{1}{D}\sum_{j=1}^D \conj{\pairing{x\groupop
    z^{-1},\omega_j}}B(\omega_j)B(\omega_j)^*y}_{\mathcal{Y}}$, for all $x$,
    $z\in\mathcal{X}$, then $\tildeK{\omega}$ is a shift-invariant
    $\mathcal{Y}$-Mercer \acl{OVK}.
\end{proposition}
Note that the above theorem does not consider the $\omega_j$'s as random
variables and therefore does not shows the convergence of the kernel
$\widetilde{K}$ to some target kernel $K$. However is shows that any
realization of $\widetilde{K}$ when $\omega_j$'s  are random variables yields
a valid $\mathcal{Y}$-Mercer operator-valued kernel.
Note that the above theorem does not considers the $\omega_j$'s as random
variables and therefore does not shows the convergence of the kernel
$\widetilde{K}$ to some target kernel $K$. However is shows that any
realization of $\widetilde{K}$ when $\omega_j$'s  are random variables yields
a valid $\mathcal{Y}$-Mercer operator-valued kernel.  Indeed, as a result of
\cref{pr:orff_defines_kernel}, in the same way we defined an \acs{ORFF}, we can
define an approximate feature operator $\tildeW{\omega}$ which maps
$\tildeH{\omega}$ onto $\mathcal{H}_{\tildeK{\omega}}$, where
$\tildeK{\omega}(x,z)= \tildePhi{\omega}(x)^\adjoint\tildePhi{\omega}(z)$, for
all $x$, $z\in\mathcal{X}$.
\begin{definition}[Random Fourier feature operator]
    Let $\seq{\omega}=(\omega_j)_{j=1}^D\in\dual{\mathcal{X}}^D$ and let
    $\tildeK{\omega}_e=\frac{1}{D}\sum_{j=1}^D
    \conj{\pairing{\cdot,\omega_j}}B(\omega_j)B(\omega_j)^\adjoint$.  We call
    random Fourier feature operator the linear application
    $\tildeW{\omega}:\tildeH{\omega}\to \mathcal{H}_{\tildeK{\omega}}$ defined
    as
    \begin{dmath*}
        \left(\tildeW{\omega} \theta\right)(x)
        \colonequals \tildePhi{\omega}(x)^\adjoint \theta
        =\frac{1}{\sqrt{D}}\sum_{j=1}^D
        \conj{\pairing{x,\omega_j}}B(\omega_j)\theta_j
    \end{dmath*}
    where $\theta=\Vect_{j=1}^D\theta_j \in\tildeH{\omega}$.  Then from
    \cref{pr:feature_operator}, $\left(\Ker \tildeW{\omega}\right)^\perp =
    \lspan\Set{\tildePhi{\omega}(x)y | \forall x\in\mathcal{X},\enskip \forall
    y\in\mathcal{Y}} \hiderel{\subseteq} \tildeH{\omega}$.
\end{definition}
The random Fourier feature operator is useful to show the relations between the
random Fourier feature map with the functional feature map defined in
\cref{pr:fourier_feature_map}. The relationship between the generic feature map
(defined for all \acl{OVK}) the functional feature map (defining a
shift-invariant $\mathcal{Y}$-Mercer \acl{OVK}) and the random Fourier feature
map is presented in \cref{fig:rel_features}.
\begin{proposition}
    \label{pr:phitilde_phi_rel} For any $g\in
    \mathcal{H}=L^2(\mathcal{\dual{X}}, \probability_{\dual{\Haar},\rho};
    \mathcal{Y}')$, let $\theta \colonequals \frac{1}{\sqrt{D}}\Vect_{j=1}^D
    g(\omega_j), \enskip \omega_j \sim \probability_{\dual{\Haar},\rho}
    \enskip\text{\ac{iid}}$ Then
    \begin{enumerate}
        \item \label{pr:cv_feature_map_1} $\left(\tildeW{\omega}
        \theta\right)(x)=\tildePhi{\omega}(x)^\adjoint \theta
        \converges{\acs{asurely}}{D\to\infty} \Phi_x^\adjoint g=(Wg)(x)$,
        \item \label{pr:cv_feature_map_2} $\norm{\theta}_{\tildeH{\omega}}^2
        \converges{\acs{asurely}}{D\to\infty} \norm{g}_{\mathcal{H}}^2$,
    \end{enumerate}
\end{proposition}
We write $\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(x)\approx K(x,z)$
when $\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(x)
\converges{\acs{asurely}}{} K(x,z)$ in the weak operator topology when $D$
tends to infinity. With mild abuse of notation we say that
$\tildePhi{\omega}(x)$ is an approximate feature map of the functional feature
map $\Phi_x$ \acs{ie}~$\tildePhi{\omega}(x)\approx \Phi_x$, when for all $y'$,
$y\in\mathcal{Y}$,
\begin{dmath*}
    \inner{y, K(x,z)y'}_{\mathcal{Y}}=\inner{\Phi_x y, \Phi_z
    y'}_{L^2(\dual{\mathcal{X}}, \probability_{\dual{\Haar}, \rho};
    \mathcal{Y'})} \hiderel{\approx} \inner{\tildePhi{\omega}(x)y,
    \tildePhi{\omega}(x)y'}_{\tildeH{\omega}}\colonequals \inner{y,
    \tilde{K}(x,z)y'}_{\mathcal{Y}}
\end{dmath*}
where $\Phi_x$ is defined in the sense of \cref{pr:fourier_feature_map}.

\subsection{Examples of Operator Random Fourier Feature maps}
\label{subsec:examples_ORFF} We now give two examples of operator-valued random
Fourier feature map. First we introduce the general form of an approximated
feature map for a matrix-valued kernel on the additive group
$(\mathbb{R}^d,+)$.
\begin{example}[Matrix-valued kernel on the additive group]
    \label{ex:additive_group} In the following let $K(x,z)=K_0(x-z)$ be a
    $\mathcal{Y}$-Mercer matrix-valued kernel on $\mathcal{X}=\mathbb{R}^d$,
    invariant \acs{wrt}~the group operation $+$. Then the function
    $\tildePhi{\omega}$ defined as follow is an \acl{ORFF} of $K_{0}$.
    For all $y\in\mathcal{Y}$,
    \begin{dmath*}
        \tildePhi{\omega}(x)y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}B(\omega_j)^\adjoint y \\
            \sin{\inner{x,\omega_j}_2}B(\omega_j)^\adjoint y
        \end{pmatrix}
        \condition{$\omega_j \sim \probability_{\dual{\Haar},\rho}$
        \acs{iid}.}
    \end{dmath*}
\end{example}
In particular we deduce the following features maps for the kernels proposed in
\cref{subsec:dec_examples}.
\begin{itemize}
    \item For the decomposable Gaussian kernel
    $K_0^{dec,gauss}(\delta)=k_0^{gauss}(\delta)\Gamma$ for all
    $\delta\in\mathbb{R}^d$, let $BB^\adjoint=\Gamma$. A bounded --and
    unbounded-- \acs{ORFF} map is
    \begin{dmath*}
        \tildePhi{\omega}(x)y
        =\frac{1}{\sqrt{D}} \Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2} B^\adjoint y \\
            \sin{\inner{x,\omega_j}_2}B^\adjoint y
        \end{pmatrix}
        \hiderel{=}(\tildephi{\omega}(x)\otimes B^\adjoint)y,
    \end{dmath*}
    where $\omega_j \hiderel{\sim}
    \probability_{\mathcal{N}(0,\sigma^{-2}I_d)}$ \ac{iid}~and
    $\tildephi{\omega}(x)=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    \begin{pmatrix}
        \cos{\inner{x,\omega_j}_2} \\
        \sin{\inner{x,\omega_j}_2}
    \end{pmatrix}$ is a scalar \acs{RFF}
    map~\citep{Rahimi2007}.
    \item For the curl-free Gaussian kernel,
    $K_0^{curl,gauss}=-\nabla\nabla^\transpose k_0^{gauss}$ an unbounded
    \acs{ORFF} map is
    \begin{dmath}
        \label{eq:unbounded_curl_free_orff}
        \tildePhi{\omega}(x)y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}g
            \cos{\inner{x,\omega_j}_2}\omega_j^\transpose y \\
            \sin{\inner{x,\omega_j}_2}\omega_j^\transpose y
        \end{pmatrix},
    \end{dmath}
    $\omega_j \hiderel{\sim} \probability_{\mathcal{N}(0,\sigma^{-2}I_d)}$
    \ac{iid}~and a bounded \acs{ORFF} map is
    \begin{dmath*}
        \tildePhi{\omega}(x) y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}\frac{\omega_j^\transpose
            }{\norm{\omega_j}} y \\
            \sin{\inner{x,\omega_j}_2}\frac{\omega_j^\transpose
            }{\norm{\omega_j}} y
        \end{pmatrix}
        \condition{$\omega_j \hiderel{\sim} \probability_{\rho}$ \ac{iid}.}
    \end{dmath*}
    where $\rho(\omega) = \frac{\sigma^2\norm{\omega}^2}{d} \mathcal{N}(0,
    \sigma^{-2} I_d)(\omega)$ for all $\omega\in\mathbb{R}^d$.
    \item For the divergence-free Gaussian kernel
    $K_0^{div,gauss}(x,z)=(\nabla\nabla^\transpose -\Delta I_d)
    k_0^{gauss}(x,z)$ an unbounded \acs{ORFF} map is
    \begin{dmath}
        \label{eq:unbounded_div_free_orff}
        \tildePhi{\omega}(x) y
        =\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}B(\omega_j)^\transpose y \\
            \sin{\inner{x,\omega_j}_2}B(\omega_j)^\transpose y
        \end{pmatrix}
    \end{dmath}
    where $\omega_j \hiderel{\sim} \probability_{\rho}$ \ac{iid}~and
    $B(\omega)=\left(\norm{\omega}I_d-\omega\omega^\transpose \right)$ and
    $\rho=\mathcal{N}(0,\sigma^{-2}I_d)$ for all $\omega\in\mathbb{R}^d$. A
    bounded \acs{ORFF} map is
    \begin{dmath*}
        \tildePhi{\omega}(x) y=\frac{1}{\sqrt{D}}\Vect_{j=1}^Dg
        \begin{pmatrix}
            \cos{\inner{x,\omega_j}_2}B(\omega_j)^\transpose y \\
            \sin{\inner{x,\omega_j}_2}B(\omega_j)^\transpose y
            \end{pmatrix}
            \condition{$\omega_j \hiderel{\sim} \probability_{\rho}$ \ac{iid},}
    \end{dmath*}
    where $B(\omega) = \left(I_d - \frac{\omega\omega^\transpose
    }{\norm{\omega}^2}\right)$ and $\rho(\omega) =
    \frac{\sigma^2\norm{\omega}^2}{d}\mathcal{N}(0,\sigma^{-2}I_d)$ for allg
    $\omega\in\mathbb{R}^d$.
\end{itemize}
The second example extends scalar-valued Random Fourier Features on the skewed
multiplicative group --described in \cref{subsec:character} and
\cref{subsubsec:skewedchi2}-- to the operator-valued case.
\begin{example}[Matrix-valued kernel on the skewed multiplicative group]
    In the following, $K(x,z)=K_{1-c}(x\odot z^{-1})$ is a $\mathcal{Y}$-Mercer
    matrix-valued kernel on $\mathcal{X}=(-c;+\infty)^d$ invariant
    \acs{wrt}~the group operation\footnote{The group operation $\odot$ is
    defined in \cref{subsubsec:skewedchi2}.} $\odot$. Then the function
    $\tildePhi{\omega}$ defined as follow is an \acl{ORFF} of $K_{1-c}$. For
    all $y\in\mathcal{Y}$,
    \begin{dmath*}
        \tildePhi{\omega}(x) y=\frac{1}{\sqrt{D}}\Vect_{j=1}^D
        \begin{pmatrix}
            \cos{\inner{\log(x+c),\omega_j}_2}B(\omega_j)^\adjoint y \\
            \sin{\inner{\log(x+c),\omega_j}_2}B(\omega_j)^\adjoint y
        \end{pmatrix}\condition{$\omega_j \sim
        \probability_{\dual{\Haar},\rho}$ \ac{iid}.}
    \end{dmath*}
\end{example}
\subsection{Regularization property}
\label{subsec:regularization_property}
We have shown so far that it is always possible to construct a feature map that
allows to approximate a shift-invariant $\mathcal{Y}$-Mercer kernel. However we
could also propose a construction of such map by studying the regularization
induced with respect to the \acl{FT} of a target function $f\in \mathcal{H}_K$.
In other words, what is the norm in $L^2(\dual{\mathcal{X}}, \dual{\Haar};
\mathcal{Y}')$ induced by $\norm{\cdot}_K$?
\begin{proposition}
    \label{pr:fourier_reg_ovk}
    Let $K$ be a shift-invariant $\mathcal{Y}$-Mercer Kernel such that for all
    $y$, $y'$ in $\mathcal{Y}$, $\inner{y', K_e(\cdot)y}_{\mathcal{Y}}\in
    L^1(\mathcal{X}, \Haar)$. Then for all $f\in\mathcal{H}_K$
    \begin{dmath}
        \norm{f}^2_K = \displaystyle\int_{\dual{\mathcal{X}}}
        \frac{\inner*{\FT{f}(\omega), A\left(\omega\right)^\dagger
        \FT{f}(\omega)}_{\mathcal{Y}}}{\rho(\omega)} d\dual{\Haar}(\omega).
        \label{eq:reg_L2}
    \end{dmath}
    where $\inner{y', A(\omega)y}\rho(\omega)\colonequals\FT{\inner{y',
    K_e(\cdot)y}}(\omega)$.  \label{pr:regularization}
\end{proposition}
Note that if $K(x,z)=k(x,z)$ is a scalar kernel then for all $\omega$ in
$\dual{\mathcal{X}}$, $A(\omega)=1$. Therefore we recover the well known result
for kernels that is for any $f\in\mathcal{H}_k$ we have $\norm{f}_k =
\int_{\dual{\mathcal{X}}} \FT{k_e}(\omega)^{-1} \FT{f}(\omega)^2
d\dual{\Haar}(\omega)$~\citep{Yang2012, vertregularization,
smola1998connection}. Eventually from this last equation we also recover
\cref{pr:kernel_reg} for decomposable kernels. If
$A(\omega)=\Gamma\in\mathcal{L}_+(\mathbb{R}^p)$, $\norm{f}_K = \sum_{i,j=1}^p
\left(\Gamma^\dagger\right)_{ij}\inner{f_i,f_j}_k$ We also note that the
regularization property in $\mathcal{H}_K$ does not depends (as expected) on
the decomposition of $A(\omega)$ into $B(\omega)B(\omega)^\adjoint $.
Therefore the decomposition should be chosen such that it optimizes the
computation cost. For instance if $A(\omega)\in\mathcal{L}(\mathbb{R}^p)$ has
rank $r$, one could find an operator $B(\omega)\in\mathcal{L}(\mathbb{R}^p,
\mathbb{R}^r)$ such that $A(\omega)=B(\omega)B(\omega)^\adjoint$. Moreover, in
light of \cref{pr:regularization} the regularization property of the kernel
with respect to the \acl{FT}, it is also possible to define an approximate
feature map of an \acl{OVK} from its regularization properties in the
\acs{vv-RKHS} as proposed in \cref{alg:ORFF2_construction}.
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\begin{center}
    \begin{algorithm2e}[t!]\label{alg:ORFF2_construction}
        \SetAlgoLined
        \Input{%
        \begin{itemize}
            \item The pairing $\pairing{x, \omega}$ of the \acs{LCA} group
            $(\mathcal{X}, \groupop)$.
            \item A probability measure $\probability_{\dual{\Haar},\rho}$ with
            density $\rho$ \acs{wrt}~the haar measure $\dual{\Haar}$ on
            $\dual{\mathcal{X}}$.
            \item An operator-valued function
            $B:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y},\mathcal{Y}')$ such
            that for all $y$ $y'\in\mathcal{Y}$, $\inner{y',
            B(\cdot)B(\cdot)^\adjoint y}\in
            L^1(\dual{\mathcal{X}},\probability_{\dual{\Haar},\rho})$.
            \item $D$ the number of features.
        \end{itemize}}
        \Output{A random feature $\tildePhi{\omega}(x)$ such that
        $\tildePhi{\omega}(x)^\adjoint \tildePhi{\omega}(z) \approx K(x,z)$.}
        \BlankLine
        Draw $D$ random vectors $(\omega_j)_{j=1}^D$ \ac{iid}~from the
        probability law $\probability_{\dual{\Haar},\rho}$\;
        \Return $
        \begin{cases}
            \tildePhi{\omega}(x) \in\mathcal{L}(\mathcal{Y}, \tildeH{\omega})
            &: y \mapsto \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
            \omega_j}B(\omega_j)^\adjoint y \\
            \tildePhi{\omega}(x)^\adjoint \in\mathcal{L}(\tildeH{\omega},
            \mathcal{Y}) &: \theta \mapsto \frac{1}{\sqrt{D}} \sum_{j=1}^D
            \pairing{x, \omega_j}B(\omega_j)\theta_j
        \end{cases}$\;
        \caption{Construction of \acs{ORFF}}
    \end{algorithm2e}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main contribution: convergence with high probability of the
\acpdfstring{ORFF} estimator}
\label{sec:consistency_of_the_ORFF_estimator}
We are now interested in a non-asymptotic analysis of the \ac{ORFF}
approximation of shift-invariant $\mathcal{Y}$-Mercer kernels on \acs{LCA}
group $\mathcal{X}$ endowed with the operation group $\groupop$ where
$\mathcal{X}$ is a Banach space (The more general case where $\mathcal{X}$ is
a Polish space is discussed in the appendix \cref{subsec:concentration_proof}).
For a given $D$, we study how close is the
approximation $\tilde{K}(x,z)=\tildePhi{1:D}(x)^*\tildePhi{1:D}(z)$ to the
target kernel $K(x,z)$ for any $x,z$ in $\mathcal{X}$.
\paragraph{}
If $A\in\mathcal{L}_+(\mathcal{Y})$ we denote
$\norm{A}_{\mathcal{Y},\mathcal{Y}}$ its operator norm (the induced norm). For
$x$ and $z$ in some non-empty compact $\mathcal{C} \subset \mathbb{R}^d$, we
consider: $F(x \groupop \inv{z}) =\tilde{K}(x,z)-K(x,z)$ and study how the
uniform norm $\norm{\tilde{K}-K}_{\mathcal{C}\times\mathcal{C}} \colonequals
\sup_{(x,z)\in\mathcal{C}\times\mathcal{C}}
\norm{\tilde{K}(x,z)-K(x,z)}_{\mathcal{Y},\mathcal{Y}}$ behaves according to
$D$. All along this document we denote $\delta=x\groupop z^{-1}$ for all $x$
and $z\in\mathcal{X}$. \Cref{fig:approximation_error} empirically shows
convergence of three different \acs{OVK} approximations for $x,z$ sampled from
the compact $[-1,1]^4$ and using an increasing number of sample points $D$. The
log-log plot shows that all three kernels have the same convergence rate, up to
a multiplicative factor.
%\paragraph{}
%In order to bound the error with high probability, we turn to concentration
%inequalities devoted to random matrices~\citep{Boucheron}. The concentration
%phenomenon can be summarized in the following sentence of
%\citet{ledoux2005concentration}. \say{A random variable that depends (in a
%smooth way) on the influence of many random variables (but not too much on any
%of them) is essentially constant}.
\paragraph{}
A typical application is the study of the deviation of the empirical mean of
\acl{iid} random variables to their expectation. This means that given an error
$\epsilon$ between the kernel approximation $\tildeK{\omega}$ and the true
kernel $K$, if we are given enough samples to construct $\tildeK{\omega}$, the
probability of measuring an error greater than $\epsilon$ is essentially zero
(it drops at an exponential rate with respect to the number of samples $D$). To
measure the error between the kernel approximation and the true kernel at a
given point many metrics are possible. \acs{eg} any matrix norm such as the
Hilbert-Schmidt norm, trace norm, the operator norm or Schatten norms. In this
work we focus on measuring the error in terms of operator norm. For all $x$,
$z\in\mathcal{X}$ we look for a bound on
\begin{dmath*}
    \probability_{\rho} \Set{(\omega_j)_{j=1}^D | \norm{\tildeK{\omega}(x, z) -
    K(x, z)}_{\mathcal{Y}, \mathcal{Y}} \ge \epsilon }
    =
    \probability_{\rho} \Set{(\omega_j)_{j=1}^D | \sup_{0\neq y\in\mathcal{Y}}
    \frac{\norm{(\tildeK{\omega}(x, z) - K(x,
    z))y}_{\mathcal{Y}}}{\norm{y}_{\mathcal{Y}}} \ge \epsilon}
\end{dmath*}
In other words, given any vector $y\in\mathcal{Y}$ we study how the residual
operator $\tildeK{\omega} - K$ is able to send $y$ to zero. We believe that
this way of measuring the \say{error} to be more intuitive. Moreover, on
contrary to an error measure with the Hilbert-Schmidt norm, the operator norm
error does not grows linearly with the dimension of the output space as the
Hilbert-Schmidt norm does. On the other hand the Hilbert-schmidt norm makes the
studied random variables Hilbert space valued, for which it is much easier to
derive concentration inequalities \citep{smale2007learning, pinelis1994optimum,
naor2012banach}. Note that in the scalar case ($A(\omega)= 1$) the
Hilbert-Schmidt norm error and the operator norm are the same and measure the
deviation between $\tildeK{\omega}$ and $K$ as the absolute value of their
difference.
\paragraph{}
A raw concentration inequality of the kernel estimator gives the error on one
point. If one is interesting in bounding the maximum error over $N$ points,
applying a union bound on all the point would yield a bound that grows linearly
with $N$. This would suggest that when the number of points increase, even if
all of them are concentrated in a small subset of $\mathcal{X}$, we should draw
increasingly more features to have an error below $\epsilon$ with high
probability. However if we restrict ourselves to study the error on a compact
subset of $\mathcal{X}$ (and in practice data points lies often in a closed
bounded subset of $\mathbb{R}^d$), we can cover this compact subset by a finite
number of closed balls and apply the concentration inequality and the union
bound only on the center of each ball. Then if the function
$\norm{\tildeK{\omega}_e-K_e}$ is smooth enough on each ball (\acs{ie}
Lipschitz) we can guarantee with high probability that the error between the
centers of the balls will not be too high. Eventually we obtain a bound in the
worst case scenario on all the points in a subset $\mathcal{C}$ of
$\mathcal{X}$. This bound depends on the covering number
$\mathcal{N}(\mathcal{C}, r)$ of $\mathcal{X}$ with ball of radius $r$. When
$\mathcal{X}$ is a Banach space, the covering number is proportional to the
diameter of the diameter of $\mathcal{C}\subseteq\mathcal{X}$.
\paragraph{}
Prior to the presentation of general results, we briefly recall the uniform
convergence of \acs{RFF} approximation for a scalar shift invariant kernel on
the additive \acs{LCA} group $\mathbb{R}^d$ and introduce a direct corollary
about decomposable shift-invariant \acs{OVK} on the \acs{LCA} group
$(\mathbb{R}^d, +)$.
\begin{figure}[t]
    \centering
    \resizebox{.85\textwidth}{!}{\input{./gfx/approximation.pgf}}
    \caption[\acs{ORFF} reconstruction error]{Error reconstructing the target
    operator-valued kernel $K$ with \acs{ORFF}
    approximation $\tilde{K}$ for the decomposable, curl-free and
    divergence-free kernel.}
    \label{fig:approximation_error}
\end{figure}
\subsection{Random Fourier Features in the scalar case and decomposable OVK}
\citet{Rahimi2007} proved the uniform convergence of \acf{RFF} approximation
for a scalar shift-invariant kernel on the \acs{LCA} group $\mathbb{R}^d$
endowed with the group operation $\groupop=+$. In the case of the
shift-invariant decomposable \acs{OVK}, an upper bound on the error can be
obtained as a direct consequence of the result in the scalar case obtained
by~\citet{Rahimi2007} and other authors~\citep{sutherland2015, sriper2015}.
\begin{theorem}[Uniform error bound for \ac{RFF},~\citet{Rahimi2007}]
    \label{rff-scalar-bound}
    Let $\mathcal{C}$ be a compact of subset of $\mathbb{R}^d$ of diameter
    $\abs{\mathcal{C}}$. Let $k$ be a shift invariant kernel, differentiable
    with a bounded second derivative and $\probability_{\rho}$ its normalized
    \acl{IFT} such that it defines a probability measure. Let
    $\widetilde{k}=\sum_{j=1}^D\cos{\inner{\cdot, \omega_j}} \hiderel{\approx}
    k(x,z) \enskip\text{and}\enskip \sigma^2\hiderel{=}\expectation_{\rho}
    \norm{\omega}^2_2$.  Then we have
    \begin{dmath*}
        \probability_{\rho}\Set{(\omega_j)_{j=1}^D |
        \norm{\tilde{k}-k}_{\mathcal {C}\times\mathcal{C}}\ge \epsilon } \le
        2^8\left( \frac{\sigma \abs{\mathcal{C}}}{\epsilon} \right)^2\exp\left(
        -\frac{\epsilon^2D}{4(d+2)} \right)
    \end{dmath*}
\end{theorem}
From \cref{rff-scalar-bound}, we can deduce the following corollary about the
uniform convergence of the \acs{ORFF} approximation of the decomposable kernel.
We recall that for a given pair $x$, $z$ in $\mathcal{C}$, $\tilde{K}(x,z)=
\tildePhi{\omega}(x)^* \tildePhi{\omega}(z)=\Gamma\tilde{k}(x,z)$ and
$K_0(x-z)=\Gamma \expectation_{\dual{\Haar},\rho}[\tilde{k}(x,z)]$.
\begin{corollary}[Uniform error bound for decomposable \acs{ORFF}]
    \label{c:dec-bound}
    Let $\mathcal{C}$ be a compact of subset of $\mathbb{R}^d$ of diameter
    $\abs{\mathcal{C}}$. Let $K$ be a decomposable kernel built from a positive
    operator self-adjoint $\Gamma$, and $k$ a shift invariant kernel with
    bounded second derivative such that
    $\widetilde{K}=\sum_{j=1}^D\cos{\inner{\cdot, \omega_j}}\Gamma
    \hiderel{\approx} K \enskip\text{and}\enskip
    \sigma^2\hiderel{=}\expectation_{\rho} \norm{\omega}^2_2$.  Then
    \begin{dmath*}
        \probability_{%
        \rho}\Set{(\omega_j)_{j=1}^D|\norm{\widetilde{K}-K}_{\mathcal{C} \times
        \mathcal{C}}\ge \epsilon } \le 2^8\left( \frac{\sigma
        \norm{\Gamma}_{\mathcal{Y},\mathcal{Y}} \abs{\mathcal{C}}}{\epsilon}
        \right)^2\exp\left( -\frac{\epsilon^2D}{4\norm{\Gamma}_2^2(d+2)} \right)
    \end{dmath*}
\end{corollary}
Note that a similar corollary could have been obtained for the recent
result of~\citet{sutherland2015} who refined the bound proposed by Rahimi and
Recht by using a Bernstein concentration inequality instead of the Hoeffding
inequality. More recently~\citet{sriper2015} showed an optimal bound for
\acl{RFF}. The improvement of~\citet{sriper2015} is mainly in the constant
factors where the bound does not depend linearly on the diameter
$\abs{\mathcal{C}}$ of $\mathcal{C}$ but exhibit a logarithmic dependency
$\log\left(\abs{\mathcal{C}}\right)$, hence requiring significantly less random
features to reach a desired uniform error with high probability. Moreover,
\citet{sutherland2015} also considered a bound on the expected max error
$\expectation_{\dual{\Haar}, \rho} \norm{\widetilde{K}-K}_{\infty}$, which is
obtained using Dudley's entropy integral~\citep{dudley1967sizes, Boucheron} as
a bound on the supremum of an empirical process by the covering number of the
indexing set. This useful theorem is also part of the proof of
\citet{sriper2015}.
\subsection{Uniform convergence of \acpdfstring{ORFF} approximation on
\acpdfstring{LCA} groups}
%In this analysis, we assume that $\mathcal{Y}$ is finite dimensional, in
%\cref{remark:infinite_dimension}, we discuss how the proof could be extended to
%infinite dimensional output Hilbert spaces. We propose a bound for \acl{ORFF}
%approximation in the general case. It relies on two main ideas:
%\begin{enumerate}
    %\item a matrix-Bernstein concentration inequality for random matrices need
    %to be used instead of concentration inequality for scalar random variables,

    %\item a general theorem, valid for random matrices with bounded norms (such
    %as decomposable kernel \acs{ORFF} approximation) as well as un\-bound\-ed
    %norms (such as the \acs{ORFF} approximation we proposed for curl and
    %divergence-free kernels, for which the norm behave as subexponential random
    %variables).
%\end{enumerate}
Before introducing the new theorem, we give the definition of the Orlicz norm
which gives a proxy-bound on the norm of subexponential random variables.
\begin{definition}[Orlicz norm~\citep{van1996weak}]
    \label{def:orlicz}
    Let $\psi:\mathbb{R}_+\to\mathbb{R}_+$ be a non-decreasing convex function
    with $\psi(0)=0$. For a random variable $X$ on a measured space
    $(\Omega,\mathcal{T} (\Omega),\mu)$, the quantity $\norm{X}_{\psi}
    \hiderel{=} \inf \Set{C > 0  | \expectation_{\mu}[\psi\left( \abs{X}/C
    \right)]\le 1}$.  is called the Orlicz norm of $X$.
\end{definition}
Here, the function $\psi$ is chosen as $\psi(u)=\psi_{\alpha}(u)$ where
$\psi_{\alpha}(u) \colonequals e^{u^{\alpha}}-1$. When $\alpha=1$, a random
variable with finite Orlicz norm is called a \emph{subexponential variable}
because its tails decrease at an exponential rate. Let $X$ be a self-adjoint
random operator. Given a scalar-valued measure $\mu$, we call \emph{variance}
of an operator $X$ the quantity $\variance_{\mu}[X]=\expectation_
{\mu}[X-\expectation_{\mu}[X]]^2$. 
%With this convention if $X$ is a $p\times
%p$ Hermitian matrix,
%\begin{dmath*}
    %\variance_{\mu}[X]_{\ell m}=\sum_{r=1}^p\covariance{X_{\ell r}, X_{rm}}.
%\end{dmath*}
Among the possible concentration inequalities adapted to random operators
\citep{tropp2015introduction, minsker2011some, ledoux2013probability,
pinelis1994optimum, koltchinskii2013remark}, we focus on the results of
\citet{tropp2015introduction, minsker2011some}, for their robustness to high or
potentially infinite dimension of the output space $\mathcal{Y}$. To guarantee
a good scaling with the dimension of $\mathcal{Y}$ we introduce the notion of
intrinsic dimension (or effective rank) of an operator.
\begin{definition}
    \label{def:intdim}
    Let $A$ be a trace class operator acting on a Hilbert space $\mathcal{Y}$.
    We call intrinsic dimension the quantity: $\intdim(A)
    =\norm{A}_{\mathcal{Y}, \mathcal{Y}}^{-1} \Tr\left[A\right]$.
\end{definition}
Indeed the bound proposed in our first publication at \acs{ACML}
\citep{brault2016random} based on \citet{koltchinskii2013remark} depends on $p$
while the present bound depends on the intrinsic dimension of the variance of
$A(\omega)$ which is always smaller than $p$ when the operator $A(\omega)$ is
Hilbert-Schmidt ($p\le\infty$).
\begin{corollary}
    \label{corr:unbounded_consistency}
    Let $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ be a
    shift-invariant $\mathcal{Y}$-Mercer kernel, where $\mathcal{Y}$ is a
    finite dimensional Hilbert space of dimension $p$ and $\mathcal{X}$ a
    finite dimensional Banach space of dimension $d$. Moreover, let
    $\mathcal{C}$ be a closed ball of $\mathcal{X}$ centred at the origin of
    diameter $\abs{\mathcal{C}}$,
    $A:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y})$ and
    $\probability_{\dual{\Haar},\rho}$ a pair such that
    \begin{dmath*}
        \tilde{K}_e = \sum_{j=1}^D \cos{\pairing{\cdot,\omega_j}}A(\omega_j)
        \hiderel{\approx}
        K_e\condition{$\omega_j\sim\probability_{\dual{\Haar}, \rho}$
        \acs{iid}.}.
    \end{dmath*}
    Let $\mathcal{D}_{\mathcal{C}}=\mathcal{C}\groupop\mathcal{C}^{-1}$ and
    $V(\delta) \succcurlyeq \variance_{\dual{\Haar},\rho} \tilde{K}_e(\delta)$
    for all $\delta\in\mathcal{D}_{\mathcal{C}}$ and $H_\omega$ be the
    Lipschitz constant of the function $h: x\mapsto
    \pairing{x,\omega}$. If the three following constants exist
    \begin{dmath*}
        m \ge \int_{\dual{\mathcal{X}}} H_\omega
        \norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}} d\probability_{\dual{\Haar},
        \rho}(\omega) \hiderel{<} \infty
    \end{dmath*}
    and
    \begin{dmath*}
        u \ge 4\left(\norm{\norm{A(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}
        + \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
        \norm{K_e(\delta)}_{\mathcal{Y},\mathcal{Y}}\right) \hiderel{<} \infty
    \end{dmath*}
    and
    \begin{dmath*}
        v \ge \sup_{\delta\in\mathcal{D}_{\mathcal{C}}} D
        \norm{V(\delta)}_{\mathcal{Y}, \mathcal{Y}} \hiderel{<} \infty.
    \end{dmath*}
    Define $p_{int}\ge \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
    \intdim(V(\delta))$, then for all $0 < \epsilon \le m \abs{C}$,
    \begin{dmath*}
        \probability_{\dual{\Haar,\rho}}\Set{(\omega_j)_{j=1}^D |
        \norm{\tilde{K}-K}_{\mathcal{C}\times\mathcal{C}} \ge \epsilon}
        \le 8\sqrt{2} \left( \frac{m\abs{\mathcal{C}}}{\epsilon}
        \right)
        {\left(p_{int}r_{v/D}(\epsilon)\right)}^{\frac{1}{d + 1}}
        \begin{cases}
            \exp\left(-D\frac{\epsilon^2}{8
            v(d+1)\left(1 + \frac{1}{p}\right)}
            \right) \condition{$\epsilon \le
            \frac{v}{u}\frac{1+1/p}{K(v,
            p)}$} \\
            \exp\left(-D\frac{\epsilon}{8u(d+1)K(v,
            p)}\right)\condition{otherwise,}
        \end{cases}
    \end{dmath*}
    where $K(v, p)=\log\left(16 \sqrt{2}
    p\right)+\log\left(\frac{u^2}{v}\right) $ and $r_{v/D}(\epsilon)=1 +
    \frac{3}{\epsilon^2\log^2(1 + D \epsilon / v)}$.
\end{corollary}
We give a comprehensive full proof of the theorem in
\cref{subsec:concentration_proof}. It follows the usual scheme derived
in~\citet{Rahimi2007} and~\citet{sutherland2015} and involves Bernstein
concentration inequality for unbounded symmetric matrices
(\cref{th:Bernstein3}).

\subsection{Dealing with infinite dimensional operators}
\label{remark:infinite_dimension}
We studied the concentration of \acsp{ORFF} under the assumption that
$\mathcal{Y}$ is finite dimensional. Indeed a $d$ term characterizing the
dimension of the input space $\mathcal{X}$ appears in the bound proposed in
\cref{corr:unbounded_consistency}, and when $d$ tends to infinity, the
exponential part goes to zero so that the probability is bounded by a
constant greater than one. Unfortunately, considering unbounded random
operators \citet{minsker2011some} doesn't give any tighter solution.
\paragraph{}
In our first bound presented at \acs{ACML}, we presented a bound based on a
matrix concentration inequality for unbounded random variable. Compared to this
previous bound, \cref{corr:unbounded_consistency} does not depend on the
dimensionality $p$ of the output space $\mathcal{Y}$ but on the intrinsic
dimension of the operator $A(\omega)$. However to remove the dependency in $p$
in the exponential part, we must turn our attention to operator concentration
inequalities for bounded random variable. To the best of our knowledge we are
not aware of concentration inequalities working for \say{unbounded} operator-
valued random variables. Following the same proof than
\cref{corr:unbounded_consistency} we obtain
\cref{corr:bounded_infinite_dim_consistency}.
\begin{corollary}
    \label{corr:bounded_infinite_dim_consistency}
    Let $K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})$ be a
    shift-invariant $\mathcal{Y}$-Mercer kernel, where $\mathcal{Y}$ is a
    Hilbert space and $\mathcal{X}$ a finite dimensional Banach space of
    dimension $D$. Moreover, let $\mathcal{C}$ be a closed ball of
    $\mathcal{X}$ centered at the origin of diameter $\abs{\mathcal{C}}$,
    subset of $\mathcal{X}$, $A:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y})$
    and $\probability_{\dual{\Haar},\rho}$ a pair such that
    \begin{dmath*}
        \tilde{K}_e = \sum_{j=1}^D \cos{\pairing{\cdot,\omega_j}}A (\omega_j)
        \hiderel{\approx}
        K_e\condition{$\omega_j\sim\probability_{\dual{\Haar}, \rho}$
        \acs{iid}.}
    \end{dmath*}
    where $A(\omega_j)$ is a Hilbert-Schmidt operator for all $j \in
    \mathbb{N}^*_D$. Let $\mathcal{D}_{\mathcal{C}}=\mathcal{C} \groupop
    \mathcal{C}^{-1}$ and $V (\delta) \succcurlyeq\variance_{\dual{\Haar},\rho}
    \tilde{K}_e (\delta)$ for all $\delta\in\mathcal{D}_{\mathcal{C}}$ and
    $H_\omega$ be the Lipschitz constant of the function $h: x\mapsto
    \pairing{x,\omega}$. If the three following constants exists
    \begin{dmath*}
        m \ge\int_{\dual{\mathcal{X}}} H_{\omega}
        \norm{A (\omega)}_{\mathcal{Y},\mathcal{Y}}
        d\probability_{\dual{\Haar}, \rho}(\omega) \hiderel{<} \infty{}
    \end{dmath*}
    and
    \begin{dmath*}
        u \ge\esssup_{\omega\in\dual{\mathcal{X}}}
        \norm{A (\omega)}_{\mathcal{Y}, \mathcal{Y}} +
        \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
        \norm{K_e (\delta)}_{\mathcal{Y}, \mathcal{Y}} \hiderel{<} \infty{}
    \end{dmath*}
    and
    \begin{dmath*}
        v \ge\sup_{\delta\in\mathcal{D}_{\mathcal{C}}} D
        \norm{V (\delta)}_{\mathcal{Y}, \mathcal{Y}} \hiderel{<} \infty.
    \end{dmath*}
    define $p_{int} \ge \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
    \intdim\left(V(\delta)\right)$ then for all $\sqrt{\frac{v}{D}} +
    \frac{u}{3D} < \epsilon < m\abs{\mathcal{C}}$,
    \begin{dmath*}
        \probability_{\dual{\Haar,\rho}}\Set{(\omega_j)_{j=1}^D |
        \sup_{\delta\in\mathcal{D}_{\mathcal{C}}}
        \norm{F (\delta)}_{\mathcal{Y}, \mathcal{Y}} \ge\epsilon} \le~8\sqrt{2}
        \left(\frac{m\abs{\mathcal{C}}}{\epsilon}\right) p_{int}^{\frac{1}{d +
        1}} \exp\left(-D\psi_{v,d,u} (\epsilon) \right)
    \end{dmath*}
    where $\psi_{v,d,u}(\epsilon)=\frac{\epsilon^2}{2(d+1)(v + u
    \epsilon / 3)}$.
\end{corollary}
Again a full comprehensive proof is given in \cref{subsec:concentration_proof}
of the appendix. Notice that in this result, The dimension
$p=\dim{\mathcal{Y}}$ does not appear. Only the intrinsic dimension of the
variance of the estimator. Moreover when $d$ is large, the term
$p_{int}^{\frac{1}{d + 1}}$ goes to one, so that the impact of the intrinsic
dimension on the bound vanish when the dimension of the input space is large.
subsection{Variance of the \acpdfstring{ORFF} approximation}
We now provide a bound on the norm of the variance of $\tilde{K}$, required to
apply \cref{corr:unbounded_consistency,corr:bounded_infinite_dim_consistency}.
This is an extension of the proof of \citet{sutherland2015} to the
operator-valued case, and we recover their results in the scalar case when
$A(\omega)=1$. An illustration of the bound is provided in
\cref{fig:approximation_error_var} for the decomposable and the curl-free
\acs{OVK}.
\begin{proposition}[Bounding the \emph{variance} of $\tilde{K}_e$]
    \label{pr:variance_bound}
    Let $K$ be a shift invariant $\mathcal{Y}$-Mercer kernel on a second
    countable \ac{LCA} topological space $\mathcal{X}$. Let
    $A:\dual{\mathcal{X}}\to\mathcal{L}(\mathcal{Y})$ and
    $\probability_{\dual{\Haar},\rho}$ a pair such that $\tilde{K}_e =
    \sum_{j=1}^D \cos{\pairing{\cdot,\omega_j}}A (\omega_j) \hiderel{\approx}
    K_e$, $\omega_j\sim\probability_{\dual{\Haar}, \rho}$ \acs{iid} Then,
    \begin{dmath*}
        \variance_{\dual{\Haar}, \rho} \left[ \tilde{K}_e(\delta) \right]
        \preccurlyeq \frac{1}{2D} \left( \left( K_e(2\delta) + K_e(e) \right)
        \expectation_{\dual{\Haar}, \rho}\left[ A(\omega) \right] -
        2 K_e(\delta)^2 + \variance_{\dual{\Haar}, \rho}\left[
        A(\omega) \right]\right)
    \end{dmath*}
\end{proposition}
\begin{figure}[t]
    \begin{minipage}[c]{.46\linewidth}
        \centering\resizebox{\linewidth}{!}{%
        \input{./gfx/variance_dec.tikz}}
    \end{minipage}
    \begin{minipage}[c]{.54\linewidth}
        \centering\resizebox{\linewidth}{!}{%
        \input{./gfx/variance_curl.tikz}}
    \end{minipage}
    \caption[ORFF variance bound]{Comparison between an empirical bound on the
    norm of the variance of the decomposable (left) and  curl-free (right) ORFF
    obtained and the theoretical bound proposed in \cref{pr:variance_bound}
    versus $D$. \label{fig:approximation_error_var}}
\end{figure}
\subsection{Application on decomposable, curl-free and divergence-free
\acpdfstring{OVK}}
First, the two following examples discuss the form of $H_\omega$ for the
additive group and the skewed-multiplicative group. Here we view
$\mathcal{X}=\mathbb{R}^d$ as a Banach space endowed with the Euclidean norm.
Thus the Lipschitz constant $H_{\omega}$ is bounded by the supremum of the norm
of the gradient of $h_{\omega}$.
\begin{example}[Additive group]
    On the additive group, $h_\omega(\delta)=\inner{\omega, \delta}$. Hence
    $H_\omega=\norm{\omega}_2$.
\end{example}
\begin{example}[Skewed-multiplicative group]
    On the skewed multiplicative group, $h_\omega(\delta)=\inner{\omega,
    \log(\delta+c)}$. Therefore $\sup_{\delta\in\mathcal{C}}\norm{\nabla
    h_\omega(\delta)}_2 = \sup_{\delta\in\mathcal{C}}\norm{\omega/(\delta +
    c)}_2$.  Eventually $\mathcal{C}$ is compact subset of $\mathcal{X}$ and
    finite dimensional thus $\mathcal{C}$ is closed and bounded. Thus
    $H_\omega=\norm{\omega}_2/(\min_{\delta\in\mathcal{C}} \norm{\delta}_2+c)$.
\end{example}
Now we compute upper bounds on the norm of the variance and Orlicz norm of the
three \acsp{ORFF} we took as examples.
\subsubsection{Decomposable kernel}
notice that in the case of the Gaussian decomposable kernel, \acs{ie}
$A(\omega)=A$, $e=0$, $K_0(\delta)= Ak_0(\delta)$, $k_0(\delta) \geq 0$ and
$k_0(\delta)=1$, then we have
\begin{equation*}
    D\norm{\variance_\mu \left[ \tilde{K}_0(\delta)
    \right]}_{\mathcal{Y},\mathcal{Y}}\leq
    (1+k_0(2\delta))\norm{A}_{\mathcal{Y},\mathcal{Y}}/2 + k_0(\delta)^2.
\end{equation*}
\subsubsection{Curl-free and divergence-free kernels:}
recall that in this case $p=d$. For the (Gaussian) curl-free kernel,
$A(\omega)=\omega\omega^*$ where $\omega\in\mathbb{R}^d\sim\mathcal{N}(0,
\sigma^{-2}I_d)$ thus $\expectation_\mu [A(\omega)] = I_d/\sigma^2$ and
$\variance_{\mu}[A(\omega)]=(d+1)I_d/\sigma^4$. Hence,
\begin{equation*}
    D\norm{\variance_\mu \left[ \tilde{K}_0(\delta)
    \right]}_{\mathcal{Y},\mathcal{Y}} \leq
    \frac{1}{2}\norm{\frac{1}{\sigma^2}K_0(2\delta)-2
    K_0(\delta)^2}_{\mathcal{Y},\mathcal{Y}} + \frac{(d+1)}{\sigma^4}.
\end{equation*}
This bound is illustrated by \cref{fig:approximation_error} B, for a given
datapoint. Eventually for the Gaussian divergence-free kernel,
$A(\omega)=I\norm{\omega}_2^2-\omega\omega^*$, thus $\expectation_\mu
[A(\omega)] = I_d(d-1)/\sigma^2$ and $
\variance_{\mu}[A(\omega)]=d(4d-3)I_d/\sigma^4$. Hence,
\begin{equation*}
    D\norm{\variance_\mu \left[ \tilde{K}_0(\delta)
    \right]}_{\mathcal{Y},\mathcal{Y}} \leq
    \frac{1}{2}\norm{\frac{(d-1)}{\sigma^2}K_0(2\delta)-2
    K_0(\delta)^2}_{\mathcal{Y}, \mathcal{Y}}+ \frac{d(4d-3)}{\sigma^4}.
\end{equation*}
To conclude, we ensure that the random variable $\norm{A(\omega)}_{\mathcal{Y},
\mathcal{Y}}$ has a finite Orlicz norm with $\psi=\psi_1$ in these three cases.
\subsubsection{Computing the Orlicz norm}
for a random variable with strictly monotonic moment generating function (MGF),
one can characterize its inverse $\psi_1$ Orlicz norm by taking the functional
inverse of the MGF evaluated at 2 (see \cref{lm:orlicz_mgf} of the
appendix). In other words
$\norm{X}_{\psi_1}^{-1}=\MGF(x)^{-1}_X(2)$. For the Gaussian curl-free and
divergence-free kernel,
\begin{dmath*}
    \norm{A^{div}(\omega)}_{\mathcal{Y},\mathcal{Y}} =
    \norm{A^{curl}(\omega)}_{\mathcal{Y},\mathcal{Y}} \hiderel{=}
    \norm{\omega}_{2}^2,
\end{dmath*}
where $\omega\sim\mathcal{N}(0,I_d/\sigma^2)$, hence $\norm{A(\omega)}_2\sim
\Gamma(p/2,2/\sigma^2)$. The MGF of this gamma distribution is
$\MGF(x)(t)=(1-2t/\sigma^2)^{-(p/2)}$. Eventually
\begin{equation*}
    \norm{\norm{A^{div}(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}^{-1} =
    \norm{\norm{A^{curl}(\omega)}_{\mathcal{Y},\mathcal{Y}}}_{\psi_1}^{-1} =
    \frac{\sigma^2}{2}\left(1-4^{-\frac{1}{p}}\right).
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learning with \acs{ORFF}}
\label{sec:learning_with_operator-valued_random-fourier_features} Before
focusing on learning function with an ORFF model, we briefly review the context
of supervised learning in \acs{vv-RKHS}.  model.
\subsection{Supervised learning within \acs{vv-RKHS}}
Let $\seq{s} = (x_i,y_i)_{i=1}^N\in\left(\mathcal{X}\times\mathcal{Y}\right)^N$
be a sequence of training samples. Given a local loss function $L:
\mathcal{X}\times\mathcal{F}\times\mathcal{Y}\to \overline{\mathbb{R}}$ such
that $L$ is proper, convex and lower semi-continuous in $\mathcal{F}$, we are
interested in finding a \emph{vector-valued function}
$f_{\seq{s}}:\mathcal{X}\to\mathcal{Y}$, that lives in a \acs{vv-RKHS} and
minimize a tradeoff between a data fitting term $L$ and a regularization term
to prevent from overfitting. Namely finding $f_{\seq{s}}\in\mathcal{H}_K$ such
that
\begin{dmath}
    f_{\seq{s}} = \argmin_{f\in\mathcal{H}_K}
    \frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, f, y_i) +
    \frac{\lambda}{2}\norm{f}^2_{K}
    \label{eq:learning_rkhs}
\end{dmath}
where $\lambda\in\mathbb{R}_+$ is a (Tychonov) regularization hyperparameter.
We call the quantity
\begin{dmath*}
    \mathcal{R}_{\lambda}(f,\seq{s})=\frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i,
    f, y_i)+\frac{\lambda}{2}\norm{f}_K^2 \condition{$\forall
    f\in\mathcal{H}_K$, $\forall
    \seq{s}\in\left(\mathcal{X}\times\mathcal{Y}\right)^N$.}
\end{dmath*}
the (Tychonov) regularized risk of the model $f\in\mathcal{H}_K$ according the
local loss $L$. A common choice for $L$ is the squared error loss $L:(x,
f, y) \mapsto \norm{f(x)-y}_{\mathcal{Y}}^2$ which yields the vector-valued
ridge regression problem.
%We introduce a corollary from Mazur and Schauder
%proposed in 1936 (see~\citet{kurdila2006convex, gorniewicz1999topological})
%showing that \cref{eq:learning_rkhs} --and \cref{eq:learning_rkhs_gen}--
%attains a unique mimimizer.
%\begin{theorem}[Mazur-Schauder]
    %\label{cor:unique_minimizer}
    %Let $\mathcal{H}$ be a Hilbert space and $J:\mathcal{H}\to
    %\overline{\mathbb{R}}$ be a proper, convex, lower semi-continuous and
    %coercive function. Then $J$ is bounded from below and attains a minimizer.
    %Moreover if $J$ is strictly convex the minimizer is unique.
%\end{theorem}
%This is easily verified for Ridge regression. Define
%\begin{dmath}
    %\label{eq:ridge}
    %\mathfrak{R}_\lambda(f, \seq{s})=\frac{1}{N}\sum_{i=1}^N\norm{f(x_i) -
    %y_i}_{\mathcal{Y}}^2+ \frac{\lambda}{2}\norm{f}_K^2,
%\end{dmath}
%where $f\in\mathcal{H}_K$ and $\lambda\in\mathbb{R}_{>0}$.
%$\mathfrak{R}_\lambda$ is continuous\footnote{Reminder, if $f\in\mathcal{H}_k,
%\text{ev}_x : f\mapsto f(x)$ is continuous, see \cref{pr:unique_rkhs}.} and
%strictly convex.  Additionally $\mathfrak{R}_\lambda$ is coercive since
%$\norm{f}_K$ is coercive, $\lambda\in\mathbb{R}_{>0}$, and all the summands of
%$\mathfrak{R}_\lambda$ are positive.  Hence for all positive $\lambda$,
%$f_{\seq{s}} = \argmin_{f\in\mathcal{H}_K}\mathfrak{R}_\lambda(f, \seq{s})$
%exists, is unique and attained.
%\begin{remark}[\citet{kadri2015operator}]
    %\label{rk:rkhs_bound} We consider the optimization problem proposed in
    %\cref{eq:ridge} where $L:(x_i, f, y_i) \mapsto
    %\norm{f(x_i)-y_i}_{\mathcal{Y}}^2$. If given a training sample $\seq{s}$,
    %we have
    %\begin{dmath*}
        %\frac{1}{N}\sum_{i=1}^N\norm{y_i}_{\mathcal{Y}}^2 \le \sigma_y^2,
    %\end{dmath*}
    %then $\lambda\norm{f_{\seq{s}}}_K\le 2\sigma_y^2$. Indeed, since
    %$\mathcal{H}_K$ is a Hilbert space, $0\in\mathcal{H}_K$, thus
    %\begin{dmath*}
        %\frac{\lambda}{2}\norm{f_{\seq{s}}}^2_{K} \le
        %\frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, f_{\seq{s}}, y_i) +
        %\frac{\lambda}{2}\norm{f_{\seq{s}}}^2_{K} \le
        %\frac{1}{N}\displaystyle\sum_{i=1}^NL(x_i, 0, y_i) \hiderel{\le}
        %\sigma_y^2 \condition{by optimality of $f_{\seq{s}}$.}
    %\end{dmath*}
    %Since for all $x\in\mathcal{X}$, $\norm{f(x)}_{\mathcal{Y}}\le
    %\sqrt{\norm{K(x, x)}_{\mathcal{Y},\mathcal{Y}}}\norm{f}_{K}$, the maximum
    %value that the solution $\norm{f_{\seq{s}}(x)}_{\mathcal{Y}}$ of
    %\cref{eq:ridge} can reach is $\sigma_y\sqrt{\frac{2\norm{K(x,
    %x)}_{\mathcal{Y}, \mathcal{Y}}}{\lambda}}$. Thus when solving a Ridge
    %regression problem, given a shift-invariant kernel $K_e$, one should choose
    %\begin{dmath*}
        %0 \hiderel{<} \lambda \hiderel{\le}
        %2\norm{K_e(e)}_{\mathcal{Y}, \mathcal{Y}}\frac{\sigma_y^2}{C^2}.
    %\end{dmath*}
    %with $C\in\mathbb{R}_{>0}$ to have a chance to fit all the $y_i$ with norm
    %$\norm{y_i}_{\mathcal{Y}} \le C$ in the train set.
%\end{remark}

\subsubsection{Representer theorem and Feature equivalence}
Regression in \acl{vv-RKHS} has been well studied~\citep{Alvarez2012,
Argyriou_jmlr09,
Minh_icml13,minh2016unifying,sangnier2016joint,kadri2015operator,Micchelli2005,
Brouard2016_jmlr}, and a cornerstone of learning in \acs{vv-RKHS} is the
representer theorem\footnote{Sometimes referred to as minimal norm
interpolation theorem.}, which allows to replace the search of a minimizer in a
infinite dimensional \acs{vv-RKHS} by a finite number of parameters
$(u_i)_{i=1}^N$, $u_i\in\mathcal{Y}$.
\paragraph{}
In the following we suppose we are given a cost function
$c:\mathcal{Y}\times\mathcal{Y}\to\overline{\mathbb{R}}$, such that $c(f(x),y)$
returns the error of the prediction $f(x)$ \acs{wrt}~the ground truth $y$. A
loss function of a model $f$ with respect to an example
$(x,y)\in\mathcal{X}\times\mathcal{Y}$ can be naturally defined from a cost
function as $L(x,f,y)=c(f(x),y)$. Conceptually the function $c$ evaluates the
quality of the prediction versus its ground truth $y\in\mathcal{Y}$ while the
loss function $L$ evaluates the quality of the model $f$ at a training point
$(x,y)\in\mathcal{X}\times\mathcal{Y}$.
\begin{theorem}[Representer theorem]
    \label{th:representer}
    Let $K$ be a $\mathcal{Y}$-Mercer \acl{OVK} and $\mathcal{H}_K$ its
    corresponding $\mathcal{Y}$-Reproducing Kernel Hilbert space.  Let
    $c:\mathcal{Y}\times\mathcal{Y}\to\overline{\mathbb{R}}$ be a cost function
    such that $L(x, f, y)=c(Vf(x), y)$ is a proper convex lower semi-continuous
    function in $f$ for all $x\in\mathcal{X}$ and all $y\in\mathcal{Y}$.
    Eventually let $\lambda\in\mathbb{R}_{>0}$ be the Tychonov regularization
    hyperparameters The solution $f_{\seq{s}}\in\mathcal{H}_K$ of the
    regularized optimization problem
    \begin{dmath}
        \label{eq:argmin_rkhs}
        f_{\seq{s}} = \argmin_{f\in\mathcal{H}_K}
        \frac{1}{N}\displaystyle\sum_{i=1}^N c(f(x_i), y_i) +
        \frac{\lambda}{2}\norm{f}^2_{K}
        \label{eq:learning_rkhs_gen}
    \end{dmath}
    has the form $f_{\seq{s}}=\sum_{j=1}^{N}K(\cdot,x_j)u_{\seq{s},j}$ where
    $u_{\seq{s},j}\in\mathcal{Y}$ and
    \begin{dmath}
        \label{eq:argmin_u} u_{\seq{s}} =
        \argmin_{u\in\Vect_{i=1}^{N}\mathcal{Y}}\frac{1}{N}
        \displaystyle\sum_{i=1}^N c\left(\sum_{k=1}^{N}K(x_i,x_j)u_j,
        y_i\right) + \frac{\lambda}{2}\sum_{k=1}^{N}u_i^\adjoint
        K(x_i,x_k)u_k.
    \end{dmath}
\end{theorem}
The first representer theorem was introduced by~\citet{Wahba90} in the case
where $\mathcal{Y}=\mathbb{R}$. The extension to an arbitrary Hilbert space
$\mathcal{Y}$ has been proved by many authors in different
forms~\citep{Brouard2011,kadri2015operator,Micchelli2005}. The idea behind the
representer theorem is that even though we minimize over the whole space
$\mathcal{H}_K$, when $\lambda>0$, the solution of \cref{eq:learning_rkhs_gen}
falls inevitably into the set $\mathcal{H}_{K,
\seq{s}}=\Set{\sum_{j=1}^{N}K_{x_j}u_j| \forall (u_i)_{i=1}^{N}
\in\mathcal{Y}^{N}}$.  Therefore the result can be expressed as a finite linear
combination of basis functions of the form $K(\cdot,x_k)$. Notice that we can
perform the kernel expansion of
$f_{\seq{s}}=\sum_{j=1}^{N}K(\cdot,x_j)u_{\seq{s},j}$ even though $\lambda=0$.
However $f_{\seq{s}}$ is no longer the solution of \cref{eq:learning_rkhs_gen}
over the whole space $\mathcal{H}_K$ but a projection on the subspace
$\mathcal{H}_{K, \seq{s}}$. 
%While this is in general not a problem for
%practical applications, it might raise issues for further theoretical
%investigations. In particular, it makes it difficult to perform theoretical
%comparison of the \say{exact} solution of \cref{eq:learning_rkhs_gen} with
%respect to the \acs{ORFF} approximation solution given in
%\cref{th:orff_representer}.  
The representer theorem show that minimizing a
functional in a \acs{vv-RKHS} yields a solution which depends on all the points
in the training set. Assuming that for all $x_i$ and $x\in\mathcal{X}$ and for
all $u_i\in\mathcal{Y}$ it takes time $O(P)$ to compute $K(x_i, x)u_i$, making
a prediction using the representer theorem takes $O(NP)$. Obviously If
$\mathcal{Y}=\mathbb{R}^p$, Then $P=O(p^2)$ thus making a prediction cost
$O(Np^2)$ operations.
%% on commence la partie approche
\subsection{Learning with Operator Random Fourier Feature maps}
Instead of  learning a model $f$ that depends on all the points of the training
set, we would like to learn a parametric model of the form
$\tildef{\omega}(x) = \tildePhi{\omega}(x)^\adjoint \theta$, where $\theta$
lives in some space $\tildeH{\omega}$. We are interested in
finding a parameter vector $\theta_{\seq{s}}$ such that
\begin{dmath}
    \label{eq:argmin_applied} \theta_{\seq{s}}=
    \argmin_{\theta\in\tildeH{\omega}} \mathfrak{R}_{\lambda}(\theta, \seq{s})
    \hiderel{=}\argmin_{\theta\in \tildeH{\omega}}
    \frac{1}{N}\sum_{i=1}^Nc\left(\tildePhi{\omega}(x_i)^\adjoint \theta,
    y_i\right) + \frac{\lambda}{2}\norm{\theta}^2_{\tildeH{\omega}}
\end{dmath}
The following theorem states that when $\lambda > 0$ then learning with a
feature map is equivalent to learn with a kernel. Moreover if
$f_{\seq{s}}\in\mathcal{H}_K$ is a solution of \cref{eq:argmin_rkhs} and
$\theta_{\seq{s}}\in\mathcal{H}$ is the solution of
\cref{eq:argmin_RKHS_rand}, then $f_{\seq{s}}=\Phi(\cdot)^\adjoint
\theta_{\seq{s}}$. This equivalence could have been obtained by means of
Lagrange duality. However in this proof we do not use such tool: we only
focus on the representer theorem and the fact that there exists a partial
isometry $W$ between the \acs{vv-RKHS} and a feature space $\mathcal{H}$. We
show that if $\theta_{\seq{s}}$ is a solution of $\cref{eq:argmin_applied}$,
then theta belongs to $(\Ker W)^\bot$, thus there is an isometry between
$\theta_{\seq{s}}\in\tilde{\mathcal{H}}$ and $\mathcal{H}_{\widetilde{K}}$:
namely $W$.
\begin{theorem}[Feature equivalence]
    \label{th:orff_representer} Let $\tildeK{\omega}$ be an \acl{OVK} such that
    for all $x$, $z\in\mathcal{X}$, $\tildePhi{\omega}(x)^\adjoint
    \tildePhi{\omega}(z) = \widetilde{K}(x,z)$ where $\widetilde{K}$ is a
    $\mathcal{Y}$-Mercer \acs{OVK} and $\mathcal{H}_{\tildeK{\omega}}$ its
    corresponding $\mathcal{Y}$-Reproducing kernel Hilbert space.  Let
    $c:\mathcal{Y}\times\mathcal{Y}\to\overline{\mathbb{R}}$ be a cost function
    such that $L\left(x, \widetilde{f}, y\right)=c\left(\widetilde{f}(x),
    y\right)$ is a proper convex lower semi-continuous function in
    $\widetilde{f}\in\mathcal{H}_{\tildeK{\omega}}$ for all $x\in\mathcal{X}$
    and all $y\in\mathcal{Y}$.  Eventually let $\lambda\in\mathbb{R}_{>0}
    \mathbb{R}_+$ be the Tychonov regularization hyperparameter. The solution
    $f_{\seq{s}}\in\mathcal{H}_{\tildeK{\omega}}$ of the regularized
    optimization problem
    \begin{dmath}
        \label{eq:argmin_RKHS_rand} \widetilde{f}_{\seq{s}} =
        \argmin_{\widetilde{f}\in\mathcal{H}_{\tildeK{\omega}}}
        \frac{1}{N}\displaystyle\sum_{i=1}^N c\left(\widetilde{f}(x_i),
        y_i\right) +
        \frac{\lambda}{2}\norm{\widetilde{f}}^2_{\tildeK{\omega}}
    \end{dmath}
    has the form $\widetilde{f}_{\seq{s}} = \tildePhi{\omega}(\cdot)^\adjoint
    \theta_{\seq{s}}$, where $\theta_{\seq{s}} \in (\Ker
    \tildeW{\omega})^{\perp}$ and
    \begin{dmath}
        \label{eq:argmin_featurespace}
        \theta_{\seq{s}}=\argmin_{\theta\in \tildeH{\omega}}
        \frac{1}{N}\sum_{i=1}^Nc\left(\tildePhi{\omega}(x_i)^\adjoint \theta,
        y_i\right) + \frac{\lambda}{2}\norm{\theta}^2_{\tildeH{\omega}}
        \label{eq:arming_RKHS_rand_feat}
    \end{dmath}
\end{theorem}
In the aforementioned theorem, we use the notation $\widetilde{K}$ and
$\tildePhi{\omega}$ because our main subject of interest is the \acs{ORFF} map.
However this theorem works for \emph{any} feature maps $\Phi(x)\in
\mathcal{L}(\mathcal{Y}, \mathcal{H})$ even when $\mathcal{H}$ is infinite
dimensional.\footnote{If $\Phi(x): \mathcal{L}(\mathcal{Y}, \mathcal{H})$ and
$\dim(\mathcal{H})=\infty$, the decomposition $\mathcal{H}=(\Ker W) \oplus
(\Ker W)^\perp$ holds since $\mathcal{H}$ is a Hilbert space and $W$ is a
bounded operator.}.  This shows that when $\lambda>0$ the solution of
\cref{eq:argmin_u} with the approximated kernel $K(x,z) \approx
\tildeK{\omega}(x,z) = \tildePhi{\omega}(x)^\adjoint\tildePhi{\omega}(z)$ is
the same than the solution of \cref{eq:argmin_featurespace} up to an isometric
isomorphm (see \cref{subsubsec:proof_feature_equiv}). Namely, if $u_{\seq{s}}$
is the solution of \cref{eq:argmin_u}, $\theta_{\seq{s}}$ is the solution of
\cref{eq:argmin_featurespace} and $\lambda>0$ we have
\begin{dmath*}
    \theta_{\seq{s}} = \sum_{i=1}^{N} \tildePhi{\omega}(x_i) (u_{\seq{s}})_i
    \hiderel{\in} (\Ker W)^{\perp} \hiderel{\subseteq} \tildeH{\omega}.
\end{dmath*}
If $\lambda_K=0$ we can still find a solution $u_{\seq{s}}$ of
\cref{eq:argmin_u}. By construction of the kernel expansion, we have
$u_{\seq{s}}\in(\Ker W)^\bot$. However looking at the proof of
\cref{th:orff_representer} we see that $\theta_{\seq{s}}$ might \emph{not}
belong to $(\Ker W)^\bot$. We can compute a residual vector $r_{\seq{s}} =
\sum_{i=1}^{N} \tildePhi{\omega}(x_i) (u_{\seq{s}})_i - \theta_{\seq{s}}$.
Since $\sum_{j=1}^N \tildePhi{\omega}(x_j)\in(\Ker W)^\bot$ by construction, if
$r_{\seq{s}}=0$, it means that $\lambda_K$ is large enough for both representer
theorem and \acs{ORFF} representer theorem to apply. If $r_{\seq{s}}\neq 0$ but
$\tildePhi{\omega}(\cdot)^\adjoint r_{\seq{s}} = 0$ it means that both
$\theta_{\seq{s}}$ and $\sum_{j=1}^{N} \tildePhi{\omega}(x_j) u_{\seq{s}}$ are
in $(\Ker W)^\bot$, thus the representer theorem fails to find the \say{true}
solution over the whole space $\mathcal{H}_{\widetilde{K}}$ but returns a
projection onto $\mathcal{H}_{\tildeK{\omega},\seq{s}}$ of the solution. If
$r_{\seq{s}} \neq 0$ and $\tildePhi{\omega}(\cdot)^\adjoint r_{\seq{s}} \neq 0$
means that $\theta_{\seq{s}}$ is \emph{not} in $(\Ker W)^\bot$, thus the
feature equivalence theorem fails to apply. Since $r_{\seq{s}} = \sum_{i=1}^N
\tildePhi{\omega}(x_i)(u_{\seq{s}})_i - \theta_{\seq{s}}^\perp -
\theta_{\seq{s}}^\parallel$ and $\sum_{i=1}^N
\tildePhi{\omega}(x_i)(u_{\seq{s}})_i$ is in $(\Ker W)^\perp$, with mild abuse
of notation we write $r_{\seq{s}}=\theta^\parallel$. This remark is illustrated
in \cref{fig:representer}.
\paragraph{}
In \cref{fig:representer}, we generated the data from a since wave to which we
add some Gaussian noise. We learned a Gaussian kernel based \ac{RFF} model
(blue curve) and a kernel model (yellow curve) where the kernel is obtained
from the \acs{RFF} map. The left column represents the fit of the model to the
points for four different valued of $\lambda$ (top to bottom: $10^{-2}$,
$10^{-5}$, $10e^{10}$, $0$). The middle column shows if the \acs{RFF} solution
$\theta_{\seq{s}}$ is in $(\Ker \tilde{W})^\perp$.  This is true for all values
of $\lambda$. The right column shows that even though $\theta_{\seq{s}}$ is in
$(\Ker \tilde{W})^\perp$, when $\lambda\to0$ learning with \acs{RFF} is
different from learning with the kernel constructed from the \acs{RFF} maps
since the coefficients of $\theta^{\parallel}$ are all different from $0$.
%\paragraph{}
%\cref{fig:representer2} is the same setting than \cref{fig:representer} except
%that we decreased the scale parameter $\sigma$ of the Gaussian kernel to make
%it overfit, and emphasize that when $\lambda=0$, $\theta_{\seq{s}}$ might not
%belong to $(\Ker \tilde{W})^\perp$, as represented on the middle column.

\begin{pycode}[representer]
sys.path.append('./src/')
import representer

err = representer.main()
\end{pycode}

%\begin{pycode}[representer2]
%sys.path.append('./src/')
%import representer2

%err = representer2.main()
%\end{pycode}

\afterpage{%
\begin{landscape}
    \begin{figure}
        \pyc{print(r'\centering\resizebox{1.5\textwidth}{!}{\input{./representer.pgf}}')}
        \caption[\acs{ORFF} equivalence theorem.]{\acs{ORFF} equivalence
        theorem. \label{fig:representer}}
    \end{figure}
    %\clearpage
    %\begin{figure}
        %\pyc{print(r'\centering\resizebox{1.5\textwidth}{!}{\input{./representer2.pgf}}')}
        %\caption[\acs{ORFF} equivalence theorem with overfitting.]{\acs{ORFF}
        %equivalence theorem with overfitting. \label{fig:representer2}}
    %\end{figure}
\end{landscape}}

\subsection{Solving ORFF-based regression}\label{subsec:gradient_methods}
% We illustrate the ORFF representer theorem (\cref{cr:orff_representer}) on
% two experiment involving scalar valued kernels.
In order to find a solution to \cref{eq:argmin_applied}, we turn our attention
to gradient descent methods. We define an algorithm (\cref{alg:close_form}) to
find efficiently a solution to \cref{eq:argmin_applied} when
$c(y,y')=\norm{y=y'}_{\mathcal{Y}}^2$ and study its complexity.
\subsubsection{Gradient methods}\label{subsec:gradient_methods}
Since the solution of \cref{eq:argmin_applied} is unique when $\lambda>0$, a
sufficient and necessary condition is that the gradient of
$\mathfrak{R}_{\lambda}$ at the minimizer $\theta_{\seq{s}}$ is zero. We use
the Frechet derivative, the strongest notion of derivative in Banach
spaces~\citep{conway2013course, kurdila2006convex} which directly generalizes
the notion of gradient to Banach spaces.
%A function $f:\mathcal{H}_0\to\mathcal{H}_1$ is call Frechet
%differentiable at $\theta_0\in \mathcal{H}_0$ if there exist a bounded linear
%operator $A\in\mathcal{L}(\mathcal{H}_0,\mathcal{H}_1)$ such that
%\begin{dmath*}
    %\lim_{\norm{h}_{\mathcal{H}_0}\to 0} \frac{\norm{f(\theta_0 + h) -
    %f(\theta_0) - Ah}_{\mathcal{H}_1}}{\norm{h}_{\mathcal{H}_0}} = 0
%\end{dmath*}
%We write
%\begin{dmath*}
    %(D_Ff)(\theta_0)
    %\hiderel{=}\derivativeat{f(\theta)}{\theta}{\theta_0}
    %\hiderel{=}A
%\end{dmath*}
%and call it Frechet derivative of $f$ with respect to $\theta$ at $\theta_0$.
%With mild abuse of notation we write
%\begin{dmath*}
    %\derivativeat{f(\theta)}{\theta}{\theta_0}
    %=\derivative{f(\theta_0)}{\theta_0}.
%\end{dmath*}
The chain rule is valid in this context \cite[theorem 4.1.1 page
140]{kurdila2006convex}. Hence
%Namely, let $\mathcal{H}_0$, $\mathcal{H}_1$ and
%$\mathcal{H}_2$ be three Hilbert spaces. If a function
%$f:\mathcal{H}_0\to\mathcal{H}_1$ is Frechet differentiable at $\theta$ and
%$g:\mathcal{H}_1\to \mathcal{H}_2$ is Frechet differentiable at $f(\theta)$
%then $g\circ f$ is Frechet differentiable at $\theta$ and for all
%$h\in\mathcal{H}_0$
%\begin{dmath*}
    %\lderivative{(g\circ f)(\theta)}{\theta}\circ h
    %=\derivative{g(f(\theta))}{f(\theta)} \circ
    %\derivative{f(\theta)}{\theta}\circ h,
%\end{dmath*}
%or equivalently,
%\begin{dmath*}
    %D_F(g\circ f)(\theta)\circ h
    %= (D_Fg)(f(\theta)) \circ (D_Ff)(\theta)\circ h.
%\end{dmath*}
%If $f:\mathcal{H}\to\mathbb{R}$ then $(D_F f)(\theta_0)
%\in\mathcal{H}^\adjoint$ for all $\theta_0\in\mathcal{H}$, and by Riesz's
%representation theorem we define the gradient of $f$ noted $\nabla_{\theta}
%f(\theta)\in\mathcal{H}$ as the the vector in $\mathcal{H}$ such that
%\begin{dmath*}
    %\inner{\nabla_{\theta} f(\theta), h}_{\mathcal{H}} = (D_Ff)(\theta)\circ h
    %\hiderel{=} \derivative{f(\theta)}{\theta} \circ h.
%\end{dmath*}
%For a function $f:\mathcal{H}_0\to\mathcal{H}_1$ we note the jacobian of $f$ as
%$\jacobian_{\theta} f(\theta) = \derivative{f(\theta)}{\theta}$. In this
%context if $f:\mathcal{H}_0\to\mathcal{H}_1$ and $g:\mathcal{H}_1\to\mathbb{R}$
%the chain rule reads for all $h\in\mathcal{H}_0$
%\begin{dmath*}
    %\lderivative{(g\circ f)(\theta)}{\theta} \circ h
    %= \derivative{g(f(\theta))}{f(\theta)} \circ \jacobian_{\theta}f(\theta)
    %\circ h.
%\end{dmath*}
%By Riesz's representation theorem,
%\begin{dmath*}
    %\inner{\nabla_\theta(g\circ f)(\theta), h}_{\mathcal{H}_0}
    %= \inner{\nabla_{f(\theta)}g(f(\theta)) ,
    %\jacobian_{\theta}f(\theta)h}_{\mathcal{H}_0}
    %= \inner{\left( \jacobian_{\theta} f(\theta) \right)^\adjoint
    %\nabla_{f(\theta)} g(f(\theta)), h}_{\mathcal{H}_0}
%\end{dmath*}
%Hence
%\begin{dmath*}
    %\nabla_{\theta}(g\circ f)(\theta) =
    %\left(\jacobian_{\theta}f(\theta)\right)^\adjoint
    %\nabla_{f(\theta)}g(f(\theta)).
%\end{dmath*}
\begin{dmath*}
    \nabla_{\theta}c\left(\tildePhi{\omega}(x_i)^\adjoint \theta,
    y_i\right)= \tildePhi{\omega}(x_i)
    \left(\lderivativeat{c\left(y,
    y_i\right)}{y}{\tildePhi{\omega}(x_i)^\adjoint
    \theta}\right)^\adjoint, \text{~and }
    \nabla_{\theta}\norm{\theta}^2_{\tildeH{\omega}}\hiderel{=}2\theta.
\end{dmath*}
Provided that $c(y,y_i)$ is Frechet differentiable \acs{wrt}~$y$, for all $y$
and $y_i\in\mathcal{Y}$ we have $\nabla_{\theta} \mathfrak{R}_{\lambda}(\theta,
\seq{s}) \in \tildeH{\omega}$ and
\begin{dmath}
    \label{eq:grad_final}
    \nabla_{\theta} \mathfrak{R}_{\lambda}(\theta, \seq{s}) =
    \frac{1}{N}\sum_{i=1}^N \tildePhi{\omega}(x_i)
    \left(\lderivativeat{c\left(y,
    y_i\right)}{y}{\tildePhi{\omega}(x_i)^\adjoint \theta}\right)^\adjoint +
    \lambda\theta
\end{dmath}
\begin{example}[Naive closed form for the squared error cost]
    Consider the cost function defined for all $y$, $y'\in\mathcal{Y}$ by
    $c(y,y')=\frac{1}{2}\norm{y-y}_{\mathcal{Y}}^2$. Then
    $\left(\lderivativeat{c\left(y,
    y_i\right)}{y}{\tildePhi{\omega}(x_i)^\adjoint \theta}\right)^\adjoint =
    \left(\tildePhi{\omega}(x_i)^\adjoint \theta-y_i\right)$.  Thus, since the
    optimal solution $\theta_{\seq{s}}$ verifies $\nabla_{\theta_{\seq{s}}}
    \mathfrak{R}_{\lambda}(\theta_{\seq{s}}, \seq{s}) = 0$ we have
    $\frac{1}{N}\sum_{i=1}^N
    \tildePhi{\omega}(x_i)\left(\tildePhi{\omega}(x_i)^\adjoint
    \theta_{\seq{s}}-y_i\right) + \lambda \theta_{\seq{s}} = 0$.  Therefore,
    \begin{dmath}
        \label{eq:iff_solution} \left(\frac{1}{N}\sum_{i=1}^N
        \tildePhi{\omega}(x_i) \tildePhi{\omega}(x_i)^\adjoint +
        \lambda I_{\tildeH{\omega}}\right) \theta_{\seq{s}}
        = \frac{1}{N}\sum_{i=1}^N \tildePhi{\omega}(x_i) y_i.
    \end{dmath}
    Suppose that $\mathcal{Y}\subseteq\mathbb{R}^p$, and for all
    $x\in\mathcal{X}$, $\tildePhi{\omega}(x): \mathbb{R}^{r}\to\mathbb{R}^p$
    where all spaces are endowed with the Euclidean inner product. From this we
    can derive \cref{alg:close_form} which returns the closed form solution of
    \cref{eq:argmin_applied} for $c(y,y')=\frac{1}{2}\norm{y-y'}_2^2$.
\end{example}
\subsubsection{Complexity analysis}
\label{subsec:complexity}
\Cref{alg:close_form} constitutes our first step toward large-scale learning
with \aclp{OVK}. We can easily compute the time complexity of
\cref{alg:close_form} when all the operators act on finite dimensional Hilbert
spaces. Suppose that $p=\dim(\mathcal{Y})<\infty$ and for all
$x\in\mathcal{X}$, $\tildePhi{\omega}(x):\mathcal{Y}\to\tildeH{\omega}$ where
$r=\dim(\tildeH{\omega})<\infty$ is the dimension of the redescription space
$\tildeH{\omega}=\mathbb{R}^{r}$. Since $p$ and $r<\infty$, we view the
operators $\tildePhi{\omega}(x)$ and $I_{\tildeH{\omega}}$ as matrices.  Step 1
costs $O_t(Nr^2p)$. Steps 2 costs $O_t(Nrp)$. For step 3, the naive inversion
of the operator costs $O_t(r^3)$. Eventually the overall complexity of
\cref{alg:close_form} is $O_t\left(r^2(Np + r)\right)$, while the space
complexity is $O_s(r^2)$.
\begin{center}
    \begin{algorithm2e}[t!]
        \label{alg:close_form}
        \SetAlgoLined
        \Input{\begin{itemize}
            \item $\seq{s}=(x_i,
            y_i)_{i=1}^N\in\left(\mathcal{X}\times\mathbb{R}^p\right)^N$ a
            sequence of supervised training points,
            \item $\tildePhi{\omega}(x_i) \in \mathcal{L}\left(\mathbb{R}^p,
            \mathbb{R}^{r}\right)$ a feature map defined for all
            $x_i\in\mathcal{X}$,
            \item $\lambda \in\mathbb{R}_{>0}$ the
            Tychonov regularization term,
        \end{itemize}}
        \Output{A model $h:\mathcal{X} \to \mathbb{R}^p$,
        $h(x)=\tildePhi{\omega}(x)^\transpose \theta_{\seq{s}}$.  such that
        $\theta_{\seq{s}}$ minimize \cref{eq:argmin_applied}, where
        $c(y,y')=\norm{y-y'}_2^2$ and $\mathbb{R}^r$ and $\mathbb{R}^p$} 
        $\mathbf{P} \gets \frac{1}{N}\sum_{i=1}^N
        \tildePhi{\omega}(x_i) \tildePhi{\omega}(x_i)^\transpose
        \in\mathcal{L}(\mathbb{R}^{r}, \mathbb{R}^{r})  $\;
        $\mathbf{Y} \gets \frac{1}{N}\sum_{i=1}^N
        \tildePhi{\omega}(x_i)  y_i \in \mathbb{R}^{r} $\;
        $\theta_{\seq{s}} \gets \text{solve}_{\theta}\left((\mathbf{P} +
        \lambda I_r)\theta = \mathbf{Y} \right)$ \;
        \Return $h: x \mapsto \tildePhi{\omega}(x)^\transpose
        \theta_{\seq{s}}$\;
        \caption{Naive closed form for the squared error cost.}
    \end{algorithm2e}
\end{center}
This complexity is to compare with the kernelized solution. Let
\begin{dmath*}
    \mathbf{K}:
    \begin{cases}
        \mathcal{Y}^{N} \to \mathcal{Y}^{N} \\
        u\mapsto\Vect_{i=1}^{N+U}\sum_{j=1}^{N+U}K(x_i, x_j)u_j
    \end{cases}
\end{dmath*}
When $\mathcal{Y}=\mathbb{R}$,
\begin{dmath*}
    \mathbf{K}=
    \begin{pmatrix} K(x_1, x_1) & \hdots & K(x_1, x_{N+U}) \\ \vdots
        & \ddots & \vdots \\  K(x_{N+U}, x_1) & \hdots & K(x_{N+U}, x_{N+U})
    \end{pmatrix}
\end{dmath*}
is called the Gram matrix of $K$. When $\mathcal{Y}=\mathbb{R}^p$, $\mathbf{K}$
is a matrix-valued Gram matrix of size $pN\times pN$ where each entry
$\mathbf{K}_{ij}\in\mathcal{M}_{p,p}(\mathbb{R})$. Then the equivalent
kernelized solution $u_{\seq{s}}$ of \cref{th:representer} is
\begin{dmath*}
    \left(\frac{1}{N} \mathbf{K}  + \lambda
    I_{\Vect_{i=1}^{N}\mathcal{Y}}\right)u_{\seq{s}}=\Vect_{i=1}^N y_i.
\end{dmath*}
which has time complexity $O_t\left(N^3p^3\right)$ and space complexity
$O_s\left(N^2p^2\right)$. Suppose we are given a generic \acs{ORFF} map (see
\cref{subsec:examples_ORFF}). Then $r=2Dp$, where $D$ is the number of samples.
Hence \cref{alg:close_form} is better that its kernelized counterpart when
$r=2Dp$ is small compared to $Np$. Thus, roughly speaking it is better to use
\cref{alg:close_form} when the number of features, $r$, required is small
compared to the number of training points. Notice that \cref{alg:close_form}
has a linear complexity with respect to the number of supervised training
points $N$ so it is better suited to large scale learning provided that $D$
does not grows linearly with $N$.  Yet naive learning with
\cref{alg:close_form} by viewing all the operators as matrices is still
problematic. Indeed learning $p$ independent models with scalar Random Fourier
Features would cost $O_t\left(D^2p^3(N + D)\right)$ since $r=2Dp$. This Means
that learning vector-valued function has increased the (expected) complexity
from $p$ to $p^3$. However in some cases we can drastically reduce the
complexity by viewing the feature-maps as linear operators rather than
matrices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Efficient learning with ORFF}
\label{subsec:efficient_learning}
When developping \cref{alg:close_form} we considered that the feature map
$\tildePhi{\omega}(x)$ was a matrix from $\mathbb{R}^p$ to $\mathbb{R}^{r}$ for
all $x\in\mathcal{X}$, and therefore that computing
$\tildePhi{\omega}(x)\tildePhi{\omega}(z)^\transpose$ has a time complexity of
$O(r^2p)$.  While this holds true in the most generic senario, in many cases
the feature maps present some structure or sparsity allowing to reduce the
computational cost of evaluating the feature map. We focus on the \acl{ORFF}
given by \cref{alg:ORFF_construction}, developped in \cref{sec:building_ORFF}
and \cref{subsec:examples_ORFF} and treat the decomposable kernel, the
curl-free kernel and the divergence-free kernel as an example. We recall that
if $\mathcal{Y}'=\mathbb{R}^{p'}$ and $\mathcal{Y}=\mathbb{R}^p$, then
$\tildeH{\omega}=\mathbb{R}^{2Dp'}$ thus the \aclp{ORFF} given in
\cref{sec:ORFF_construction} have the form
\begin{dmath*}
    \begin{cases}
        \tildePhi{\omega}(x) \in\mathcal{L}\left(\mathbb{R}^p,
        \mathbb{R}^{2Dp'}\right) &: y \mapsto
        \frac{1}{\sqrt{D}}\Vect_{j=1}^D\pairing{x,
        \omega_j}B(\omega_j)^\transpose  y \\ \tildePhi{\omega}(x)^\transpose
        \in\mathcal{L}\left(\mathbb{R}^{2Dp'}, \mathbb{R}^p\right) &: \theta
        \mapsto \frac{1}{\sqrt{D}} \sum_{j=1}^D \pairing{x,
        \omega_j}B(\omega_j)\theta_j
    \end{cases},
\end{dmath*}
where $\omega_j\sim\probability_{\dual{\Haar}, \rho}$ \ac{iid}~and
$B(\omega_j)\in\mathcal{L}\left(\mathbb{R}^p,\mathbb{R}^{p'}\right)$ for all
$\omega_j\in\dual{\mathcal{X}}$. Hence the \acl{ORFF} can be seen as the block
matrix $\hiderel{\in}\mathcal{M}_{2Dp',p}\left(\mathbb{R}\right)$
\begin{dmath}
    \label{eq:matrix_orff}
    \tildePhi{\omega}(x) =
    \begin{pmatrix}
        \cos\inner{x,\omega_1}B(\omega_1)  &
        \sin\inner{x,\omega_1}B(\omega_1)  &
        \hdots &
        \cos\inner{x,\omega_D}B(\omega_D)  &
        \sin\inner{x,\omega_D}B(\omega_D)
    \end{pmatrix}^\transpose
\end{dmath}

\subsubsection{Case of study: the decomposable kernel}
\label{subsec:fast_decomposable}
Throughout this section we show how the mathematical formulation relates to a
concrete (Python) implementation. We propose a Python implementation based on
NumPy~\citep{oliphant2006guide}, SciPy~\citep{jones2014scipy} and
Scikit-learn~\citep{pedregosa2011scikit}. Following \cref{eq:matrix_orff}, the
feature map associated to the decomposable kernel would be
\begin{pycode}[efficient_linop][fontsize=\scriptsize]
r"""Example of efficient implementation of Gaussian decomposable ORFF."""

from time import time

from numpy.linalg import svd
from numpy.random import rand, seed
from numpy import (dot, diag, sqrt, kron, zeros,
                   logspace, log10, matrix, eye, int)
from scipy.sparse.linalg import LinearOperator
from sklearn.kernel_approximation import RBFSampler
from matplotlib.pyplot import savefig, subplots
\end{pycode}

\begin{dmath*}
    \label{eq:matrix_decomposable_orff}
    \tildePhi{\omega}(x) =
    \frac{1}{\sqrt{D}}
    \begin{pmatrix}
        \cos\inner{x,\omega_1} B  &
        \sin\inner{x,\omega_1} B  & \dots &
        \cos\inner{x,\omega_D} B  &
        \sin\inner{x,\omega_D} B
    \end{pmatrix}^\transpose
    = \underbrace{\frac{1}{\sqrt{D}}
    \begin{pmatrix}
        \cos\inner{x,\omega_1} & \sin\inner{x,\omega_1} & \dots &
        \cos\inner{x,\omega_D} & \sin\inner{x,\omega_D}
    \end{pmatrix}}_{\tildephi{\omega}(x)}^\transpose \otimes B^\transpose ,
    % \hiderel{\in}\mathcal{M}_{2Du'u}\left(\mathbb{R}\right),
\end{dmath*}
$\omega_j\sim\probability_{\dual{\Haar}, \rho}$ \ac{iid}, which would lead to
the following naive python implementation for the Gaussian (RBF) kernel of
parameter $\gamma$, whose associated spectral distribution is
$\probability_{\rho}=\mathcal{N}(0, 2\gamma)$.  Let
$\theta\in\mathbb{R}^{2Dp'}$ and $y\in\mathbb{R^p}$. With such implementation
evaluating a matrix vector product such as $\tildePhi{\omega}(x)^\transpose
\theta$ or $\tildePhi{\omega}(x)y$ have $O_t(2Dp'p)$ time complexity and
$O_s(2Dp'p)$ of space complexity, which is utterly inefficient. Indeed, recall
that if $B\in\mathcal{M}_{p,p'}\left(\mathbb{R}^{p'}\right)$ is matrix, the
operator $\tildePhi{\omega}(x)$ corresponding to the decomposable kernel is
\begin{dmath}
    \label{eq:phi_efficient}
    \tildePhi{\omega}(x)y =
    \frac{1}{\sqrt{D}}\Vect_{j=1}^D\begin{pmatrix}\cos\inner{x, \omega_j}
    B^\transpose y \\ \sin\inner{x, \omega_j} B^\transpose y \end{pmatrix}
    \hiderel{=}
    \left(\frac{1}{\sqrt{D}}\Vect_{j=1}^D\begin{pmatrix}\cos\inner{x, \omega_j}
    \\ \sin\inner{x, \omega_j} \end{pmatrix}\right)\otimes (B^\transpose y)
\end{dmath}
and
\begin{dmath}
    \label{eq:phi_transpose_efficient} \tildePhi{\omega}(x)^\transpose \theta =
    \frac{1}{\sqrt{D}} \sum_{j=1}^D \cos\inner{x, \omega_j}B\theta_j +
    \sin\inner{x, \omega_j}B\theta_j \hiderel{=} B\left(\frac{1}{\sqrt{D}}
    \sum_{j=1}^D \left(\cos\inner{x, \omega_j} + \sin\inner{x,
    \omega_j}\right)\theta_j\right).
\end{dmath}
Which requires only evaluation of $B$ on $y$ and can be implemented easily in
Python thanks to SciPy's LinearOperator. Note that the computation of these
expressions can be fully vectorized\footnote{See~\citet{walt2011numpy}.} using
the vectorization property of the Kronecker product. In the following we
consider $\Theta \in \mathcal{M}_{2D,u'}(\mathbb{R})$ and the operator
$\vectorize: \mathcal{M}_{p',2D}(\mathbb{R}) \to \mathbb{R}^{2Dp'}$ which turns
a matrix into a vector (\acs{ie}~$\theta_{p'i+j} = \vectorize(\Theta_{ij})$,
$i\in\mathbb{N}_{(2D-1)}$ and $j\in\mathbb{N}^*_{p'}$). Then
$\left(\tildephi{\omega}(x) \otimes B^\transpose \right)^\transpose  \theta
\hiderel{=} \left(\tildephi{\omega}(x)^\transpose  \otimes B\right)
\vectorize(\Theta) \hiderel{=} \vectorize\left(B \Theta \tildephi{\omega}(x)
\right)$.  with this trick, many authors \citep{Sindhwani2013,
brault2016random, rosasco2010learning, Carmeli2010} notice that the
decomposable kernel usually yields a Stein equation \citep{penzl1998numerical}.
Indeed rewriting step 3 of \cref{alg:close_form} gives a system to solve of the
form
\begin{dmath*}
    \tildephi{\omega}(X)\tildephi{\omega}(X)^\transpose \Theta B^\transpose B +
    \lambda \Theta - Y \hiderel{=} 0
    \Leftrightarrow
    \left(\tildephi{\omega}(X) \tildephi{\omega}(X)^\transpose
    \hiderel{\otimes} B^\transpose B \hiderel{+} \lambda I_{2Dp'}\right) \theta
    \hiderel{-} Y \hiderel{=} 0
\end{dmath*}
Many solvers exists to solve efficiently this kind of systems\footnote{For
instance \citet{sleijpen2010bi}.}, but most of them share the particularity
that they are not just restricted to handle Stein equations. Broadly speaking,
iterative solvers (or matrix free solvers) are designed to solve any systems of
equation of ther form $PX=C$, where $P$ is a linear operator (not a matrix).
This is exacly our case where $\tildephi{\omega}(x)\otimes B^T$ is the matrix
form of the operator $\Theta \mapsto \vectorize(B\Theta\tildephi{\omega}X)$.
\paragraph{}
This leads us to the following (more efficient) Python implementation of the
Decomposable \acs{ORFF} \say{operator} to be feed to a matrix-free solvers.
\begin{pyblock}[efficient_linop][fontsize=\scriptsize]
def EfficientDecomposableGaussianORFF(X, A, gamma=1.,
                                      D=100, eps=1e-5, random_state=0):
    r"""Return the efficient ORFF map associated with the data X.

    Parameters
    ----------
    X : {array-like}, shape = [n_samples, n_features]
        Samples.
    A : {array-like}, shape = [n_targets, n_targets]
        Operator of the Decomposable kernel (positive semi-definite)
    gamma : {float},
        Gamma parameter of the RBF kernel.
    D : {integer}
        Number of random features.
    eps : {float}
        Cutoff threshold for the singular values of A.
    random_state : {integer}
        Seed of the generator.

    Returns
    -------
    \tilde{\Phi}(X) : Linear Operator, callable
    """
    # Decompose A=BB^\transpose
    u, s, v = svd(A, full_matrices=False, compute_uv=True)
    B = dot(diag(sqrt(s[s > eps])), v[s > eps, :])

    # Sample a RFF from the scalar Gaussian kernel
    phi_s = RBFSampler(gamma=gamma, n_components=D, random_state=random_state)
    phiX = phi_s.fit_transform(X)

    # Create the ORFF linear operator
    return LinearOperator((phiX.shape[0] * B.shape[1], D * B.shape[0]),
                          matvec=lambda b: dot(phiX, dot(b.reshape((D, B.shape[0])), B)),
                          rmatvec=lambda r: dot(phiX.T, dot(r.reshape((X.shape[0], B.shape[1])), B.T)))
\end{pyblock}
%\subsection{Linear operators in matrix form}
%\label{subsec:efficient_linop}
%For convenience we give the operators corresponding to the decomposable,
%curl-free and divergence-free kernels in matrix form. Let $(x_i)_{i=1}^N$,
%$N\in\mathbb{N}^*$, $x_i$'s in $\mathbb{R}^d$, $d\le\infty$ be a sequence of
%points in $\mathbb{R}^d$. We note
%\begin{dmath*}
    %X=\begin{pmatrix}x_1 & \hdots &
    %x_N\end{pmatrix}\hiderel{\in}\mathcal{M}_{d,N}
%\end{dmath*}
%the data matrix where each column represents a data point\footnote{In many
%programming language, such as Python, C, C{}\verb!++! or Java each data point
%is traditionally represented by a row in the data matrix (row major
%formulation).  While this is more natural when parsing a data file, it is less
%common in mathematical formulations. In this document we adopt the \emph{column
%major} formulation used by Matlab, Fortran or Julia. Moreover although
%C{}\verb!++! is commonly row major, some libraries such as Eigen are column
%major. When dealing with row major formulation, one should \say{transpose} all
%the equations given in \cref{table:efficient-op}.}. Naturally if
%$\tildePhi{\omega}(x):\mathbb{R}^p\to\mathbb{R}^{r_1}$ and
%$\tildephi{\omega}(x):\mathbb{R}\to\mathbb{R}^{r_2}$, for all
%$x\in\mathbb{R}^d$ we define
%\begin{dmath*}
    %\tildePhi{\omega}(X)=\begin{pmatrix}\tildePhi{\omega}(x_1) & \hdots &
    %\tildePhi{\omega}(x_N)\end{pmatrix}\hiderel{\in}\mathcal{M}_{r_1,Np}
%\end{dmath*}
%and
%\begin{dmath*}
    %\tildephi{\omega}(X)=\begin{pmatrix}\tildephi{\omega}(x_1) & \hdots &
    %\tildephi{\omega}(x_N)\end{pmatrix}\hiderel{\in}\mathcal{M}_{r_2, N}
%\end{dmath*}
%and
%\begin{dmath*}
    %Y=\begin{pmatrix} y_1 & \hdots&  y_N
    %\end{pmatrix}\hiderel{\in}\mathcal{M}_{p, N}.
%\end{dmath*}
%Given a matrix $X\in\mathcal{M}_{m,n}(\mathbb{R})$, we note $X_{\bullet i}$ the
%\emph{column} vector corresponding to the $i$-th column of the matrix $X$ and
%$X_{i \bullet}$ the \emph{row} vector (covector) corresponding to the $i$-th
%line of the matrix $X$. With these notations, if $X\in\mathcal{M}_{m,n}$ and
%$Z\in\mathcal{M}_{n,m'}$, $X_{i\bullet}Z_{\bullet j}\in\mathbb{R}$ is the inner
%product between the $i$-th row of $X$ and the $j$-th column of $Z$ and
%$X_{\bullet i} Z_{j \bullet}\in\mathcal{M}_{m,m'}(\mathbb{R})$ is the outer
%product between the $i$-th column of $X$ and $j$-th row of $X$.
%\paragraph{}
%For the curl-free and divergence-free kernel given in
%\cref{subsec:examples_ORFF} we recall the unbounded \acs{ORFF} maps are
%respectively for all $y\in\mathcal{Y}$
%\begin{dmath*}
    %\tildePhi{\omega}(x) y =\frac{1}{\sqrt{D}}\Vect_{j=1}^D
    %\begin{pmatrix}
        %\cos{\inner{x,\omega_j}_2}\omega_j^\transpose y \\
        %\sin{\inner{x,\omega_j}_2}\omega_j^\transpose y
    %\end{pmatrix},
%\end{dmath*}
%and
%\begin{dmath*}
    %\tildePhi{\omega}(x) y = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
    %\begin{pmatrix}
        %\cos{\inner{x,\omega_j}_2}\left(\norm{\omega_j }_2I_d -
        %\frac{\omega_j\omega_j^\transpose }{\norm{\omega_j}_2}\right) y\\
        %\sin{\inner{x,\omega_j}_2}\left(\norm{\omega_j}_2I_d-
        %\frac{\omega_j\omega_j^\transpose }{\norm{\omega_j}_2}\right) y
    %\end{pmatrix},
%\end{dmath*}
%where $\omega_j\sim \probability_{\mathcal{N}(0,\sigma^{-2}I_d)}$. To avoid
%complex index notations we decompose the feature maps $\tildePhi{\omega}(X)$
%into two sub feature maps $\tildePhi{\omega}^c$ and $\tildePhi{\omega}^s$
%corresponding to the cosine part and the sine part of each feature map. Namely,
%for the curl-free kernel, for all $y\in\mathcal{Y}$
%\begin{dmath*}
    %\tildePhi{\omega}(x) y =
    %\begin{cases}
        %\tildePhi{\omega}^c(x) y = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
        %\begin{pmatrix}
            %\cos{\inner{x,\omega_j}_2}\omega_j^\transpose y
        %\end{pmatrix}, \\
        %\tildePhi{\omega}^s(x) y = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
        %\begin{pmatrix}
            %\sin{\inner{x,\omega_j}_2}\omega_j^\transpose  y
        %\end{pmatrix}.
    %\end{cases}
%\end{dmath*}
%In the same way, for the divergence-free kernel,
%\begin{dmath*}
    %\tildePhi{\omega}(x) y =
    %\begin{cases}
        %\tildePhi{\omega}^c(x) y  = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
        %\begin{pmatrix}
            %\cos{\inner{x,\omega_j}_2}\left(\norm{\omega_j}_2 I_d -
            %\frac{\omega_j\omega_j^\transpose}{\norm{\omega_j}_2} \right) y
        %\end{pmatrix}, \\
        %\tildePhi{\omega}^s(x) y  = \frac{1}{\sqrt{D}}\Vect_{j=1}^D
        %\begin{pmatrix}
            %\sin{\inner{x,\omega_j}_2} \left(\norm{\omega_j}_2 I_d -
            %\frac{\omega_j\omega_j^\transpose}{\norm{\omega_j}_2} \right) y
        %\end{pmatrix}.
    %\end{cases}
%\end{dmath*}
%We also introduce $\tildePhi{\omega}^e$, $e\in\Set{s,c}$ which denotes either
%$\tildePhi{\omega}^s$ or $\tildePhi{\omega}^c$. This equivalent formulation
%allows us to keep the notation \say{lighter} and closer to a proper
%Python/Matlab implementation with vectorization. With these notations, a
%summary of efficient linear operators in matrix form is given in
%\cref{table:efficient-op}. The complexity of evaluating all this operators is
%given in \cref{table:efficient-complexity}.
%\afterpage{%
%\begin{landscape}
    %\begin{table}[htb]{}
        %\centering
        %\begin{threeparttable}
            %\caption[Efficient linear-operators for different
            %\acs{ORFF}.]{Efficient linear-operator (in matrix form) for
            %different Feature maps. \label{table:efficient-op}}
            %\begin{tabularx}{1.4\textwidth}{Xcc}
                %\toprule
                    %Kernel & $\tildePhi{\omega}(X)^\adjoint$ &
                    %$\tildePhi{\omega}(X)$ \\
                %\midrule
                    %Decomposable\tnote{1} &$\Theta\mapsto B\left(\Theta
                    %\tildephi{\omega}(X)\right)$ & $Y\mapsto B^\transpose
                    %\left(Y\tildephi{\omega}(X)^\transpose \right)$ \\ Gaussian
                    %curl-free\tnote{2} & $\Theta^c, \Theta^s\mapsto
                    %\displaystyle\sum_{j=1}^D \omega_j \left(\Theta_{j}^c
                    %\tildephi{\omega}^{c}(X)_{j\bullet} +
                    %\Theta_{j}^s\tildephi{\omega}^{s}(X)_{j\bullet}\right)$ &
                    %$Y\mapsto \Theta_j^e=\omega_j^\transpose
                    %\left(Y\tildephi{\omega}^{e}(X)_{\bullet j}^\transpose
                    %\right)$ \\ Gaussian divergence-free\tnote{2,3} &
                    %$\Theta^c, \Theta^s \mapsto \displaystyle\sum_{j=1}^D
                    %\left(B(\omega_j) \Theta^{c}_{\bullet j}\right)
                    %\tildephi{\omega}^{c}(X)_{j\bullet} +
                    %\left(B(\omega_j)\Theta^{s}_{\bullet
                    %j}\right)\tildephi{\omega}^{s}(X)_{j\bullet}$ & $Y \mapsto
                    %\Theta^e_{\bullet
                    %j}=B(\omega_j)\left(Y\tildephi{\omega}^{e}(X)_{\bullet
                    %j}^\transpose \right)$ \\
                %\bottomrule
            %\end{tabularx}
            %\begin{tablenotes}
                %\item[1] Where $\tildephi{\omega}(X)=\begin{pmatrix}
                %\tildephi{\omega}(X_{\bullet 1}) & \hdots &
                %\tildephi{\omega}(X_{\bullet N})
                %\end{pmatrix}\in\mathcal{M}_{r, N}$ is any design matrix, with
                %scalar feature map
                %$\tildephi{\omega}:\mathbb{R}^d\to\mathbb{R}^r$ such that
                %$\tildephi{\omega}(x)^\adjoint
                %\tildephi{\omega}(z)=k(x,z)\in\mathbb{R}$ for all $x$,
                %$z\in\mathcal{X}$. The input data
                %$X\in\mathcal{M}_{d,N}(\mathbb{R})$, the output data
                %$U\in\mathcal{M}_{p,N}(\mathbb{R})$, the parameter matrices
                %$\Theta^c$ and $\Theta^s\in\mathcal{M}_{p', r}(\mathbb{R})$ and
                %the decomposable operator $B\in\mathcal{M}_{p,p'}(\mathbb{R})$.
                %\item[2] Where
                %$\tildephi{\omega}^{c}(X)_{ji}=\cos\inner{\omega_j, x_i}$ and
                %$\tildephi{\omega}^{s}(X)_{ji}=\sin\inner{\omega_j, x_i}$,
                %$j\in\mathbb{N}^*_D$ and $i\in\mathbb{N}^*_N$. Thus
                %$\tildephi{\omega}^{c}(X)\in\mathcal{M}_{D,N}(\mathbb{R})$ and
                %$\tildephi{\omega}^{s}(X)\in\mathcal{M}_{D,N}(\mathbb{R})$. The
                %input data $X\in\mathcal{M}_{d,N}(\mathbb{R})$, the output data
                %$U\in\mathcal{M}_{d,N}(\mathbb{R})$, the parameter matrices
                %$\Theta^c$ and $\Theta^s\in\mathbb{R}^D$, $\omega_j\sim
                %\probability_{\mathcal{N}(0,\sigma^{-2}I_d)}$ \ac{iid}~for all
                %$j\in\mathbb{N}^*_D$. Eventually $e\in\Set{s,c}$, namely
                %$\Theta^c=\begin{pmatrix} \Theta^{e=c}_1 & \hdots &
                %\Theta^{e=c}_D \end{pmatrix}^\transpose $ and
                %$\Theta^s=\begin{pmatrix} \Theta^{e=s}_1 & \hdots &
                %\Theta^{e=s}_D\end{pmatrix}^\transpose $.
                %\item[3] Here,
                %$\Theta^c$ and $\Theta^s\in\mathcal{M}_{d,D}(\mathbb{R})$ thus
                %$\Theta^c=\begin{pmatrix}\Theta^{e=c}_{\bullet 1} & \hdots &
                %\Theta^{e=c}_{\bullet D}\end{pmatrix}$,
                %$\Theta^s=\begin{pmatrix}\Theta^{e=s}_{\bullet 1} & \hdots &
                %\Theta^{e=s}_{\bullet D}\end{pmatrix}$ and
                %$B(\omega)=\left(\norm{\omega}_2I_d-\frac{\omega\omega^\transpose
                %}{\norm{\omega}_2}\right)\in\mathcal{M}_{d,d}$.
                %\end{tablenotes}
        %\end{threeparttable}
    %\end{table}
%\end{landscape}}
%\paragraph{}
It is worth mentioning that the same strategy can be applied in many different
language. For instance in C{}\verb!++!, the library Eigen~\citep{eigenweb}
allows to wrap a sparse matrix with a custom type, where the user overloads the
transpose and dot product operator (as in Python). Then the custom user
operator behaves as a (sparse) matrix --see
\url{https://eigen.tuxfamily.org/dox/group__MatrixfreeSolverExample.html}. With
this implementation the time complexity of $\tildePhi{\omega}(x)^\transpose
\theta$ and $\tildePhi{\omega}(x)y$ falls down to $O_t((D+p)p')$ and the same
holds for space complexity.
\paragraph{}
A quick experiment shows the advantage of seeing the decomposable kernel as a
linear operator rather than a matrix. We draw $N=100$ points $(x_i)_{i=1}^N$ in
the interval $(0,1)^{20}$ and use a decomposable kernel with matrix
$\Gamma=BB^\transpose \in\mathcal{M}_{p,p}(\mathbb{R})$ where
$B\in\mathcal{M}_{p,p}(\mathbb{R})$ is a random matrix with coefficients drawn
uniformly in $(0,1)$. We compute $\tildePhi{\omega}(x)^\transpose \theta$ for
all $x_i$'s, where $\theta\in\mathcal{M}_{2D,1}(\mathbb{R})$, $D=100$, with the
implementation \texttt{Ef\-fi\-cient\-De\-com\-po\-sa\-ble\-Gaus\-sian\-ORFF},
\cref{eq:phi_transpose_efficient}. The coefficients of $\theta$ were drawn at
random uniformly in $(0,1)$. We report the execution time in
\cref{fig:efficient_decomposable_gaussian} for different values of $p$, $1\le
p\le100$.
\begin{pycode}[efficient_linop]
sys.path.append('./src/')
import efficient_decomposable_gaussian

efficient_decomposable_gaussian.main()
\end{pycode}
\begin{figure}[t]
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./efficient_decomposable_gaussian.pgf}}')}
    \caption[Efficient decomposable Gaussian \acs{ORFF}]{Efficient decomposable
    Gaussian ORFF (lower is better).}
    \label{fig:efficient_decomposable_gaussian}
\end{figure}
The left plot reports the execution time in seconds of the construction of the
feature. The middle plot reports the execution time of
$\tildePhi{\omega}(x)^\transpose \theta$, and the right plot the memory used in
bytes  to store $\tildePhi{\omega}(x)$ for all $x_i$'s. We averaged the results
over ten runs.
\paragraph{Curl-free kernel.}
We use the unbounded \acs{ORFF} map presented in
\cref{eq:unbounded_curl_free_orff}. We draw $N=1000$ points $(x_i)_{i=1}^N$ in
the interval $(0,1)^{p}$ and use a curl-free kernel. We compute
$\tildePhi{\omega}(x)^\transpose \theta$ for all $x_i$'s, where
$\theta\in\mathcal{M}_{2D,1}(\mathbb{R})$, $D=500$, with the matrix
implementation and the \texttt{LinearOperator} implementation. The coefficients
of $\theta$ were drawn at random uniformly in $(0,1)$. We report the execution
time in \cref{fig:efficient_curlfree_gaussian} for different values of $p$,
$1\le p\le100$.
\begin{pycode}[efficient_linop]
sys.path.append('./src/')
import efficient_curlfree_gaussian

efficient_curlfree_gaussian.main()
\end{pycode}
\begin{figure}[t]
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./efficient_curlfree_gaussian.pgf}}')}
    \caption[Efficient curl-free Gaussian \acs{ORFF}]{Efficient curl-free
    Gaussian ORFF (lower is better).}
    \label{fig:efficient_curlfree_gaussian}
\end{figure}
The left plot reports the execution time in seconds of the construction of the
features. The middle plot reports the execution time of
$\tildePhi{\omega}(x)^\transpose \theta$, and the right plot the memory used in
bytes  to store $\tildePhi{\omega}(x)$ for all $x_i$'s. We averaged the results
over fifty runs. As we can see the linear-operator implementation is one order
of magnitude slower than its matrix counterpart. However it uses considerably
less memory.
\paragraph{Divergence-free kernel.}
We use the unbounded \acs{ORFF} map presented in
\cref{eq:unbounded_div_free_orff}. We draw $N=100$ points $(x_i)_{i=1}^N$ in
the interval $(0,1)^{p}$ and use a curl-free kernel. We compute
$\tildePhi{\omega}(x)^\transpose \theta$ for all $x_i$'s, where
$\theta\in\mathcal{M}_{2Dp,1}(\mathbb{R})$, $D=100$, with the matrix
implementation and the \texttt{LinearOperator} implementation. The coefficients
of $\theta$ were drawn at random uniformly in $(0,1)$. We report the execution
time in \cref{fig:efficient_curlfree_gaussian} for different values of $p$,
$1\le p\le100$.
\begin{pycode}[efficient_linop]
sys.path.append('./src/')
import efficient_divfree_gaussian

efficient_divfree_gaussian.main()
\end{pycode}
\begin{figure}[t]
    \pyc{print(r'\centering\resizebox{\textwidth}{!}{\input{./efficient_divfree_gaussian.pgf}}')}
    \caption[Efficient divergence-free Gaussian \acs{ORFF}]{Efficient
    divergence-free Gaussian ORFF (lower is better).}
    \label{fig:efficient_divfree_gaussian}
\end{figure}
The left plot reports the execution time in seconds of the construction of the
feature. The middle plot reports the execution time of
$\tildePhi{\omega}(x)^\transpose \theta$, and the right plot the memory used in
bytes  to store $\tildePhi{\omega}(x)$ for all $x_i$'s. We averaged the results
over ten runs. We draw the same conclusions as the curl-free kernel.
%\begin{table}[!htb]
    %\centering
    %\caption[Time complexity of efficient linear-operators for different
    %\acs{ORFF}.]{Complexity of efficient linear-operator (in matrix form) for
    %different Feature maps given in \cref{table:efficient-op}.
    %\label{table:efficient-complexity}}
    %\begin{tabularx}{\textwidth}{Xcc}
        %\toprule
            %Kernel & $\tildePhi{\omega}(X)^\adjoint$ & $\tildePhi{\omega}(X)$
            %\\
        %\midrule
            %Decomposable & $O_t\left((p'D+p'p)N\right)$ &
            %$O_t\left((pN+p'p)D\right)$ \\
            %Curl-free & $O_t\left(pND\right)$ & $O_t\left(pND\right)$ \\
            %Divergence-free & $O_t\left((p^2+pN)D\right)$ &
            %$O_t\left((p^2+pN)D\right)$ \\
        %\bottomrule
    %\end{tabularx}
%\end{table}

\section{Numerical experiments}
\label{sec:num_exp}
We present a set of experiments to complete the theoretical contribution and
illustrate the behavior of ORFF-regression. First we study how well the ORFF
regression recover the result of operator-valued kernel regression. Second we
show the advantages of ORFF regression over independent RFF regression. A code
implementing ORFF is available at \url{https://github.com/operalib/operalib} a
framework for OVK Learning.

\subsection{Learning with {ORFF} vs learning with {OVK}}
\subsubsection{Datasets}
The \emph{first dataset} considered is the handwritten digits recognition
dataset \textsc{MNIST}\footnote{available at
\url{http://yann.lecun.com/exdb/mnist}}. We select a training set of $12,000$
images and a test set of $10,000$ images. The inputs are images represented as
a vector $x_i\in[0,255]^{784}$ and the targets $y_i\in\mathbb{N}_9$ are
integers between $0$ and $9$.  First we scaled the inputs such that they take
values in $[-1,1]^{784}$. Then we binarize the targets such that each number is
represented by a unique binary vector of dimension $10$. The vector $y_i$ is
zero everywhere except on the dimension corresponding to the class where it is
one.  For instance the class $4$ is encoded $\begin{pmatrix} 0 & 0 & 0 & 0 & 1
& 0 & 0 & 0 & 0 & 0 \end{pmatrix}^\transpose$.  To predict classes, we use the
simplex coding method presented in \citet{mroueh2012multiclass}. The intuition
behind simplex coding is to project the binarized labels of dimension $p$ onto
the most separated vectors on the hypersphere of dimension $p-1$. For ORFF we
can encode directly this projection in the $B$ matrix of the decomposable
kernel $K_0(\delta)=B B^* k_0(\delta)$ where $k_0$ is a Gaussian kernel. The
matrix $B$ is computed via the recursion
\begin{dmath*}
    B_{p+1} \hiderel{=}
    \begin{pmatrix}
        1 & u^T \\
        0_{p-1} & \sqrt{1-p^{-2}}B_p
    \end{pmatrix},
    \qquad B_2 \hiderel{=}
    \begin{pmatrix}
        1 & -1
    \end{pmatrix},
\end{dmath*}
where $u=\begin{pmatrix} -p^{-2} & \hdots & -p^{-2}
\end{pmatrix}^T\in\mathbb{R}^{p-1}$ and $0_{p-1} = \begin{pmatrix} 0 & \hdots &
0 \end{pmatrix}^T \in\mathbb{R}^{p-1}$. For \aclp{OVK} we project the binarized
targets on the simplex as a preprocessing step, before learning with the
decomposable $K_0(\delta)=I_p k_0(\delta)$, where $k_0$ is a scalar Gaussian
kernel.
\paragraph{}
The \emph{second dataset} is a simulated five dimensional ($5D$) vector field
with structure. We generate a scalar field as a random function
$f:[-1,1]^5\to\mathbb{R}$, where
$\tildef{\omega}(x)=\tildephi{\omega}(x)^\adjoint \theta$ where $\theta$ is a
random matrix with each entry following a standard normal distribution,
$\tildephi{\omega}$ is a scalar Gaussian RFF with bandwidth $\sigma=0.4$. The
input data $x$ are generated from a uniform probability distribution. We take
the gradient of $\tildef{\omega}$ to generate the curl-free $5D$ vector field.
\paragraph{}
The \emph{third dataset} is a synthetic of data from
$\mathbb{R}^{20}\to\mathbb{R}^4$ as described in \citet{audiffren2013online}.
In this dataset, inputs ($x_1, \hdots, x_{20}$) are generated independently and
uniformly over $[0, 1]$ and the different outputs are computed as follows. Let
$\phi(x)=(x_1^2, x_4^2, x_1x_2, x_3x_5, x_2, x_4, 1)$ and $(w_i)$ denotes the
\acs{iid}~copies of a seven dimensional Gaussian distribution with zero mean
and covariance $\Sigma\in\mathcal{M}_{7,7}(\mathbb{R})$ such that
$\begin{pmatrix} 0.5 & 0.25 & 0.1 & 0.05 & 0.15 & 0.1 & 0.15 \end{pmatrix}$
Then, the outputs of the different tasks are generated as $y_i=w_i\phi(x)$. We
use this dataset with $p=4$, $10^5$ instances and for the train set and also
$10^5$ instances for the test set.

\subsubsection{Results}
\paragraph{Performance of ORFF regression on the first dataset.}
We trained both \acs{ORFF} and \acs{OVK} models on \textsc{MNIST} dataset with
a decomposable Gaussian kernel with signature
$K_0(\delta)=\exp\left(-\norm{\delta}/(2\sigma^2)\right)\Gamma$.  To apply
$\cref{alg:close_form}$ after noticing that in the case of the decomposable
kernel with $\lambda_M=0$, it boilds down to a Stein equation \citep[section
5.1]{brault2016random}, we use an off-the-shelf solver\footnote{Available at
\url{http://ta.twi.tudelft.nl/nw/users/gijzen/IDR.html}} able to handle Stein's
equation. For both methods we choose $\sigma=20$ and use a $2$-fold cross
validation on the training set to select the optimal $\lambda$. First,
\cref{fig:learning_accuracy} compares the running time between OVK and ORFF
models using $D=1000$ Fourier features against the number of data\-points $N$.
The log-log plot shows ORFF scaling better than the OVK \acs{wrt} the number of
points.  Second, \cref{fig:learning_accuracy} shows the test prediction error
versus the number of ORFFs $D$, when using $N=1000$ training points. As
expected, the ORFF model converges toward the OVK model when the number of
features increases.
\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
        \resizebox{.45\textwidth}{!}{\input{./gfx/learning_accuracy_MNIST.tikz}} &
        \resizebox{.45\textwidth}{!}{\input{./gfx/learning_time_MNIST.tikz}}
    \end{tabular}
    % \includegraphics[\textwidth]{./gfx/learning_time_MNIST.tikz}
    \caption[Prediction Error in percent on the MNIST dataset versus $D$, the
    number of Fourier features]{Empirical comparison of ORFF and OVK regression
    on MNIST dataset and empirical behavior of ORFF regression versus $D$ and
    $N$.\label{fig:learning_accuracy}}
\end{figure}
\paragraph{Performance of ORFF regression on the second dataset.}
We perform a similar experiment on the second dataset ($5$D-vector field with
structure). We use a Gaussian curl-free kernel with bandwidth equal to the
median of the pairwise distances and tune the hyperparameter $\lambda$ on a
grid. Here we optimize \cref{eq:argmin_applied}, where $c$ is the squared error
cost, using Scipy's \acs{L-BFGS-B} \citep{byrd1995limited}
solver\footnote{Available at
\url{http://docs.scipy.org/doc/scipy/reference/optimize.html}} with the
gradients given in \cref{eq:grad_final} and the efficient linear operator
described in \cref{subsec:efficient_learning} (\acs{eg}
\cref{eq:phi_efficient,eq:phi_transpose_efficient}).
\Cref{fig:curl_experiment} (bottom row) reports the $R^2$ (coefficient of
determination) score on the test set versus the number of curl-\acs{ORFF} $D$
with a comparison with curl-\acs{OVK}.  In this experiment, we see that
curl-\acs{ORFF} can even be better than curl-\acs{OVK}, suggesting that
\acs{ORFF} might play an additional regularizing role. It also shows the
computation time of curl-\acs{ORFF} and curl-\acs{OVK}. We see that \acs{OVK}
regression does not scale with large datasets, while \acs{ORFF} regression
does. When $N>10^4$, \acs{OVK} regression exceeds memory capacity.
\begin{figure}[t]
    \centering
    \resizebox{.85\textwidth}{!}{\input{./gfx/Curl_ORFFvsOVK.pgf}}
    \caption{Empirical comparison between curl-free ORFF, curl-free OVK,
    independent ORFF, independent OVK on a synthetic vector field regression
    task. \label{fig:curl_experiment}}
\end{figure}
\paragraph{Structured prediction vs Independent (RFF) prediction.}
On the second dataset, \cref{fig:curl_experiment} (top row) compares $R^2$ score
and time of \acs{ORFF} regression using the trivial identity decomposable
kernel, \acs{eg} independent \acsp{RFF}, to curl-free \acs{ORFF} regression.
Curl-free \acs{ORFF} outperforms independent \acsp{RFF}, as expected, since the
dataset involves structured outputs.
\paragraph{Impact of the number of random features ($D$).}
In this setting we solved the optimisation problem for both \acs{ORFF} and
\acs{OVK} using a \acs{L-BFGS-B}. \Cref{fig:ORFFvsOVK_dec} top row shows that
for a fixed number of instance in the train set, \acs{OVK} performs better than
\acs{ORFF} in terms of accuracy ($R^2$). However \acs{ORFF} scales better than
\acs{OVK} \acs{wrt} the number of data. \acs{ORFF} is able to process more data
than \acs{OVK} in the same time and thus reach a better accuracy for a given
amount of time. Bottom row shows that \acs{ORFF} tends to reach \acs{OVK}'s
accuracy for a fixed number of data when the number of features increase.
\begin{figure}[t]
    \centering
    \resizebox{.85\textwidth}{!}{%
    \input{./gfx/ORFFvsOVK.pgf}}
    \caption{Decomposable kernel on the third dataset: $R^2$ score vs number of
    data in the train set ($N$) \label{fig:ORFFvsOVK_dec}}
\end{figure}
\begin{figure}[t]
    \centering
    \resizebox{.85\textwidth}{!}{%
    \input{./gfx/ORFFvsOVK_Dvariation.pgf}}
    \caption{Decomposable kernel on the third dataset: $R^2$ score vs number of
    data in the train set ($N$) for different number for different number of
    random samples ($D$). \label{fig:ORFFvsOVK}}
\end{figure}
\paragraph{Multitask learning.}
In this experiment we are interested in multitask learning with operator-valued
random Fourier features, and see whether the approximation of a joint \acs{OVK}
performs better than an independent \acs{OVK}. In this setting we assume that
for each entry $x_i\in\mathbb{R}^d$ we only have access to one observation
$y_i\in\mathbb{R}$ corresponding to a task $t_i$.  We used the SARCOS dataset,
taken from \url{http://www.gaussianprocess.org/gpml/data/} website. This is an
inverse dynamics problem, \acs{ie} we have to predict the $7$ joint torques
given the joint positions, velocities and accelerations.  Hence, we have to
solve a regression problem with $21$ inputs and $7$ outputs which is a very
nonlinear function. It has $45K$ inputs data.  Suppose that we are given a
collection of inputs data $x_1, \hdots, x_N\in\mathbb{R}^{21}$ and a collection
of output data $((y_1, t_1) \hdots, (y_N, t_N)) \in \left(\mathbb{R}\times
\mathbb{N}_T\right)^N$ where $T$ is the number of tasks.  We consider the
following multitask loss function $L(h(x), (y,t))=\frac{1}{2}\left(\inner{h(x),
e_t}_2-y\right)^2$, This loss function is adapted to datasets where the number
of data per tasks is unbalanced (\acs{ie}~for one input data we observe the
value of only one task and not all the tasks.). We optimise the regularized
risk
\begin{dmath*}
    \frac{1}{N}\sum_{i=1}^N  L\left(h(x_i), (y_i, t_i)\right) +
    \frac{\lambda}{2N}||{h}||_{\mathcal{H}}^2=\frac{1}{2N}\sum_{i=1}^N
    \left(\langle h(x_i), e_{t_i} \rangle-y_i\right)^2 +
    \frac{\lambda}{2N}||{h}||_{\mathcal{H}}^2
\end{dmath*}
We used a model $h$ based on the decomposable kernel $h(x)= (\phi(x)^T \otimes
B) \theta$ we chose $B$ such that $BB^T=A$, where $A$ is the inverse graph
Laplacian $L$ of the similarities between the tasks, parametrized by an
hyperparameter $\gamma \in \mathbb{R}_+$.
$L_{kl}=\exp\left(-\gamma\sqrt{\sum_{i=1}^N \left(y_i^k -
y_i^l\right)^2}\right)$.  We draw $N$ data randomly for each task, hence
creating a dataset of $N\times 7$ data and computed the nMSE on the proposed
test set ($4.5$K points). We repeated the experiments $80$ times to avoid
randomness. We choose $D=\frac{\max(N, 500)}{2}$ features, and optimized the
problem with a second order batch gradient.
\begin{table}
    \centering
    \begin{tabular}{c|cccc}
        \toprule
            N & Independant (\%) & Laplacian (\%)& p-value & T \\
        \midrule
            $50\times 7$ & $23.138 \pm 0.577$ & $22.254\pm 0.536$ & $2.68\%$ &
            $4$(s) \\
            $100\times 7$ & $16.191 \pm 0.221$ & $15.568 \pm 0.187$ & $<0.1\%$
            & $16$(s) \\
            %$150\times 7$ & $13.821 \pm 0.115$ & $13.459 \pm 0.106$ & $<0.1\%$
            %& $13$(s) \\
            $200\times 7$ & $12.713 \pm 0.0978$ & $12.554 \pm 0.0838$ &
            $1.52\%$ & $12$(s) \\
            $400\times 7$ & $10.785 \pm 0.0579$ & $10.651 \pm 0.0466$ & $<
            0.1\%$ & $10$(s) \\
            $800\times 7$ & $7.512\pm 0.0344$ & $7.512\pm 0.0344$ & $100\%$ &
            $15$(s) \\
            %$1600\times 7$ & $6.486 \pm 0.0242$ & $6.486 \pm 0.0242$ & $100\%$
            %& $20$(s) \\
            $3200\times 7$ & $5.658 \pm 0.0187$ & $5.658 \pm 0.0187$ & $100\%$
            & $20$(s) \\
        \bottomrule
    \end{tabular}
    \caption{Error (\% of nMSE) on SARCOS dataset.}
    \label{table:sarcos}
\end{table}
\Cref{table:sarcos} shows that using the \acs{ORFF} approximation of an
operator-valued kernel with a good prior on the data improves the performances
\acs{wrt} the independent \acs{ORFF}. However the advantage seems to be less
important the more data are available.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sec:conclusion}
\acsp{OVK} naturally extend the celebrated kernel method used to learn
scalar-valued functions, to the case of learning vector-valued functions.
Although \acsp{OVK} are appealing from a theoretical aspect, these methods
scale poorly in terms of computation time when the number of data is high.
Indeed, to evaluate the value of function with an \acl{OVK}, it requires to
evaluate an \acl{OVK} on all the point in the given dataset.  Hence naive
learning with kernels usually scales cubicly in time with the number of data.
In the context of large-scale learning such scaling is not acceptable. Through
this work we propose a methodology to tackle this difficulty.
\paragraph{}
Enlightened by the literature on large-scale learning with
\emph{scalar}-valued kernel, in particular the work of Rahimi and Recht
\citep{Rahimi2007}, we propose to replace an \acs{OVK} by a random feature
map that we called \acl{ORFF}. Our contribution start with the formal
mathematical construction of this feature from an \acs{OVK}. Then we show
that it is also possible to obtain a kernel from an \acs{ORFF}. Eventually
we analyse the regularization properties in terms of \acl{FT} of
$\mathcal{Y}$-Mercer kernels. Then we moved on giving a bound on the error
due to the random approximation of the \acs{OVK} with high probability.
We showed that it is possible to bound the error even though the \acs{ORFF}
estimator of an \acs{OVK} is not a bounded random variable. Moreover we
also give a bound when the dimension of the output data infinite.
\paragraph{}
After ensuring that an \acs{ORFF} is a good approximation of a kernel, we
moved on giving a framework for supervised learnin with \aclp{OVK}. We showed
that learning with a feature map is equivalent to learn with the reconstructed
\acs{OVK} under some mild conditions. Then we focused on an efficient
implementation of \acs{ORFF} by viewing them as linear operators rather than
matrices and using matrix-free (iterative) solvers and concluded with some
numerical experiments.
\paragraph{}
Following Rahimi and Recht a generalization bound for \acs{ORFF} kernel ridge
would probably suggest that the number of feature to draw is proportional to
the number of data.  However new results of \citet{rudi2016generalization}
suggest that the number of feature should be proportional to the \emph{square
root} of the number of data. In a future work, we shall investigate this
results and extend it to \acs{ORFF}.
\paragraph{}
Since the contruction of \acs{ORFF} is valid for infinite dimensional Hilbert
spaces such as function spaces, we would also like to investigate learning
function valued functions in an efficient manner.

% Acknowledgements should go at the end, before appendices and references
\acks{The authors are grateful to Maxime Sangnier (UPMC, France) for the
insightful discussions and Markus Heinonen (Aalto University, Sweden) for the
preliminary experiments.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.


\appendix
\input{appendix}

\bibliography{jmlr-orff}

\end{document}

